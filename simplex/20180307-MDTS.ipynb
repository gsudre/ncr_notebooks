{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a few interactions with the author, it now install properly. We do need to find a better way to parallelize it. But since Tri is done with the new pre-processing pipeline, let's properly link the BAM files first. we'll need to generate new ones for the simplex only samples, so let's see what we have so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples to re-run:\n",
      "CLIA_400185\n",
      "CLIA_400200\n",
      "CLIA_400193\n",
      "CLIA_400195\n",
      "CLIA_400148\n",
      "CLIA_400188\n",
      "CLIA_400189\n",
      "CLIA_400191\n",
      "CLIA_400181\n",
      "CLIA_400203\n",
      "CLIA_400202\n",
      "CLIA_400201\n",
      "CLIA_400192\n",
      "CLIA_400184\n",
      "CLIA_400206\n",
      "CLIA_400186\n",
      "CLIA_400204\n",
      "CLIA_400125\n",
      "CLIA_400190\n",
      "CLIA_400159\n",
      "CLIA_400167\n",
      "CLIA_400179\n",
      "CLIA_400197\n",
      "CLIA_400215\n",
      "CLIA_400171\n",
      "CLIA_400170\n",
      "CLIA_400173\n",
      "CLIA_400172\n",
      "CLIA_400175\n",
      "CLIA_400174\n",
      "CLIA_400177\n",
      "CLIA_400176\n",
      "CLIA_400157\n",
      "CLIA_400156\n",
      "CLIA_400154\n",
      "CLIA_400153\n",
      "CLIA_400152\n",
      "CLIA_400151\n",
      "CLIA_400149\n",
      "CLIA_400135\n",
      "CLIA_400134\n",
      "CLIA_400137\n",
      "CLIA_400136\n",
      "CLIA_400131\n",
      "CLIA_400130\n",
      "CLIA_400133\n",
      "CLIA_400132\n",
      "CLIA_400180\n",
      "CLIA_400187\n",
      "CLIA_400158\n",
      "CLIA_400139\n",
      "CLIA_400138\n",
      "CLIA_400122\n",
      "CLIA_400199\n",
      "CLIA_400198\n",
      "CLIA_400178\n",
      "CLIA_400210\n",
      "CLIA_400211\n",
      "CLIA_400212\n",
      "CLIA_400213\n",
      "CLIA_400214\n",
      "CLIA_400196\n",
      "CLIA_400216\n",
      "CLIA_400194\n",
      "CLIA_400147\n",
      "CLIA_400205\n",
      "CLIA_400121\n",
      "CLIA_400182\n",
      "CLIA_400127\n",
      "CLIA_400166\n",
      "CLIA_400183\n",
      "CLIA_400128\n",
      "CLIA_400129\n",
      "CLIA_400162\n",
      "CLIA_400163\n",
      "CLIA_400160\n",
      "CLIA_400161\n",
      "CLIA_400144\n",
      "CLIA_400123\n",
      "CLIA_400150\n",
      "CLIA_400140\n",
      "CLIA_400141\n",
      "CLIA_400142\n",
      "CLIA_400143\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd ~/data/cnv/mdts\n",
    "cp ../conifer84/samples.txt .\n",
    "\n",
    "echo \"Samples to re-run:\"\n",
    "while read s; do\n",
    "    if [ ! -e /data/NCR_SBRB/ADHDQTL/GATK/BAM/${s}/${s}_trimmed_sorted_RG_markduplicate_recalibrated.bam ]; then\n",
    "        echo $s;\n",
    "    fi;\n",
    "done < samples.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that's disheartening... do we have anyone ready?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples ready:\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd ~/data/cnv/mdts\n",
    "\n",
    "echo \"Samples ready:\"\n",
    "while read s; do\n",
    "    if [ -e /data/NCR_SBRB/ADHDQTL/GATK/BAM/${s}/${s}_trimmed_sorted_RG_markduplicate_recalibrated.bam ]; then\n",
    "        echo $s;\n",
    "    fi;\n",
    "done < samples.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright then... we'll need to re-run everyone.\n",
    "\n",
    "Since we're re-running this, it'll be interesting to try different BAM pipelines:\n",
    "\n",
    "1. Standard (BWA MEM + Picard + Base recalibration)\n",
    "2. No base recalibration (BWA MEM + Picard)\n",
    "3. mrsFast (as suggested in CONIFER)\n",
    "\n",
    "Running mrsFast might not be worth it, and it's somewhat older software (2014). So, if we end up doing it, we might as well run some of their own CNV tools (http://mrcanavar.sourceforge.net/manual.html)\n",
    "\n",
    "Also, note that I'm running the WPS version of the trimming, because I first tried the CCGO/CLIA version, and the bad KMER contents were still there. I'm pretty sure it's because the last wave had CLIA samples with WPS, and the issue is with the wave itself, not the sample code. But I'll double check the QC files when they're done running before going ahead with all different CNV pipelines. There's a chance we'll be removing too much for some samples (i.e. the CLIA samples not processed in the last wave, but who are part of the simplex study). But hopefully it won't be too bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd /data/NCR_SBRB/simplex\n",
    "\n",
    "while read s; do\n",
    "    echo \"bash ~/autodenovo/gatk_upToSingleCalls.sh $s\" >> swarm.single;\n",
    "done < ~/data/cnv/mdts/samples.txt;\n",
    "\n",
    "[sudregp@cn2350 simplex]$ head swarm.single\n",
    "bash ~/autodenovo/gatk_upToSingleCalls.sh CLIA_400185\n",
    "bash ~/autodenovo/gatk_upToSingleCalls.sh CLIA_400200\n",
    "bash ~/autodenovo/gatk_upToSingleCalls.sh CLIA_400193\n",
    "bash ~/autodenovo/gatk_upToSingleCalls.sh CLIA_400195\n",
    "bash ~/autodenovo/gatk_upToSingleCalls.sh CLIA_400148\n",
    "bash ~/autodenovo/gatk_upToSingleCalls.sh CLIA_400188\n",
    "bash ~/autodenovo/gatk_upToSingleCalls.sh CLIA_400189\n",
    "bash ~/autodenovo/gatk_upToSingleCalls.sh CLIA_400191\n",
    "bash ~/autodenovo/gatk_upToSingleCalls.sh CLIA_400181\n",
    "bash ~/autodenovo/gatk_upToSingleCalls.sh CLIA_400203\n",
    "\n",
    "[sudregp@cn2350 simplex]$ wc -l swarm.single\n",
    "84 swarm.single\n",
    "[sudregp@cn2350 simplex]$ swarm -f swarm.single -t 32 -g 120 --job-name gatk_single --logdir trash --time=48:00:00 --gres=lscratch:1\n",
    "00\n",
    "63296498"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "library(MDTS, lib='~/R')\n",
    "library(BSgenome.Hsapiens.UCSC.hg19)\n",
    "genome = BSgenome.Hsapiens.UCSC.hg19\n",
    "map_file = \"chr1.map.bw\"\n",
    "setwd('~/data/cnv/mdts')\n",
    "pD = getMetaData(\"simplex84.ped\")\n",
    "bins = calcBins(pD, n=5, readLength=100, medianCoverage=150,\n",
    "                minimumCoverage=5, genome=genome, mappabilityFile=map_file)\n",
    "\n",
    "# it might be possible to make this go faster if we split the genome by chromosomes?\n",
    "\n",
    "counts = calcCounts(pD, bins, rl=100)\n",
    "mCounts <- normalizeCounts(counts, bins)\n",
    "md <- calcMD(mCounts, pD)\n",
    "cbs <- segmentMD(md=md, bins=bins)\n",
    "denovo <- denovoDeletions(cbs, mCounts, bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEED TO DOWNLOAD MAPABILITY FILES FOR ALL CHROMOSOMES. SOMETHING LIKE THIS: http://hgdownload.soe.ucsc.edu/goldenPath/hg19/encodeDCC/wgEncodeMapability/. BUT SHOULD HECK THE QC FILES FOR A BETTER READ LENGTH ESTIMATE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
