{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's very weird to me that the results in xhmm_clean and xhmm_clean2 are so different, and we just removed 2 subjects in them. Let's see what could be causing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique samples: 84\n",
      "Unique families: 19\n",
      "Unique children: 46\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "ped_files = ['/data/sudregp/multiplex_simplex/simplex.ped']\n",
    "wes_prefix = ['CLIA', 'CCGO', 'WPS']\n",
    "# fid = open('/home/sudregp/data/multiplex_simplex/samples_simplex_all.txt', 'r')\n",
    "# exclude_list = [line.rstrip() for line in fid]\n",
    "# fid.close()\n",
    "\n",
    "# no controls/affected pair for comparison\n",
    "exclude_list = ['CLIA_400165', 'CLIA_400164', 'CLIA_400155', 'CLIA_400146',\n",
    "                'CLIA_400145', 'CLIA_400126', 'CLIA_400207', 'CLIA_400208',\n",
    "                'CLIA_400209']\n",
    "# missing one parent\n",
    "exclude_list += ['CLIA_400169', 'CLIA_400168']\n",
    "# family 9030\n",
    "exclude_list += ['CCGO_800978', 'CCGO_800977', 'CCGO_800976', 'CCGO_800979',\n",
    "                 'CCGO_800980', 'CLIA_400067']\n",
    "\n",
    "trios = []\n",
    "affected = []\n",
    "controls = []\n",
    "samples = []\n",
    "famids = []\n",
    "for ped_file in ped_files:\n",
    "    fid = open(ped_file, 'r')\n",
    "    for line in fid:\n",
    "        famid, sid, fa, mo, sex, aff = line.rstrip().split('\\t')\n",
    "        # if the current ID and its parents have WES data, and the sample is \n",
    "        # not in yet\n",
    "        if (fa.split('_')[0] in wes_prefix and\n",
    "            mo.split('_')[0] in wes_prefix and\n",
    "            sid.split('_')[0] in wes_prefix and\n",
    "            sid not in samples and\n",
    "            (sid not in exclude_list or fa not in exclude_list or mo not in exclude_list)):\n",
    "            fam = {}\n",
    "            fam['child'] = sid\n",
    "            if aff == '1':\n",
    "                affected.append(sid)\n",
    "            else:\n",
    "                controls.append(sid)\n",
    "            fam['father'] = fa\n",
    "            fam['mother'] = mo\n",
    "            fam['famid'] = famid\n",
    "            trios.append(fam)\n",
    "            samples += [sid, fa, mo]\n",
    "            famids.append(famid)\n",
    "    fid.close()\n",
    "samples = set(samples)\n",
    "famids = set(famids)\n",
    "kids = set(affected + controls)\n",
    "\n",
    "print 'Unique samples:', len(samples)\n",
    "print 'Unique families:', len(famids)\n",
    "print 'Unique children:', len(kids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samples84 = samples.copy()\n",
    "kids84 = kids.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with 88 samples, in 20 families\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "ped_file = '/data/sudregp/cnv/simplex.ped'\n",
    "wes_prefix = ['CLIA', 'CCGO', 'WPS']\n",
    "\n",
    "# cleaning up the data for this analysis\n",
    "fid = open('/home/sudregp/data/cnv/exclude.txt', 'r')\n",
    "exclude_list = [line.rstrip() for line in fid]\n",
    "fid.close()\n",
    "fid = open('/home/sudregp/data/cnv/exclude_multikids.txt', 'r')\n",
    "exclude_list += [line.rstrip() for line in fid]\n",
    "fid.close()\n",
    "\n",
    "trios = []\n",
    "affected = []\n",
    "controls = []\n",
    "samples = []\n",
    "famids = []\n",
    "fid = open(ped_file, 'r')\n",
    "for line in fid:\n",
    "    famid, sid, fa, mo, sex, aff = line.rstrip().split('\\t')\n",
    "    if (fa[:4] in wes_prefix and mo[:4] in wes_prefix and\n",
    "        sid[:4] in wes_prefix and sid not in exclude_list):\n",
    "        fam = {}\n",
    "        fam['child'] = sid\n",
    "        if aff == '1':\n",
    "            affected.append(sid)\n",
    "        else:\n",
    "            controls.append(sid)\n",
    "        fam['father'] = fa\n",
    "        fam['mother'] = mo\n",
    "        fam['famid'] = famid\n",
    "        trios.append(fam)\n",
    "        samples += [sid, fa, mo]\n",
    "        famids.append(famid)\n",
    "fid.close()\n",
    "samples = set(samples)\n",
    "famids = set(famids)\n",
    "good_kids = [t['child'] for t in trios]\n",
    "\n",
    "print 'Working with %d samples, in %d families' % (len(samples), len(famids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['CCGO_800979', 'CCGO_800976', 'CCGO_800977', 'CCGO_800980']\n"
     ]
    }
   ],
   "source": [
    "samples88 = samples.copy()\n",
    "kids88 = kids.copy()\n",
    "print [s for s in samples84 if s not in samples88]\n",
    "print [s for s in samples88 if s not in samples84]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so everyone in 84 is in 88, and we're removing the 4 subjects we thought we were. Can we reproduce the difference in results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fid = open('/data/sudregp/cnv/tmp88/samples.txt', 'w')\n",
    "for s in samples88:\n",
    "    fid.write(s + '\\n')\n",
    "fid.close()\n",
    "fid = open('/data/sudregp/cnv/tmp88/kid_samples.txt', 'w')\n",
    "for s in kids88:\n",
    "    fid.write(s + '\\n')\n",
    "fid.close()\n",
    "\n",
    "fid = open('/data/sudregp/cnv/tmp84/samples.txt', 'w')\n",
    "for s in samples84:\n",
    "    fid.write(s + '\\n')\n",
    "fid.close()\n",
    "fid = open('/data/sudregp/cnv/tmp84/kid_samples.txt', 'w')\n",
    "for s in kids84:\n",
    "    fid.write(s + '\\n')\n",
    "fid.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# in terminal for both tmp84 and tmp88\n",
    "\n",
    "exome_targets='/data/NCR_SBRB/simplex/SeqCapEZ_Exome_v3.0_Design_Annotation_files/SeqCap_EZ_Exome_v3_hg19_capture_targets.bed'\n",
    "gatk_memory=\"50g\"\n",
    "ref_fa='/fdb/GATK_resource_bundle/hg19-2.8/ucsc.hg19.fasta'\n",
    "out_dir='/data/sudregp/cnv/tmp84'\n",
    "\n",
    "cd $out_dir\n",
    "module load GATK\n",
    "module load XHMM\n",
    "\n",
    "GATK -m ${gatk_memory} GCContentByInterval -L ${exome_targets} -R ${ref_fa} -o ./DATA.locus_GC.txt\n",
    "cat ./DATA.locus_GC.txt | awk '{if ($2 < 0.1 || $2 > 0.9) print $1}' > ./extreme_gc_targets.txt\n",
    "\n",
    "# merging all subjects in the directory\n",
    "while read s; do\n",
    "    cp ../xhmm_clean/${s}* .\n",
    "done < samples.txt\n",
    "ls -1 *.sample_interval_summary > depth_list.txt;\n",
    "cp ../xhmm_clean/params.txt .\n",
    "\n",
    "xhmm --mergeGATKdepths --GATKdepthsList=depth_list.txt -o ./DATA.RD.txt;\n",
    "\n",
    "# this does the same thing as the XHMM script, but it actually works in parsing \n",
    "# the base pair start and ends\n",
    "cat ${exome_targets} | awk 'BEGIN{OFS=\"\\t\"; print \"#CHR\\tBP1\\tBP2\\tID\"}{print $1, $2, $3, NR}' > ./EXOME.targets.reg\n",
    "\n",
    "module load plinkseq\n",
    "pseq . loc-load --locdb ./EXOME.targets.LOCDB --file ./EXOME.targets.reg --group targets \\\n",
    "    --out ./EXOME.targets.LOCDB.loc-load --noweb\n",
    "\n",
    "# this has the same effect as the suggested command, but it actually works\n",
    "pseq . loc-stats --locdb ./EXOME.targets.LOCDB --group targets --seqdb ./seqdb.hg19 --noweb | \\\n",
    "    awk '{if (NR > 1) { print  $4, $10 }}' | sed 's/\\.\\./-/' - > ./DATA.locus_complexity.txt\n",
    "\n",
    "cat ./DATA.locus_complexity.txt | awk '{if ($2 > 0.25) print $1}' > ./low_complexity_targets.txt\n",
    "\n",
    "xhmm --matrix -r ./DATA.RD.txt --centerData --centerType target \\\n",
    "-o ./DATA.filtered_centered.RD.txt \\\n",
    "--outputExcludedTargets ./DATA.filtered_centered.RD.txt.filtered_targets.txt \\\n",
    "--outputExcludedSamples ./DATA.filtered_centered.RD.txt.filtered_samples.txt \\\n",
    "--excludeTargets ./extreme_gc_targets.txt --excludeTargets ./low_complexity_targets.txt \\\n",
    "--minTargetSize 10 --maxTargetSize 10000 \\\n",
    "--minMeanTargetRD 10 --maxMeanTargetRD 500 \\\n",
    "--minMeanSampleRD 25 --maxMeanSampleRD 200 \\\n",
    "--maxSdSampleRD 150\n",
    "\n",
    "xhmm --PCA -r ./DATA.filtered_centered.RD.txt --PCAfiles ./DATA.RD_PCA\n",
    "\n",
    "xhmm --normalize -r ./DATA.filtered_centered.RD.txt --PCAfiles ./DATA.RD_PCA \\\n",
    "--normalizeOutput ./DATA.PCA_normalized.txt \\\n",
    "--PCnormalizeMethod PVE_mean --PVE_mean_factor 0.7\n",
    "\n",
    "xhmm --matrix -r ./DATA.PCA_normalized.txt --centerData --centerType sample --zScoreData \\\n",
    "-o ./DATA.PCA_normalized.filtered.sample_zscores.RD.txt \\\n",
    "--outputExcludedTargets ./DATA.PCA_normalized.filtered.sample_zscores.RD.txt.filtered_targets.txt \\\n",
    "--outputExcludedSamples ./DATA.PCA_normalized.filtered.sample_zscores.RD.txt.filtered_samples.txt \\\n",
    "--maxSdTargetRD 30\n",
    "\n",
    "xhmm --matrix -r ./DATA.RD.txt \\\n",
    "--excludeTargets ./DATA.filtered_centered.RD.txt.filtered_targets.txt \\\n",
    "--excludeTargets ./DATA.PCA_normalized.filtered.sample_zscores.RD.txt.filtered_targets.txt \\\n",
    "--excludeSamples ./DATA.filtered_centered.RD.txt.filtered_samples.txt \\\n",
    "--excludeSamples ./DATA.PCA_normalized.filtered.sample_zscores.RD.txt.filtered_samples.txt \\\n",
    "-o ./DATA.same_filtered.RD.txt\n",
    "\n",
    "xhmm --discover -p params.txt -r ./DATA.PCA_normalized.filtered.sample_zscores.RD.txt \\\n",
    "    -R ./DATA.same_filtered.RD.txt -c ./DATA.xcnv -a ./DATA.aux_xcnv -s ./DATA\n",
    "\n",
    "xhmm --genotype -p params.txt -r ./DATA.PCA_normalized.filtered.sample_zscores.RD.txt \\\n",
    "    -R ./DATA.same_filtered.RD.txt -g ./DATA.xcnv -F $ref_fa -v ./DATA.vcf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create PSEQ and PLINK files for both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#in terminal\n",
    "\n",
    "pseq DATA new-project\n",
    "# adding a first column with subject ID for PSEQ\n",
    "cut -f 2 ../simplex.ped > junk.txt\n",
    "paste junk.txt ../simplex.ped > simplex.ped.info\n",
    "pseq DATA load-pedigree --file simplex.ped.info\n",
    "pseq DATA load-vcf --vcf DATA.vcf\n",
    "\n",
    "for q in 50 60 70 80 90; do\n",
    "    pseq DATA cnv-denovo --noweb --minSQ $q --minNQ $q --out DATA_q${q}\n",
    "    grep DENOVO DATA_q${q}.denovo.cnv > pseq_DENOVO.txt\n",
    "    # borrow the header row\n",
    "    head -1 DATA.xcnv > denovo.xcnv;\n",
    "\n",
    "    # filter out denovo CNVs\n",
    "    while read sample; do\n",
    "        grep $sample DATA.xcnv > sample.xcnv;\n",
    "        for cnv in `grep $sample pseq_DENOVO.txt | cut -f 3 -`; do\n",
    "            # replacing .. by -\n",
    "            cnv=`echo $cnv | sed -e 's/\\.\\./\\-/'`;\n",
    "            grep $cnv sample.xcnv >> denovo.xcnv; \n",
    "        done;\n",
    "    done < kid_samples.txt;\n",
    "    /usr/local/apps/XHMM/2016-01-04/sources/scripts/xcnv_to_cnv denovo.xcnv > tmp.cnv\n",
    "    # switch around FAMID and IID columns, and remove header\n",
    "    awk '{OFS=\"\\t\"; if ( $3 != \"CHR\" ) {print $2, $1, $3, $4, $5, $6, $7, $8 }}' tmp.cnv > denovo_q${q}.cnv\n",
    "    rm sample.xcnv pseq_DENOVO.txt tmp.cnv denovo.xcnv\n",
    "    \n",
    "    # filter out inherited cnvs\n",
    "    grep MATERNAL_TRANSMITTED DATA_q${q}.denovo.cnv > pseq_TRANSMITTED.txt\n",
    "    grep PATERNAL_TRANSMITTED DATA_q${q}.denovo.cnv >> pseq_TRANSMITTED.txt\n",
    "    # borrow the header row\n",
    "    head -1 DATA.xcnv > inherited.xcnv;\n",
    "\n",
    "    while read sample; do\n",
    "        grep $sample DATA.xcnv > sample.xcnv;\n",
    "        for cnv in `grep $sample pseq_TRANSMITTED.txt | cut -f 3 -`; do\n",
    "            # replacing .. by -\n",
    "            cnv=`echo $cnv | sed -e 's/\\.\\./\\-/'`;\n",
    "            grep $cnv sample.xcnv >> inherited.xcnv; \n",
    "        done;\n",
    "    done < kid_samples.txt;\n",
    "    /usr/local/apps/XHMM/2016-01-04/sources/scripts/xcnv_to_cnv inherited.xcnv > tmp.cnv\n",
    "    # switch around FAMID and IID columns, and remove header\n",
    "    awk '{OFS=\"\\t\"; if ( $3 != \"CHR\" ) {print $2, $1, $3, $4, $5, $6, $7, $8 }}' tmp.cnv > inherited_q${q}.cnv\n",
    "    rm sample.xcnv pseq_TRANSMITTED.txt tmp.cnv inherited.xcnv\n",
    "    \n",
    "    # compile all CNVs for kids\n",
    "    # borrow the header row\n",
    "    head -1 DATA.xcnv > all.xcnv;\n",
    "\n",
    "    # effectively just filtering DATA.xcnv to keep only kids\n",
    "    while read sample; do\n",
    "        grep $sample DATA.xcnv >> all.xcnv;\n",
    "    done < kid_samples.txt;\n",
    "    /usr/local/apps/XHMM/2016-01-04/sources/scripts/xcnv_to_cnv all.xcnv > tmp.cnv\n",
    "    # switch around FAMID and IID columns, and remove header\n",
    "    awk '{OFS=\"\\t\"; if ( $3 != \"CHR\" ) {print $2, $1, $3, $4, $5, $6, $7, $8 }}' tmp.cnv > all_q${q}.cnv\n",
    "    rm tmp.cnv all.xcnv\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166\n",
      "30\n",
      "30\n",
      "168\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "grep 178 ~/data/cnv/tmp88/denovo_q60.cnv | wc -l\n",
    "grep 178 ~/data/cnv/tmp84/denovo_q60.cnv | wc -l\n",
    "grep 178 ~/data/cnv/xhmm_clean2/denovo_q60.cnv | wc -l\n",
    "grep 178 ~/data/cnv/xhmm_clean/denovo_q60.cnv | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122\n",
      "17\n",
      "17\n",
      "123\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "grep 178 ~/data/cnv/tmp88/denovo_q70.cnv | wc -l\n",
    "grep 178 ~/data/cnv/tmp84/denovo_q70.cnv | wc -l\n",
    "grep 178 ~/data/cnv/xhmm_clean2/denovo_q70.cnv | wc -l\n",
    "grep 178 ~/data/cnv/xhmm_clean/denovo_q70.cnv | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n",
      "8\n",
      "8\n",
      "67\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "grep 178 ~/data/cnv/tmp88/denovo_q80.cnv | wc -l\n",
    "grep 178 ~/data/cnv/tmp84/denovo_q80.cnv | wc -l\n",
    "grep 178 ~/data/cnv/xhmm_clean2/denovo_q80.cnv | wc -l\n",
    "grep 178 ~/data/cnv/xhmm_clean/denovo_q80.cnv | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah, it's quite concerning. Let's see if any of the other methods are as sensitive to the samples involved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CLIA_400121',\n",
       " 'CLIA_400122',\n",
       " 'CLIA_400123',\n",
       " 'CLIA_400125',\n",
       " 'CLIA_400127',\n",
       " 'CLIA_400128',\n",
       " 'CLIA_400129',\n",
       " 'CLIA_400130',\n",
       " 'CLIA_400131',\n",
       " 'CLIA_400132',\n",
       " 'CLIA_400133',\n",
       " 'CLIA_400134',\n",
       " 'CLIA_400135',\n",
       " 'CLIA_400136',\n",
       " 'CLIA_400137',\n",
       " 'CLIA_400138',\n",
       " 'CLIA_400139',\n",
       " 'CLIA_400140',\n",
       " 'CLIA_400141',\n",
       " 'CLIA_400142',\n",
       " 'CLIA_400143',\n",
       " 'CLIA_400144',\n",
       " 'CLIA_400147',\n",
       " 'CLIA_400148',\n",
       " 'CLIA_400149',\n",
       " 'CLIA_400150',\n",
       " 'CLIA_400151',\n",
       " 'CLIA_400152',\n",
       " 'CLIA_400153',\n",
       " 'CLIA_400154',\n",
       " 'CLIA_400156',\n",
       " 'CLIA_400157',\n",
       " 'CLIA_400158',\n",
       " 'CLIA_400159',\n",
       " 'CLIA_400160',\n",
       " 'CLIA_400161',\n",
       " 'CLIA_400162',\n",
       " 'CLIA_400163',\n",
       " 'CLIA_400166',\n",
       " 'CLIA_400167',\n",
       " 'CLIA_400170',\n",
       " 'CLIA_400171',\n",
       " 'CLIA_400172',\n",
       " 'CLIA_400173',\n",
       " 'CLIA_400174',\n",
       " 'CLIA_400175',\n",
       " 'CLIA_400176',\n",
       " 'CLIA_400177',\n",
       " 'CLIA_400178',\n",
       " 'CLIA_400179',\n",
       " 'CLIA_400180',\n",
       " 'CLIA_400181',\n",
       " 'CLIA_400182',\n",
       " 'CLIA_400183',\n",
       " 'CLIA_400184',\n",
       " 'CLIA_400185',\n",
       " 'CLIA_400186',\n",
       " 'CLIA_400187',\n",
       " 'CLIA_400188',\n",
       " 'CLIA_400189',\n",
       " 'CLIA_400190',\n",
       " 'CLIA_400191',\n",
       " 'CLIA_400192',\n",
       " 'CLIA_400193',\n",
       " 'CLIA_400194',\n",
       " 'CLIA_400195',\n",
       " 'CLIA_400196',\n",
       " 'CLIA_400197',\n",
       " 'CLIA_400198',\n",
       " 'CLIA_400199',\n",
       " 'CLIA_400200',\n",
       " 'CLIA_400201',\n",
       " 'CLIA_400202',\n",
       " 'CLIA_400203',\n",
       " 'CLIA_400204',\n",
       " 'CLIA_400205',\n",
       " 'CLIA_400206',\n",
       " 'CLIA_400210',\n",
       " 'CLIA_400211',\n",
       " 'CLIA_400212',\n",
       " 'CLIA_400213',\n",
       " 'CLIA_400214',\n",
       " 'CLIA_400215',\n",
       " 'CLIA_400216'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples84"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One interesting thing here is that the 84 analysis only contains CLIA samples, as we ended up removing all CCGO samples. It could be a coincidence, but maybe not. In any case, let's look at other methods.\n",
    "\n",
    "It does look like the DOC of CCGO samples is generally higher than CLIA... but shouldn't that get autocorrected by XHMM?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
