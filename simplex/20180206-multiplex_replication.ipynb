{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea here is to use the multiplex sample as replication (or negation) sample for the CNV findings in the simplex cohort. In other words, we want to check that whatever we found in the simplex cohort is not true for the multiplex cohort. So, no samples in the simplex cohort should be used in the multiplex findings. For that, I'll use the samples_multiplex_only.txt, which means that the samples are only in multiplex and not in simplex (151 samples raw).\n",
    "\n",
    "Let's start the preprocessing then. First, make sure that no weird samples need to be thrown out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique samples: 153\n",
      "Unique families: 20\n",
      "Unique children: 80\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "ped_files = ['/data/sudregp/multiplex_simplex/multiplex.ped']\n",
    "wes_prefix = ['CLIA', 'CCGO', 'WPS']\n",
    "fid = open('/data/sudregp/cnv/xhmm_clean2/samples.txt', 'r')\n",
    "exclude_list = [line.rstrip() for line in fid]\n",
    "fid.close()\n",
    "\n",
    "trios = []\n",
    "affected = []\n",
    "controls = []\n",
    "samples = []\n",
    "famids = []\n",
    "for ped_file in ped_files:\n",
    "    fid = open(ped_file, 'r')\n",
    "    for line in fid:\n",
    "        famid, sid, fa, mo, sex, aff = line.rstrip().split('\\t')\n",
    "        # if the current ID and its parents have WES data, and the sample is \n",
    "        # not in yet\n",
    "        if (fa.split('_')[0] in wes_prefix and\n",
    "            mo.split('_')[0] in wes_prefix and\n",
    "            sid.split('_')[0] in wes_prefix and\n",
    "            sid not in samples and\n",
    "            (sid not in exclude_list or fa not in exclude_list or mo not in exclude_list)):\n",
    "            fam = {}\n",
    "            fam['child'] = sid\n",
    "            if aff == '1':\n",
    "                affected.append(sid)\n",
    "            else:\n",
    "                controls.append(sid)\n",
    "            fam['father'] = fa\n",
    "            fam['mother'] = mo\n",
    "            fam['famid'] = famid\n",
    "            trios.append(fam)\n",
    "            samples += [sid, fa, mo]\n",
    "            famids.append(famid)\n",
    "    fid.close()\n",
    "samples = set(samples)\n",
    "famids = set(famids)\n",
    "kids = set(affected + controls)\n",
    "\n",
    "print 'Unique samples:', len(samples)\n",
    "print 'Unique families:', len(famids)\n",
    "print 'Unique children:', len(kids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, basically we'll looking at everyone in the multiplex pedigree, provided they're not in the simplex analysis and have parents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fid = open('/data/sudregp/multiplex_simplex/xhmm_replication/samples.txt', 'w')\n",
    "for s in samples:\n",
    "    fid.write(s + '\\n')\n",
    "fid.close()\n",
    "\n",
    "fid = open('/data/sudregp/multiplex_simplex/xhmm_replication/kid_samples.txt', 'w')\n",
    "for s in kids:\n",
    "    fid.write(s + '\\n')\n",
    "fid.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting a very similar analysis we did for xhmm_ximplex_clean of today's date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# in terminal\n",
    "exome_targets='/data/NCR_SBRB/simplex/SeqCapEZ_Exome_v3.0_Design_Annotation_files/SeqCap_EZ_Exome_v3_hg19_capture_targets.bed'\n",
    "gatk_memory=\"50g\"\n",
    "ref_fa='/fdb/GATK_resource_bundle/hg19-2.8/ucsc.hg19.fasta'\n",
    "out_dir='/data/sudregp/multiplex_simplex/xhmm_replication/'\n",
    "\n",
    "cd $out_dir\n",
    "module load GATK\n",
    "module load XHMM\n",
    "\n",
    "GATK -m ${gatk_memory} GCContentByInterval -L ${exome_targets} -R ${ref_fa} -o ./DATA.locus_GC.txt\n",
    "cat ./DATA.locus_GC.txt | awk '{if ($2 < 0.1 || $2 > 0.9) print $1}' > ./extreme_gc_targets.txt\n",
    "\n",
    "# merging all subjects in the directory\n",
    "while read s; do\n",
    "    cp ../xhmm/${s}* .\n",
    "done < samples.txt\n",
    "ls -1 *.sample_interval_summary > depth_list.txt;\n",
    "cp ../xhmm/params.txt .\n",
    "\n",
    "xhmm --mergeGATKdepths --GATKdepthsList=depth_list.txt -o ./DATA.RD.txt;\n",
    "\n",
    "# this does the same thing as the XHMM script, but it actually works in parsing \n",
    "# the base pair start and ends\n",
    "cat ${exome_targets} | awk 'BEGIN{OFS=\"\\t\"; print \"#CHR\\tBP1\\tBP2\\tID\"}{print $1, $2, $3, NR}' > ./EXOME.targets.reg\n",
    "\n",
    "module load plinkseq\n",
    "pseq . loc-load --locdb ./EXOME.targets.LOCDB --file ./EXOME.targets.reg --group targets \\\n",
    "    --out ./EXOME.targets.LOCDB.loc-load --noweb\n",
    "\n",
    "# this has the same effect as the suggested command, but it actually works\n",
    "pseq . loc-stats --locdb ./EXOME.targets.LOCDB --group targets --seqdb ./seqdb.hg19 --noweb | \\\n",
    "    awk '{if (NR > 1) { print  $4, $10 }}' | sed 's/\\.\\./-/' - > ./DATA.locus_complexity.txt\n",
    "\n",
    "cat ./DATA.locus_complexity.txt | awk '{if ($2 > 0.25) print $1}' > ./low_complexity_targets.txt\n",
    "\n",
    "xhmm --matrix -r ./DATA.RD.txt --centerData --centerType target \\\n",
    "-o ./DATA.filtered_centered.RD.txt \\\n",
    "--outputExcludedTargets ./DATA.filtered_centered.RD.txt.filtered_targets.txt \\\n",
    "--outputExcludedSamples ./DATA.filtered_centered.RD.txt.filtered_samples.txt \\\n",
    "--excludeTargets ./extreme_gc_targets.txt --excludeTargets ./low_complexity_targets.txt \\\n",
    "--minTargetSize 10 --maxTargetSize 10000 \\\n",
    "--minMeanTargetRD 10 --maxMeanTargetRD 500 \\\n",
    "--minMeanSampleRD 25 --maxMeanSampleRD 200 \\\n",
    "--maxSdSampleRD 150\n",
    "\n",
    "xhmm --PCA -r ./DATA.filtered_centered.RD.txt --PCAfiles ./DATA.RD_PCA\n",
    "\n",
    "xhmm --normalize -r ./DATA.filtered_centered.RD.txt --PCAfiles ./DATA.RD_PCA \\\n",
    "--normalizeOutput ./DATA.PCA_normalized.txt \\\n",
    "--PCnormalizeMethod PVE_mean --PVE_mean_factor 0.7\n",
    "\n",
    "xhmm --matrix -r ./DATA.PCA_normalized.txt --centerData --centerType sample --zScoreData \\\n",
    "-o ./DATA.PCA_normalized.filtered.sample_zscores.RD.txt \\\n",
    "--outputExcludedTargets ./DATA.PCA_normalized.filtered.sample_zscores.RD.txt.filtered_targets.txt \\\n",
    "--outputExcludedSamples ./DATA.PCA_normalized.filtered.sample_zscores.RD.txt.filtered_samples.txt \\\n",
    "--maxSdTargetRD 30\n",
    "\n",
    "xhmm --matrix -r ./DATA.RD.txt \\\n",
    "--excludeTargets ./DATA.filtered_centered.RD.txt.filtered_targets.txt \\\n",
    "--excludeTargets ./DATA.PCA_normalized.filtered.sample_zscores.RD.txt.filtered_targets.txt \\\n",
    "--excludeSamples ./DATA.filtered_centered.RD.txt.filtered_samples.txt \\\n",
    "--excludeSamples ./DATA.PCA_normalized.filtered.sample_zscores.RD.txt.filtered_samples.txt \\\n",
    "-o ./DATA.same_filtered.RD.txt\n",
    "\n",
    "xhmm --discover -p params.txt -r ./DATA.PCA_normalized.filtered.sample_zscores.RD.txt \\\n",
    "    -R ./DATA.same_filtered.RD.txt -c ./DATA.xcnv -a ./DATA.aux_xcnv -s ./DATA\n",
    "\n",
    "xhmm --genotype -p params.txt -r ./DATA.PCA_normalized.filtered.sample_zscores.RD.txt \\\n",
    "    -R ./DATA.same_filtered.RD.txt -g ./DATA.xcnv -F $ref_fa -v ./DATA.vcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw---- 1 sudregp sudregp 0 Feb  6 17:38 DATA.filtered_centered.RD.txt.filtered_samples.txt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ~/data/multiplex_simplex/xhmm_replication/\n",
    "ls -ltr DATA.filtered_centered.RD.txt.filtered_samples.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#in terminal\n",
    "module load plinkseq\n",
    "module load plink/1.07\n",
    "\n",
    "cd ~/data/multiplex_simplex/xhmm_replication/\n",
    "\n",
    "pseq DATA new-project\n",
    "# adding a first column with subject ID for PSEQ\n",
    "cut -f 2 ../multiplex.ped > junk.txt\n",
    "paste junk.txt ../multiplex.ped > multiplex.ped.info\n",
    "pseq DATA load-pedigree --file multiplex.ped.info\n",
    "pseq DATA load-vcf --vcf DATA.vcf\n",
    "\n",
    "for q in 50 60 70 80 90; do\n",
    "    pseq DATA cnv-denovo --noweb --minSQ $q --minNQ $q --out DATA_q${q}\n",
    "    grep DENOVO DATA_q${q}.denovo.cnv > pseq_DENOVO.txt\n",
    "    # borrow the header row\n",
    "    head -1 DATA.xcnv > denovo.xcnv;\n",
    "\n",
    "    # filter out denovo CNVs\n",
    "    while read sample; do\n",
    "        grep $sample DATA.xcnv > sample.xcnv;\n",
    "        for cnv in `grep $sample pseq_DENOVO.txt | cut -f 3 -`; do\n",
    "            # replacing .. by -\n",
    "            cnv=`echo $cnv | sed -e 's/\\.\\./\\-/'`;\n",
    "            grep $cnv sample.xcnv >> denovo.xcnv; \n",
    "        done;\n",
    "    done < kid_samples.txt;\n",
    "    /usr/local/apps/XHMM/2016-01-04/sources/scripts/xcnv_to_cnv denovo.xcnv > tmp.cnv\n",
    "    # switch around FAMID and IID columns, and remove header\n",
    "    awk '{OFS=\"\\t\"; if ( $3 != \"CHR\" ) {print $2, $1, $3, $4, $5, $6, $7, $8 }}' tmp.cnv > denovo_q${q}.cnv\n",
    "    rm sample.xcnv pseq_DENOVO.txt tmp.cnv denovo.xcnv\n",
    "    \n",
    "    # filter out inherited cnvs\n",
    "    grep MATERNAL_TRANSMITTED DATA_q${q}.denovo.cnv > pseq_TRANSMITTED.txt\n",
    "    grep PATERNAL_TRANSMITTED DATA_q${q}.denovo.cnv >> pseq_TRANSMITTED.txt\n",
    "    # borrow the header row\n",
    "    head -1 DATA.xcnv > inherited.xcnv;\n",
    "\n",
    "    while read sample; do\n",
    "        grep $sample DATA.xcnv > sample.xcnv;\n",
    "        for cnv in `grep $sample pseq_TRANSMITTED.txt | cut -f 3 -`; do\n",
    "            # replacing .. by -\n",
    "            cnv=`echo $cnv | sed -e 's/\\.\\./\\-/'`;\n",
    "            grep $cnv sample.xcnv >> inherited.xcnv; \n",
    "        done;\n",
    "    done < kid_samples.txt;\n",
    "    /usr/local/apps/XHMM/2016-01-04/sources/scripts/xcnv_to_cnv inherited.xcnv > tmp.cnv\n",
    "    # switch around FAMID and IID columns, and remove header\n",
    "    awk '{OFS=\"\\t\"; if ( $3 != \"CHR\" ) {print $2, $1, $3, $4, $5, $6, $7, $8 }}' tmp.cnv > inherited_q${q}.cnv\n",
    "    rm sample.xcnv pseq_TRANSMITTED.txt tmp.cnv inherited.xcnv\n",
    "    \n",
    "    # compile all CNVs for kids\n",
    "    # borrow the header row\n",
    "    head -1 DATA.xcnv > all.xcnv;\n",
    "\n",
    "    # effectively just filtering DATA.xcnv to keep only kids\n",
    "    while read sample; do\n",
    "        grep $sample DATA.xcnv >> all.xcnv;\n",
    "    done < kid_samples.txt;\n",
    "    /usr/local/apps/XHMM/2016-01-04/sources/scripts/xcnv_to_cnv all.xcnv > tmp.cnv\n",
    "    # switch around FAMID and IID columns, and remove header\n",
    "    awk '{OFS=\"\\t\"; if ( $3 != \"CHR\" ) {print $2, $1, $3, $4, $5, $6, $7, $8 }}' tmp.cnv > all_q${q}.cnv\n",
    "    rm tmp.cnv all.xcnv\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#terminal\n",
    "\n",
    "cp ~/data/cnv/penncnv/wellknown_region_hg19 bad_regions.list\n",
    "cp ~/data/cnv/penncnv/glist-hg19 .\n",
    "cp ~/data/cnv/penncnv/genes.txt .\n",
    "cp ~/data/cnv/penncnv/hg19_allenBrainGene_trimmed.txt .\n",
    "\n",
    "for q in 50 60 70 80 90; do\n",
    "    for cnvtype in all denovo inherited; do\n",
    "        cnvname=${cnvtype}_q${q}.cnv\n",
    "        plink --cnv-list $cnvname --cnv-make-map --noweb --out ${cnvtype}_q${q};\n",
    "        \n",
    "        # remove bad regions\n",
    "        plink --map ${cnvname}.map --fam ../multiplex_nofamid.ped --cnv-list $cnvname \\\n",
    "            --noweb --1 --cnv-exclude bad_regions.list --cnv-overlap .5 \\\n",
    "            --cnv-write --out ${cnvtype}_q${q}_clean\n",
    "        plink --cnv-list ${cnvtype}_q${q}_clean.cnv --cnv-make-map --noweb --1 \\\n",
    "            --out ${cnvtype}_q${q}_clean\n",
    "        \n",
    "        for qc in '' '_clean'; do\n",
    "            cnvname=${cnvtype}_q${q}${qc}.cnv\n",
    "            # whole burden\n",
    "            plink --map ${cnvname}.map --fam ../multiplex_nofamid.ped --cnv-list $cnvname \\\n",
    "                --noweb --1 --cnv-check-no-overlap --out ${cnvtype}_q${q}${qc}_burden;\n",
    "            # gene sets\n",
    "            plink --map ${cnvname}.map --fam ../multiplex_nofamid.ped --cnv-list $cnvname \\\n",
    "                --noweb --1 --cnv-intersect glist-hg19 --cnv-verbose-report-regions \\\n",
    "                --cnv-subset genes.txt --out ${cnvtype}_q${q}${qc}_genes;\n",
    "            plink --map ${cnvname}.map --fam ../multiplex_nofamid.ped --cnv-list $cnvname \\\n",
    "                --noweb --1 --cnv-intersect glist-hg19 --cnv-verbose-report-regions \\\n",
    "                --cnv-subset hg19_allenBrainGene_trimmed.txt \\\n",
    "                --out ${cnvtype}_q${q}${qc}_brainGenes;\n",
    "            # subtypes only\n",
    "            for sub in del dup; do\n",
    "                plink --map ${cnvname}.map --fam ../multiplex_nofamid.ped --cnv-list $cnvname \\\n",
    "                --noweb --1 --cnv-${sub} --out ${cnvtype}_q${q}${qc}_${sub}Burden;\n",
    "                # gene sets\n",
    "                plink --map ${cnvname}.map --fam ../multiplex_nofamid.ped --cnv-list $cnvname \\\n",
    "                    --noweb --1 --cnv-intersect glist-hg19 --cnv-verbose-report-regions \\\n",
    "                    --cnv-subset genes.txt --cnv-${sub} \\\n",
    "                    --out ${cnvtype}_q${q}${qc}_${sub}Genes;\n",
    "                plink --map ${cnvname}.map --fam ../multiplex_nofamid.ped --cnv-list $cnvname \\\n",
    "                    --noweb --1 --cnv-intersect glist-hg19 --cnv-verbose-report-regions \\\n",
    "                    --cnv-subset hg19_allenBrainGene_trimmed.txt --cnv-${sub} \\\n",
    "                    --out ${cnvtype}_q${q}${qc}_${sub}BrainGenes;\n",
    "            done;\n",
    "        done;\n",
    "    done;\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# junk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if anyone got removed:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
