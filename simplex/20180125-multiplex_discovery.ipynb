{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea here is to use the multiplex sample as the discovery sample, and confirm it in the simplex. For that, I'll use the samples_multiplex_all.txt for discovery, which means that the samples that could be both in multiplex or simplex will be used for discovery (160 samples raw). The replication set will be families that are simplex only (samples_simplex_only.txt, 90 samples raw).\n",
    "\n",
    "Note that the ideas here can get blurry. If a person has ADHD in an extended family, enriched for ADHD, then it would make sense that ADHD comes from inherited CNVs. Conversely, if the family is truly simplex, we'd expect the changes to come from denovo CNVs. So, how is this a discovery/replication set? \n",
    "\n",
    "1) Even in extended families, we might identify denovo mutations, especially as the sample size is bigger. We could then check these mutations in the simplex families.\n",
    "\n",
    "2) Inherited mutations might have effects in variables other than DX, so we could check that in simplex as well. \n",
    "\n",
    "Let's start the preprocessing then. First, make sure that no weird samples need to be thrown out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique samples: 158\n",
      "Unique families: 20\n",
      "Unique children: 83\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "ped_files = ['/data/sudregp/multiplex_simplex/multiplex.ped']\n",
    "wes_prefix = ['CLIA', 'CCGO', 'WPS']\n",
    "trios = []\n",
    "affected = []\n",
    "controls = []\n",
    "samples = []\n",
    "famids = []\n",
    "for ped_file in ped_files:\n",
    "    fid = open(ped_file, 'r')\n",
    "    for line in fid:\n",
    "        famid, sid, fa, mo, sex, aff = line.rstrip().split('\\t')\n",
    "        # if the current ID and its parents have WES data, and the sample is \n",
    "        # not in yet\n",
    "        if (fa.split('_')[0] in wes_prefix and\n",
    "            mo.split('_')[0] in wes_prefix and\n",
    "            sid.split('_')[0] in wes_prefix and sid not in samples):\n",
    "            fam = {}\n",
    "            fam['child'] = sid\n",
    "            if aff == '1':\n",
    "                affected.append(sid)\n",
    "            else:\n",
    "                controls.append(sid)\n",
    "            fam['father'] = fa\n",
    "            fam['mother'] = mo\n",
    "            fam['famid'] = famid\n",
    "            trios.append(fam)\n",
    "            samples += [sid, fa, mo]\n",
    "            famids.append(famid)\n",
    "    fid.close()\n",
    "samples = set(samples)\n",
    "famids = set(famids)\n",
    "kids = set(affected + controls)\n",
    "\n",
    "print 'Unique samples:', len(samples)\n",
    "print 'Unique families:', len(famids)\n",
    "print 'Unique children:', len(kids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, 2 out of the 160 unique multiplex_all samples didn't make it. Let's see who:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CCGO_800940', 'CCGO_800809']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fid = open('/home/sudregp/data/multiplex_simplex/samples_multiplex_all.txt')\n",
    "m = [l.rstrip() for l in fid]\n",
    "fid.close()\n",
    "[s for s in m if s not in samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9011\tCCGO_800940\t0\t0\t1\t1\n",
      "9011\tCCGO_800809\tCCGO_800940\t0\t1\t1\n",
      "9011\tCCGO_800809\tCCGO_800940\t0\t1\t1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "grep CCGO_800940 /data/sudregp/multiplex_simplex/multiplex.ped\n",
    "grep CCGO_800809 /data/sudregp/multiplex_simplex/multiplex.ped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, that makes sense. They're both in a family without WES data for the mother, only dad and son. Let's add them to the exclude list, and start processing at least XHMM: **NOTE that this is the first time I'm running XHMM excluding targets based on extreme GC contents and rpeeated basis!!! Might need to re-run previous stuff as well.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# in terminal\n",
    "exome_targets='/data/NCR_SBRB/simplex/SeqCapEZ_Exome_v3.0_Design_Annotation_files/SeqCap_EZ_Exome_v3_hg19_capture_targets.bed'\n",
    "gatk_memory=\"50g\"\n",
    "ref_fa='/fdb/GATK_resource_bundle/hg19-2.8/ucsc.hg19.fasta'\n",
    "out_dir='/data/sudregp/multiplex_simplex/xhmm'\n",
    "\n",
    "cd $out_dir\n",
    "module load GATK\n",
    "module load XHMM\n",
    "\n",
    "GATK -m ${gatk_memory} GCContentByInterval -L ${exome_targets} -R ${ref_fa} -o ./DATA.locus_GC.txt\n",
    "cat ./DATA.locus_GC.txt | awk '{if ($2 < 0.1 || $2 > 0.9) print $1}' > ./extreme_gc_targets.txt\n",
    "\n",
    "# merging all subjects in the directory\n",
    "ls -1 *.sample_interval_summary > depth_list.txt;\n",
    "grep -v -f ../exclude.txt depth_list.txt > depth_list2.txt\n",
    "\n",
    "xhmm --mergeGATKdepths --GATKdepthsList=depth_list2.txt -o ./DATA.RD.txt;\n",
    "\n",
    "# this does the same thing as the XHMM script, but it actually works in parsing \n",
    "# the base pair start and ends\n",
    "cat ${exome_targets} | awk 'BEGIN{OFS=\"\\t\"; print \"#CHR\\tBP1\\tBP2\\tID\"}{print $1, $2, $3, NR}' > ./EXOME.targets.reg\n",
    "\n",
    "pseq . loc-load --locdb ./EXOME.targets.LOCDB --file ./EXOME.targets.reg --group targets --out ./EXOME.target\n",
    "s.LOCDB.loc-load --noweb\n",
    "\n",
    "# this has the same effect as the suggested command, but it actually works\n",
    "pseq . loc-stats --locdb ./EXOME.targets.LOCDB --group targets --seqdb ./seqdb.hg19 --noweb | awk '{if (NR >\n",
    "1) { print  $4, $10 }}' | sed 's/\\.\\./-/' - > ./DATA.locus_complexity.txt\n",
    "\n",
    "cat ./DATA.locus_complexity.txt | awk '{if ($2 > 0.25) print $1}' > ./low_complexity_targets.txt\n",
    "\n",
    "xhmm --matrix -r ./DATA.RD.txt --centerData --centerType target \\\n",
    "-o ./DATA.filtered_centered.RD.txt \\\n",
    "--outputExcludedTargets ./DATA.filtered_centered.RD.txt.filtered_targets.txt \\\n",
    "--outputExcludedSamples ./DATA.filtered_centered.RD.txt.filtered_samples.txt \\\n",
    "--excludeTargets ./extreme_gc_targets.txt --excludeTargets ./low_complexity_targets.txt \\\n",
    "--minTargetSize 10 --maxTargetSize 10000 \\\n",
    "--minMeanTargetRD 10 --maxMeanTargetRD 500 \\\n",
    "--minMeanSampleRD 25 --maxMeanSampleRD 200 \\\n",
    "--maxSdSampleRD 150\n",
    "\n",
    "xhmm --PCA -r ./DATA.filtered_centered.RD.txt --PCAfiles ./DATA.RD_PCA\n",
    "\n",
    "xhmm --normalize -r ./DATA.filtered_centered.RD.txt --PCAfiles ./DATA.RD_PCA \\\n",
    "--normalizeOutput ./DATA.PCA_normalized.txt \\\n",
    "--PCnormalizeMethod PVE_mean --PVE_mean_factor 0.7\n",
    "\n",
    "xhmm --matrix -r ./DATA.PCA_normalized.txt --centerData --centerType sample --zScoreData \\\n",
    "-o ./DATA.PCA_normalized.filtered.sample_zscores.RD.txt \\\n",
    "--outputExcludedTargets ./DATA.PCA_normalized.filtered.sample_zscores.RD.txt.filtered_targets.txt \\\n",
    "--outputExcludedSamples ./DATA.PCA_normalized.filtered.sample_zscores.RD.txt.filtered_samples.txt \\\n",
    "--maxSdTargetRD 30\n",
    "\n",
    "xhmm --matrix -r ./DATA.RD.txt \\\n",
    "--excludeTargets ./DATA.filtered_centered.RD.txt.filtered_targets.txt \\\n",
    "--excludeTargets ./DATA.PCA_normalized.filtered.sample_zscores.RD.txt.filtered_targets.txt \\\n",
    "--excludeSamples ./DATA.filtered_centered.RD.txt.filtered_samples.txt \\\n",
    "--excludeSamples ./DATA.PCA_normalized.filtered.sample_zscores.RD.txt.filtered_samples.txt \\\n",
    "-o ./DATA.same_filtered.RD.txt\n",
    "\n",
    "xhmm --discover -p params.txt -r ./DATA.PCA_normalized.filtered.sample_zscores.RD.txt \\\n",
    "    -R ./DATA.same_filtered.RD.txt -c ./DATA.xcnv -a ./DATA.aux_xcnv -s ./DATA\n",
    "\n",
    "xhmm --genotype -p params.txt -r ./DATA.PCA_normalized.filtered.sample_zscores.RD.txt \\\n",
    "    -R ./DATA.same_filtered.RD.txt -g ./DATA.xcnv -F $ref_fa -v ./DATA.vcf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if anyone got removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw---- 1 sudregp sudregp 0 Jan 25 15:37 DATA.filtered_centered.RD.txt.filtered_samples.txt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ~/data/multiplex_simplex/xhmm/\n",
    "ls -ltr DATA.filtered_centered.RD.txt.filtered_samples.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
