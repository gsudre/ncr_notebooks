---
title: "Voxelwise baseline"
output: html_notebook
---

In both DTI and structural, let's use the voxelwise data to classify diagnostic at baseline. As usual, we start with the univariate filters, and then try everything.

# Thickness
```{r}
source('~/ncr_notebooks/baseline_prediction/src/aux_functions.R')
struct_data = read.csv('~/data/baseline_prediction/stripped/structural.csv')
load('~/data/baseline_prediction/struct_thickness.rData')
# the first column of lh and rh is an index variable
vdata = cbind(struct_data$Mask.ID...Scan,
              lh_thickness[,2:ncol(lh_thickness)],
              rh_thickness[,2:ncol(rh_thickness)])
rm(lh_thickness)
rm(rh_thickness)
gf_fname = '~/data/baseline_prediction/stripped/clinical.csv'
gf = read.csv(gf_fname)
gf_base = gf[gf$BASELINE=='BASELINE', ]
my_ids = intersect(gf_base$MRN, struct_data$MRN)
mstruct = mergeOnClosestDate(gf_base, struct_data, my_ids)
rm_me = abs(mstruct$dateX.minus.dateY.months) > 12
mstruct = mstruct[!rm_me, ]
struct_base_vdata = merge(mstruct$Mask.ID...Scan, vdata, by.x=1, by.y=1, all.y=F, all.x=T)
rm(vdata)
```

## Univariate filters
```{r}
X = struct_base_vdata[, 2:ncol(struct_base_vdata)]
rm_me = colSums(is.na(X)) > 0
X = X[, !rm_me]

keep_me = mstruct$age <= 12
X = X[keep_me, ]
y = mstruct$DX_BASELINE
y[y!='NV'] = 'ADHD'
y = factor(y, levels=c('NV', 'ADHD'))
y = y[keep_me]

myseed = 1234
default_preproc = c("center", 'scale')
set.seed(myseed)
split <- createDataPartition(y, p = .8, list = FALSE)
Xtrain <- X[ split, ]
ytrain <- y[ split ]
Xtest  <- X[-split, ]
ytest = y[-split]

# in voxel analysis it's actually more costly to do the correlation in the entire set of variables
# then to run the for loop, so let's first reduce the variables to only the univariate ones
pvals = sapply(Xtrain, function(d) t.test(d ~ ytrain)$p.value)
Xtrain = Xtrain[, which(pvals <= .01)]
print(dim(Xtrain))

# now we can remove correlated and non-informative variables
nzv = nearZeroVar(Xtrain)
print(nzv)
if (length(nzv) > 0) {
  Xtrain = Xtrain[, -nzv]
}
correlations = cor(Xtrain, use='na.or.complete')
# # library(corrplot)
# # corrplot(correlations, order = "hclust")
highCorr = findCorrelation(correlations, cutoff=.75)
print(length(highCorr))
if (length(highCorr) > 0) {
  Xtrain = Xtrain[, -highCorr]
}
print(dim(Xtrain))

keep_me = sapply(colnames(Xtrain), function(d) which(colnames(Xtest) == d))
Xtest = Xtest[, keep_me]

tuneLength=10
set.seed(myseed)
index <- createMultiFolds(ytrain, k = 5, times = 5)

set.seed(myseed)
fullCtrl <- trainControl(method = "repeatedcv",
                         index = index,
                         savePredictions="final",
                         classProbs=TRUE,
                         preProcOptions = c("center", 'scale'))

library(caretEnsemble)
model_list <- caretList(
  Xtrain, ytrain,
  tuneLength=10,
  trControl=fullCtrl,
  methodList=c('rf', 'kernelpls', 'svmRadial', 'knn', 'rpart', 'bagEarthGCV', 'LogitBoost', 'lda', 'nb')
  )

greedy_ensemble <- caretEnsemble(
  model_list, 
  trControl=trainControl(
    number=2
    ))

glm_ensemble <- caretStack(
  model_list,
  method="glm",
  trControl=trainControl(
    method="boot",
    number=10,
    savePredictions="final"
  )
)

gbm_ensemble <- caretStack(
  model_list,
  method="gbm",
  verbose=FALSE,
  tuneLength=10,
  trControl=trainControl(
    method="boot",
    number=10,
    savePredictions="final"
  )
)
summary(greedy_ensemble)
model_preds <- lapply(model_list, predict, newdata=Xtest)
model_preds <- lapply(model_preds, function(d, obs) postResample(d, obs)[1], obs=ytest)
model_preds <- data.frame(model_preds)
ens_preds <- predict(greedy_ensemble, newdata=Xtest)
model_preds$greedyE <- postResample(ens_preds, ytest)[1]
ens_preds <- predict(glm_ensemble, newdata=Xtest)
model_preds$glmE <- postResample(ens_preds, ytest)[1]
ens_preds <- predict(gbm_ensemble, newdata=Xtest)
model_preds$gbmE <- postResample(ens_preds, ytest)[1]
print(model_preds)
print(sprintf('No information rate: Accuracy=%f', max(table(ytrain)/length(ytrain))))
```
```
> summary(greedy_ensemble)
The following models were ensembled: rf, kernelpls, svmRadial, knn, rpart, bagEarthGCV, LogitBoost, lda, nb 
They were weighted: 
5.8322 -2.5238 -6.127 -1.8159 -2.3931 -0.0012 2.1497 -0.1831 0.2349 -0.5027
The resulting Accuracy is: 0.7746
The fit for each individual model on the Accuracy is: 
      method  Accuracy AccuracySD
          rf 0.7070715 0.07751524
   kernelpls 0.7455601 0.07188450
   svmRadial 0.7619433 0.06356524
         knn 0.6598111 0.07186649
       rpart 0.5719838 0.08331574
 bagEarthGCV 0.6689609 0.05071701
  LogitBoost 0.6765182 0.09839470
         lda 0.6154386 0.08346707
          nb 0.7225371 0.06132158
> model_preds <- lapply(model_list, predict, newdata=Xtest)
There were 43 warnings (use warnings() to see them)
> model_preds <- lapply(model_preds, function(d, obs) postResample(d, obs)[1], obs=ytest)
> model_preds <- data.frame(model_preds)
> ens_preds <- predict(greedy_ensemble, newdata=Xtest)
There were 43 warnings (use warnings() to see them)
> model_preds$greedyE <- postResample(ens_preds, ytest)[1]
> ens_preds <- predict(glm_ensemble, newdata=Xtest)
There were 43 warnings (use warnings() to see them)
> model_preds$glmE <- postResample(ens_preds, ytest)[1]
> ens_preds <- predict(gbm_ensemble, newdata=Xtest)
There were 43 warnings (use warnings() to see them)
> model_preds$gbmE <- postResample(ens_preds, ytest)[1]
> print(model_preds)
                rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb   greedyE      glmE      gbmE
Accuracy 0.5957447 0.5957447 0.5957447 0.5531915 0.6382979   0.6382979  0.4680851 0.5957447 0.6808511 0.4042553 0.4042553 0.4042553
> print(sprintf('No information rate: Accuracy=%f', max(table(ytrain)/length(ytrain))))
[1] "No information rate: Accuracy=0.567010"
```
It's not all bad, and we might be able to better combine thes models. nb did alright, and so did rpart and MARS. Note that even though I had 17K variables with pval <= .05, almost all of them were highly correlated, which left me with only 96. These might still be somewhat correlated, just not above .75 threshold. Maybe some PCA of these univariate variables wold help? But because I was generating too many variables at .05 with area and volume, let's run a smaller threshold here first as well. That puts be at 3K variables, and only 31 after removing correlated ones. 

About this variable removal, it might be easier to do a PCA also because of back propagation to the brain. If I remove variables, it might be hard later to show just small triangles in the brain.
```
> summary(greedy_ensemble)
The following models were ensembled: rf, kernelpls, svmRadial, knn, rpart, bagEarthGCV, LogitBoost, lda, nb 
They were weighted: 
4.777 2.1297 -9.488 -5.372 0.9137 -1.0819 2.2701 -0.467 1.3375 0.0391
The resulting Accuracy is: 0.7768
The fit for each individual model on the Accuracy is: 
      method  Accuracy AccuracySD
          rf 0.7421862 0.05142204
   kernelpls 0.7948988 0.05657312
   svmRadial 0.7876113 0.04590266
         knn 0.6721727 0.06396115
       rpart 0.6022402 0.08070039
 bagEarthGCV 0.7166802 0.05550396
  LogitBoost 0.7000270 0.06557287
         lda 0.7401619 0.05412007
          nb 0.7100945 0.06361055
> model_preds <- lapply(model_list, predict, newdata=Xtest)
There were 11 warnings (use warnings() to see them)
> model_preds <- lapply(model_preds, function(d, obs) postResample(d, obs)[1], obs=ytest)
> model_preds <- data.frame(model_preds)
> ens_preds <- predict(greedy_ensemble, newdata=Xtest)
There were 11 warnings (use warnings() to see them)
> model_preds$greedyE <- postResample(ens_preds, ytest)[1]
> ens_preds <- predict(glm_ensemble, newdata=Xtest)
There were 11 warnings (use warnings() to see them)
> model_preds$glmE <- postResample(ens_preds, ytest)[1]
> ens_preds <- predict(gbm_ensemble, newdata=Xtest)
There were 11 warnings (use warnings() to see them)
> model_preds$gbmE <- postResample(ens_preds, ytest)[1]
> print(model_preds)
                rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb   greedyE      glmE      gbmE
Accuracy 0.5531915 0.6170213 0.6382979 0.5531915 0.5319149   0.6170213  0.5744681 0.5744681 0.5957447 0.3617021 0.3617021 0.4255319
> print(sprintf('No information rate: Accuracy=%f', max(table(ytrain)/length(ytrain))))
[1] "No information rate: Accuracy=0.567010"
```
Even though training results improved, testing did not, indicating some overfitting. Let's go back to the filtering then PCA idea:

```{r}
X = struct_base_vdata[, 2:ncol(struct_base_vdata)]
rm_me = colSums(is.na(X)) > 0
X = X[, !rm_me]

keep_me = mstruct$age <= 12
X = X[keep_me, ]
y = mstruct$DX_BASELINE
y[y!='NV'] = 'ADHD'
y = factor(y, levels=c('NV', 'ADHD'))
y = y[keep_me]

myseed = 1234
default_preproc = c("center", 'scale')
set.seed(myseed)
split <- createDataPartition(y, p = .8, list = FALSE)
Xtrain <- X[ split, ]
ytrain <- y[ split ]
Xtest  <- X[-split, ]
ytest = y[-split]

# in voxel analysis it's actually more costly to do the correlation in the entire set of variables
# then to run the for loop, so let's first reduce the variables to only the univariate ones
pvals = sapply(Xtrain, function(d) t.test(d ~ ytrain)$p.value)
Xtrain = Xtrain[, which(pvals <= .05)]
print(dim(Xtrain))

keep_me = sapply(colnames(Xtrain), function(d) which(colnames(Xtest) == d))
Xtest = Xtest[, keep_me]

pp <- preProcess(Xtrain, method = c('BoxCox', 'center', 'scale', 'pca'), thresh=.9)
filtXtrain<- predict(pp, Xtrain)
filtXtest <- predict(pp, Xtest)
print(dim(filtXtrain))

tuneLength=10
set.seed(myseed)
index <- createMultiFolds(ytrain, k = 5, times = 5)

set.seed(myseed)
fullCtrl <- trainControl(method = "repeatedcv",
                         index = index,
                         savePredictions="final",
                         classProbs=TRUE)

library(caretEnsemble)
model_list <- caretList(
  filtXtrain, ytrain,
  tuneLength=10,
  trControl=fullCtrl,
  methodList=c('rf', 'kernelpls', 'svmRadial', 'knn', 'rpart', 'bagEarthGCV', 'LogitBoost', 'lda', 'nb')
  )

greedy_ensemble <- caretEnsemble(
  model_list, 
  trControl=trainControl(
    number=2
    ))

glm_ensemble <- caretStack(
  model_list,
  method="glm",
  trControl=trainControl(
    method="boot",
    number=10,
    savePredictions="final"
  )
)

gbm_ensemble <- caretStack(
  model_list,
  method="gbm",
  verbose=FALSE,
  tuneLength=10,
  trControl=trainControl(
    method="boot",
    number=10,
    savePredictions="final"
  )
)
summary(greedy_ensemble)
model_preds <- lapply(model_list, predict, newdata=filtXtest)
model_preds <- lapply(model_preds, function(d, obs) postResample(d, obs)[1], obs=ytest)
model_preds <- data.frame(model_preds)
ens_preds <- predict(greedy_ensemble, newdata=filtXtest)
model_preds$greedyE <- postResample(ens_preds, ytest)[1]
ens_preds <- predict(glm_ensemble, newdata=filtXtest)
model_preds$glmE <- postResample(ens_preds, ytest)[1]
ens_preds <- predict(gbm_ensemble, newdata=filtXtest)
model_preds$gbmE <- postResample(ens_preds, ytest)[1]
print(model_preds)
print(sprintf('No information rate: Accuracy=%f', max(table(ytrain)/length(ytrain))))
```
```
The following models were ensembled: rf, kernelpls, svmRadial, knn, rpart, bagEarthGCV, LogitBoost, lda, nb 
They were weighted: 
6.8634 2.4965 -11.4843 1.3208 -4.1785 -0.0033 0.0916 -0.34 0.2371 -0.7375
The resulting Accuracy is: 0.7765
The fit for each individual model on the Accuracy is: 
      method  Accuracy AccuracySD
          rf 0.7123617 0.05304788
   kernelpls 0.7873954 0.05814756
   svmRadial 0.7132254 0.06884493
         knn 0.7421862 0.06110772
       rpart 0.6486910 0.06432352
 bagEarthGCV 0.7258570 0.06752318
  LogitBoost 0.6785155 0.07260379
         lda 0.6934953 0.06589098
          nb 0.6400810 0.08224580
> model_preds <- lapply(model_list, predict, newdata=filtXtest)
There were 47 warnings (use warnings() to see them)
> model_preds <- lapply(model_preds, function(d, obs) postResample(d, obs)[1], obs=ytest)
> model_preds <- data.frame(model_preds)
> ens_preds <- predict(greedy_ensemble, newdata=filtXtest)
There were 47 warnings (use warnings() to see them)
> model_preds$greedyE <- postResample(ens_preds, ytest)[1]
> ens_preds <- predict(glm_ensemble, newdata=filtXtest)
There were 47 warnings (use warnings() to see them)
> model_preds$glmE <- postResample(ens_preds, ytest)[1]
> ens_preds <- predict(gbm_ensemble, newdata=filtXtest)
There were 47 warnings (use warnings() to see them)
> model_preds$gbmE <- postResample(ens_preds, ytest)[1]
> print(model_preds)
                rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb   greedyE      glmE      gbmE
Accuracy 0.5957447 0.5531915 0.6595745 0.5531915 0.5957447   0.5531915  0.5319149 0.5531915 0.5319149 0.3617021 0.3617021 0.4255319
> print(sprintf('No information rate: Accuracy=%f', max(table(ytrain)/length(ytrain))))
[1] "No information rate: Accuracy=0.567010"
```
A bit of overfitting again, and we do get 66% using svmRadial, as well as a few other significant results using rf/rpart. This is the best result so far using voxelwise data and different combinations of filtering. If we optimize on ROC would it be better? We could also play with the training to reduce overfitting and hopefully make it generalize better. Another option would be to better combine the classifiers. 

Also, note that we went from 17K significant features to 69 after PCA.

```{r}
X = struct_base_vdata[, 2:ncol(struct_base_vdata)]
rm_me = colSums(is.na(X)) > 0
X = X[, !rm_me]

keep_me = mstruct$age <= 12
X = X[keep_me, ]
y = mstruct$DX_BASELINE
y[y!='NV'] = 'ADHD'
# twoClassSummary uses the first level as the case of interest (https://topepo.github.io/caret/measuring-performance.html#measures-for-class-probabilities)
y = factor(y, levels=c('ADHD', 'NV'))
y = y[keep_me]

myseed = 1234
set.seed(myseed)
split <- createDataPartition(y, p = .8, list = FALSE)
Xtrain <- X[ split, ]
ytrain <- y[ split ]
Xtest  <- X[-split, ]
ytest = y[-split]

# pvals = sapply(Xtrain, function(d) t.test(d ~ ytrain)$p.value)
library(parallel)
cl <- makeCluster(8)
pvals = parSapply(cl, Xtrain, function(d, ytrain) t.test(d ~ ytrain)$p.value, ytrain)
Xtrain = Xtrain[, which(pvals <= .05)]
print(dim(Xtrain))

keep_me = sapply(colnames(Xtrain), function(d) which(colnames(Xtest) == d))
Xtest = Xtest[, keep_me]

pp <- preProcess(Xtrain, method = c('BoxCox', 'center', 'scale', 'pca'), thresh=.9)
filtXtrain<- predict(pp, Xtrain)
filtXtest <- predict(pp, Xtest)
print(dim(filtXtrain))

tuneLength=10
set.seed(myseed)
index <- createMultiFolds(ytrain, k = 5, times = 5)

set.seed(myseed)
fullCtrl <- trainControl(method = "repeatedcv",
                         index = index,
                         savePredictions="final",
                         classProbs=TRUE,
                         summaryFunction=twoClassSummary)

library(caretEnsemble)
model_list <- caretList(
  filtXtrain, ytrain,
  tuneLength=10,
  trControl=fullCtrl,
  metric='ROC',
  methodList=c('rf', 'kernelpls', 'svmRadial', 'knn', 'rpart', 'bagEarthGCV', 'LogitBoost', 'lda', 'nb')
  )

greedy_ensemble <- caretEnsemble(
  model_list,
  metric='ROC',
  trControl=trainControl(
    number=2,
    summaryFunction=twoClassSummary,
    classProbs=TRUE
    ))
# ROC stats
summary(greedy_ensemble)
model_preds <- lapply(model_list, function(d, x, y) eval_model(d, x, y, c('ADHD', 'NV'))['ROC'], filtXtest, ytest)
model_preds <- data.frame(model_preds)
print(model_preds)
# sensitivity stats
model_preds <- lapply(model_list, function(d, x, y) eval_model(d, x, y, c('ADHD', 'NV'))['Sens'], filtXtest, ytest)
model_preds <- data.frame(model_preds)
print(model_preds)
# specificity stats
model_preds <- lapply(model_list, function(d, x, y) eval_model(d, x, y, c('ADHD', 'NV'))['Spec'], filtXtest, ytest)
model_preds <- data.frame(model_preds)
print(model_preds)
# Accuracy stats
model_preds <- lapply(model_list, predict, newdata=filtXtest)
model_preds <- lapply(model_preds, function(d, obs) postResample(d, obs)[1], obs=ytest)
model_preds <- data.frame(model_preds)
print(model_preds)
print(sprintf('No information rate: Accuracy=%f', max(table(ytrain)/length(ytrain))))
model_preds <- lapply(model_list, predict, newdata=filtXtest)
model_preds <- lapply(model_preds, function(d) confusionMatrix(d, ref=ytest)$overall['AccuracyPValue'])
model_preds <- data.frame(model_preds)
print(model_preds)
```

```
> summary(greedy_ensemble)
The following models were ensembled: rf, kernelpls, svmRadial, knn, rpart, bagEarthGCV, LogitBoost, lda, nb 
They were weighted: 
-6.8705 -2.7635 13.5016 1.6603 2.0818 -0.2614 -0.3467 0.3188 -1.0627 0.1505
The resulting ROC is: 0.8618
The fit for each individual model on the ROC is: 
      method       ROC      ROCSD
          rf 0.7719886 0.05846680
   kernelpls 0.8667848 0.06455004
   svmRadial 0.7975401 0.05057801
         knn 0.8093316 0.07463702
       rpart 0.6169586 0.06897069
 bagEarthGCV 0.8080214 0.04359728
  LogitBoost 0.7297961 0.08621376
         lda 0.7729078 0.05976948
          nb 0.7198864 0.06077537
> print(model_preds)
     rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
ROC 0.7 0.6425926 0.5351852 0.5768519 0.5842593   0.7259259  0.7027778 0.6351852 0.5777778
> print(model_preds)
            rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
Sens 0.7037037 0.6666667 0.4444444 0.3703704 0.5185185   0.6666667  0.6296296 0.5925926 0.5185185
> print(model_preds)
      rf kernelpls svmRadial knn rpart bagEarthGCV LogitBoost  lda   nb
Spec 0.4       0.7       0.6 0.8  0.65         0.7       0.65 0.65 0.65
> print(model_preds)
                rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
Accuracy 0.5744681 0.6808511 0.5106383 0.5531915 0.5744681   0.6808511  0.6382979 0.6170213 0.5744681
[1] "No information rate: Accuracy=0.567010"
> print(model_preds)
                      rf  kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
AccuracyPValue 0.5613819 0.09089091 0.8490277 0.6728609 0.5613819  0.09089091  0.2316227 0.3314565 0.5613819
```
Just wondering if it makes it better to cut off at p<.01. 
```
> summary(greedy_ensemble)
The following models were ensembled: rf, kernelpls, svmRadial, knn, rpart, bagEarthGCV, LogitBoost, lda, nb 
They were weighted: 
-4.744 0.712 4.7562 -2.6149 3.2751 0.0836 -0.386 -0.1489 2.6493 1.2208
The resulting ROC is: 0.8424
The fit for each individual model on the ROC is: 
      method       ROC      ROCSD
          rf 0.7464572 0.06987908
   kernelpls 0.8203075 0.05421284
   svmRadial 0.7860160 0.07222984
         knn 0.8058656 0.07524555
       rpart 0.6055147 0.10623108
 bagEarthGCV 0.7722794 0.06109415
  LogitBoost 0.7175401 0.07235192
         lda 0.8037500 0.08030333
          nb 0.7717781 0.05672598
> print(model_preds)
           rf kernelpls svmRadial      knn     rpart bagEarthGCV LogitBoost       lda        nb
ROC 0.5555556 0.6925926 0.5481481 0.587037 0.5712963   0.5851852  0.6018519 0.5388889 0.5833333
> print(model_preds)
            rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
Sens 0.7407407 0.6666667 0.4444444 0.4814815 0.6296296   0.5185185  0.6296296 0.5925926 0.5555556
> print(model_preds)
      rf kernelpls svmRadial knn rpart bagEarthGCV LogitBoost  lda   nb
Spec 0.3      0.65       0.6 0.5   0.5         0.5       0.55 0.55 0.55
> print(model_preds)
                rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
Accuracy 0.5531915 0.6595745 0.5106383 0.4893617 0.5744681   0.5106383  0.5957447 0.5744681 0.5531915
[1] "No information rate: Accuracy=0.567010"
> print(model_preds)
                      rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
AccuracyPValue 0.6728609 0.1507574 0.8490277 0.9072319 0.5613819   0.8490277   0.444329 0.5613819 0.6728609
```
It might get a bit worse, actually, if judging by ROC.

## Builtin feature selection

# Area
```{r}
source('~/ncr_notebooks/baseline_prediction/src/aux_functions.R')
struct_data = read.csv('~/data/baseline_prediction/stripped/structural.csv')
load('~/data/baseline_prediction/struct_area.rData')
# the first column of lh and rh is an index variable
vdata = cbind(struct_data$Mask.ID...Scan,
              lh_area[,2:ncol(lh_area)],
              rh_area[,2:ncol(rh_area)])
rm(lh_area)
rm(rh_area)
gf_fname = '~/data/baseline_prediction/stripped/clinical.csv'
gf = read.csv(gf_fname)
gf_base = gf[gf$BASELINE=='BASELINE', ]
my_ids = intersect(gf_base$MRN, struct_data$MRN)
mstruct = mergeOnClosestDate(gf_base, struct_data, my_ids)
rm_me = abs(mstruct$dateX.minus.dateY.months) > 12
mstruct = mstruct[!rm_me, ]
struct_base_vdata = merge(mstruct$Mask.ID...Scan, vdata, by.x=1, by.y=1, all.y=F, all.x=T)
rm(vdata)
```

## Univariate filters
I was getting 70K out of the 320K variables significant at .05, so I had to reduce it to .01 otherwise I couldn't compute the correlation matrix. At .01 I get closer to 26K variables, and down to 59 after removing correlated ones.
```
The following models were ensembled: rf, kernelpls, svmRadial, knn, rpart, bagEarthGCV, LogitBoost, lda, nb 
They were weighted: 
7.9121 10.1706 -25.2372 -1.0408 -0.8392 -0.8276 -0.0889 0.6438 1.8808 -0.288
The resulting Accuracy is: 0.6848
The fit for each individual model on the Accuracy is: 
      method  Accuracy AccuracySD
          rf 0.6000540 0.06127191
   kernelpls 0.6414305 0.05658469
   svmRadial 0.6352227 0.05760148
         knn 0.6299325 0.05882619
       rpart 0.6002969 0.08863066
 bagEarthGCV 0.5700945 0.04838709
  LogitBoost 0.5196221 0.08461516
         lda 0.4937112 0.06274691
          nb 0.6485020 0.06116605
> model_preds <- lapply(model_list, predict, newdata=Xtest)
Warning message:
In FUN(X[[i]], ...) :
  Numerical 0 probability for all classes with observation 7
> model_preds <- lapply(model_preds, function(d, obs) postResample(d, obs)[1], obs=ytest)
> model_preds <- data.frame(model_preds)
> ens_preds <- predict(greedy_ensemble, newdata=Xtest)
Warning message:
In FUN(X[[i]], ...) :
  Numerical 0 probability for all classes with observation 7
> model_preds$greedyE <- postResample(ens_preds, ytest)[1]
> ens_preds <- predict(glm_ensemble, newdata=Xtest)
Warning message:
In FUN(X[[i]], ...) :
  Numerical 0 probability for all classes with observation 7
> model_preds$glmE <- postResample(ens_preds, ytest)[1]
> ens_preds <- predict(gbm_ensemble, newdata=Xtest)
Warning message:
In FUN(X[[i]], ...) :
  Numerical 0 probability for all classes with observation 7
> model_preds$gbmE <- postResample(ens_preds, ytest)[1]
> print(model_preds)
                rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb   greedyE      glmE      gbmE
Accuracy 0.6808511 0.5957447 0.6170213 0.5957447 0.6595745   0.6382979  0.5744681 0.5957447 0.6382979 0.3829787 0.3829787 0.3617021
> print(sprintf('No information rate: Accuracy=%f', max(table(ytrain)/length(ytrain))))
[1] "No information rate: Accuracy=0.567010"
```
Here we get somewhat decent results with rf and rpart. I wonder if they would complement each other. If we do PCA, we go from 69K significant variables at .05 to 73. 
```
> summary(greedy_ensemble)
The following models were ensembled: rf, kernelpls, svmRadial, knn, rpart, bagEarthGCV, LogitBoost, lda, nb 
They were weighted: 
3.9461 1.5273 -12.4434 1.7709 -0.5249 0.0968 0.8886 0.1227 0.9059 0.204
The resulting Accuracy is: 0.6122
The fit for each individual model on the Accuracy is: 
      method  Accuracy AccuracySD
          rf 0.5566802 0.06638913
   kernelpls 0.6403239 0.05016658
   svmRadial 0.5660189 0.02439334
         knn 0.6371660 0.07639957
       rpart 0.6104993 0.04552918
 bagEarthGCV 0.5680972 0.08381546
  LogitBoost 0.5103104 0.07369730
         lda 0.4855601 0.06313611
          nb 0.4948988 0.08321329
> model_preds <- lapply(model_list, predict, newdata=filtXtest)
There were 47 warnings (use warnings() to see them)
> model_preds <- lapply(model_preds, function(d, obs) postResample(d, obs)[1], obs=ytest)
> model_preds <- data.frame(model_preds)
> ens_preds <- predict(greedy_ensemble, newdata=filtXtest)
There were 47 warnings (use warnings() to see them)
> model_preds$greedyE <- postResample(ens_preds, ytest)[1]
> ens_preds <- predict(glm_ensemble, newdata=filtXtest)
There were 47 warnings (use warnings() to see them)
> model_preds$glmE <- postResample(ens_preds, ytest)[1]
> ens_preds <- predict(gbm_ensemble, newdata=filtXtest)
There were 47 warnings (use warnings() to see them)
> model_preds$gbmE <- postResample(ens_preds, ytest)[1]
> print(model_preds)
                rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb   greedyE      glmE      gbmE
Accuracy 0.6808511 0.5957447 0.4255319 0.7021277 0.5744681   0.6595745  0.6382979 0.6595745 0.6170213 0.3617021 0.3617021 0.3829787
> print(sprintf('No information rate: Accuracy=%f', max(table(ytrain)/length(ytrain))))
[1] "No information rate: Accuracy=0.567010"
```
We seem to generalize better using PCA. rf and knn seem to do particularly well.

## Builtin feature selection

# Volume
```{r}
source('~/ncr_notebooks/baseline_prediction/src/aux_functions.R')
struct_data = read.csv('~/data/baseline_prediction/stripped/structural.csv')
load('~/data/baseline_prediction/struct_volume.rData')
# the first column of lh and rh is an index variable
vdata = cbind(volume_ids,
              lh_volume[,2:ncol(lh_volume)],
              rh_volume[,2:ncol(rh_volume)])
rm(lh_volume)
rm(rh_volume)
gf_fname = '~/data/baseline_prediction/stripped/clinical.csv'
gf = read.csv(gf_fname)
gf_base = gf[gf$BASELINE=='BASELINE', ]
my_ids = intersect(gf_base$MRN, struct_data$MRN)
mstruct = mergeOnClosestDate(gf_base, struct_data, my_ids)
rm_me = abs(mstruct$dateX.minus.dateY.months) > 12
mstruct = mstruct[!rm_me, ]
keep_me = c()
keep_mstruct = c()
for (i in 1:nrow(vdata)) {
  if (vdata[i, 1] %in% mstruct$Mask.ID...Scan) {
    keep_me = c(keep_me, i)
    keep_mstruct = c(keep_mstruct, which(mstruct$Mask.ID...Scan == vdata[i, 1]))
  }
}
struct_base_vdata = vdata[keep_me, ]
mstruct = mstruct[keep_mstruct, ]
rm(vdata)
```

## Univariate filters
I was getting 90K out of the 320K variables significant at .05, so I had to reduce it to .01 otherwise I couldn't compute the correlation matrix. At .01 I get it down to 34K, which is a bit more maneageble, and only 127 after removing correlated ones.
```
> summary(greedy_ensemble)
The following models were ensembled: rf, kernelpls, svmRadial, knn, rpart, bagEarthGCV, LogitBoost, lda, nb 
They were weighted: 
5.6698 -4.841 -14.7526 5.1806 -0.6719 0.0664 3.7901 0.3214 0.6186 -0.8784
The resulting Accuracy is: 0.6736
The fit for each individual model on the Accuracy is: 
      method  Accuracy AccuracySD
          rf 0.6387584 0.06341041
   kernelpls 0.6470715 0.07334973
   svmRadial 0.6190013 0.06232832
         knn 0.6231849 0.05463854
       rpart 0.5647503 0.07432835
 bagEarthGCV 0.5617274 0.08979624
  LogitBoost 0.5303644 0.08655079
         lda 0.4523077 0.05938073
          nb 0.6438596 0.06621545
> model_preds <- lapply(model_list, predict, newdata=Xtest)
There were 47 warnings (use warnings() to see them)
> model_preds <- lapply(model_preds, function(d, obs) postResample(d, obs)[1], obs=ytest)
> model_preds <- data.frame(model_preds)
> ens_preds <- predict(greedy_ensemble, newdata=Xtest)
There were 47 warnings (use warnings() to see them)
> model_preds$greedyE <- postResample(ens_preds, ytest)[1]
> ens_preds <- predict(glm_ensemble, newdata=Xtest)
There were 47 warnings (use warnings() to see them)
> model_preds$glmE <- postResample(ens_preds, ytest)[1]
> ens_preds <- predict(gbm_ensemble, newdata=Xtest)
There were 47 warnings (use warnings() to see them)
> model_preds$gbmE <- postResample(ens_preds, ytest)[1]
> print(model_preds)
                rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb   greedyE      glmE      gbmE
Accuracy 0.5319149 0.5319149 0.5106383 0.4255319 0.4255319   0.5319149  0.5319149 0.6382979 0.5319149 0.4893617 0.4893617 0.4680851
> print(sprintf('No information rate: Accuracy=%f', max(table(ytrain)/length(ytrain))))
[1] "No information rate: Accuracy=0.572917"
```
Not great... lda seems to have a nice generalization, but you wouldn't pick it out of training.

PCA takes us from 89K variables to 103.

```
> summary(greedy_ensemble)
The following models were ensembled: rf, kernelpls, svmRadial, knn, rpart, bagEarthGCV, LogitBoost, lda, nb 
They were weighted: 
5.7292 6.0259 -15.8784 -3.5669 -0.0506 -0.1954 1.4039 0.052 0.4595 0.7945
The resulting Accuracy is: 0.6984
The fit for each individual model on the Accuracy is: 
      method  Accuracy AccuracySD
          rf 0.5468556 0.02674489
   kernelpls 0.6326316 0.07722953
   svmRadial 0.6020513 0.05456565
         knn 0.5771660 0.07581487
       rpart 0.5802699 0.05699641
 bagEarthGCV 0.5271795 0.06233948
  LogitBoost 0.5104453 0.08552396
         lda 0.4368421 0.06097659
          nb 0.4205668 0.07336499
> model_preds <- lapply(model_list, predict, newdata=filtXtest)
There were 47 warnings (use warnings() to see them)
> model_preds <- lapply(model_preds, function(d, obs) postResample(d, obs)[1], obs=ytest)
> model_preds <- data.frame(model_preds)
> ens_preds <- predict(greedy_ensemble, newdata=filtXtest)
There were 47 warnings (use warnings() to see them)
> model_preds$greedyE <- postResample(ens_preds, ytest)[1]
> ens_preds <- predict(glm_ensemble, newdata=filtXtest)
There were 47 warnings (use warnings() to see them)
> model_preds$glmE <- postResample(ens_preds, ytest)[1]
> ens_preds <- predict(gbm_ensemble, newdata=filtXtest)
There were 47 warnings (use warnings() to see them)
> model_preds$gbmE <- postResample(ens_preds, ytest)[1]
> print(model_preds)
                rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb   greedyE      glmE      gbmE
Accuracy 0.5744681 0.5319149 0.5319149 0.4042553 0.4680851   0.5106383  0.4680851 0.4893617 0.5531915 0.4893617 0.4893617 0.4468085
> print(sprintf('No information rate: Accuracy=%f', max(table(ytrain)/length(ytrain))))
[1] "No information rate: Accuracy=0.572917"
```
Volume doesn't seem to be getting much out of PCA either. It seems like not a good data domain, maybe stick to area and thickness? This is what I get maximizing ROC:

```
> summary(greedy_ensemble)
The following models were ensembled: rf, kernelpls, svmRadial, knn, rpart, bagEarthGCV, LogitBoost, lda, nb 
They were weighted: 
-8.0443 -4.033 14.4565 4.5288 2.4184 0.7021 -0.4691 -0.3744 -0.8528 0.433
The resulting ROC is: 0.7776
The fit for each individual model on the ROC is: 
      method       ROC      ROCSD
          rf 0.5333189 0.06959458
   kernelpls 0.6792380 0.07099921
   svmRadial 0.6657219 0.06545432
         knn 0.6221758 0.08199017
       rpart 0.6091277 0.08279658
 bagEarthGCV 0.5592313 0.08482214
  LogitBoost 0.4744318 0.06910360
         lda 0.3572594 0.06723027
          nb 0.4889171 0.05480865
> print(model_preds)
           rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
ROC 0.6462963 0.6444444 0.3555556 0.6231481 0.5555556   0.6814815  0.6231481 0.6703704 0.5703704
> print(model_preds)
            rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
Sens 0.7407407 0.7777778 0.7037037 0.8888889 0.5185185   0.7407407  0.7777778 0.8148148 0.8148148
> print(model_preds)
      rf kernelpls svmRadial  knn rpart bagEarthGCV LogitBoost  lda   nb
Spec 0.5      0.45       0.2 0.35   0.6        0.45       0.45 0.45 0.15
> print(model_preds)
                rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
Accuracy 0.6382979 0.6382979 0.4893617 0.6382979 0.5531915   0.6170213  0.6382979 0.6595745 0.5319149
[1] "No information rate: Accuracy=0.572917"
> print(model_preds)
                      rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda       nb
AccuracyPValue 0.2316227 0.2316227 0.9072319 0.2316227 0.6728609   0.3314565  0.2316227 0.1507574 0.770452
```
Nothing particularly earth-shattering either.

## Builtin feature selection

# DTI, FA
```{r}
source('~/ncr_notebooks/baseline_prediction/src/aux_functions.R')
tract_data = read.csv('~/data/baseline_prediction/stripped/dti.csv')
load('~/data/baseline_prediction/dti/fa_voxelwise.RData')
dti_vdata = cbind(tract_data$maskid, fa_data)
gf_fname = '~/data/baseline_prediction/stripped/clinical.csv'
gf = read.csv(gf_fname)
gf_base = gf[gf$BASELINE=='BASELINE', ]
my_ids = intersect(gf_base$MRN, tract_data$MRN)
merged = mergeOnClosestDate(gf_base, tract_data, my_ids)
rm_me = abs(merged$dateX.minus.dateY.months) > 12
merged = merged[!rm_me, ]
dti_base_vdata = merge(merged$maskid, dti_vdata, by.x=1, by.y=1, all.y=F, all.x=T)
```
## Univariate filtering
```{r}
X = dti_base_vdata[, 2:ncol(dti_base_vdata)]
rm_me = colSums(is.na(X)) > 0
X = X[, !rm_me]

keep_me = merged$age <= 12
X = X[keep_me, ]
y = merged$DX_BASELINE
y[y!='NV'] = 'ADHD'
y = factor(y, levels=c('NV', 'ADHD'))
y = y[keep_me]

myseed = 1234
default_preproc = c("center", 'scale')
set.seed(myseed)
split <- createDataPartition(y, p = .8, list = FALSE)
Xtrain <- X[ split, ]
ytrain <- y[ split ]
Xtest  <- X[-split, ]
ytest = y[-split]

# in voxel analysis it's actually more costly to do the correlation in the entire set of variables
# then to run the for loop, so let's first reduce the variables to only the univariate ones
pvals = sapply(Xtrain, function(d) t.test(d ~ ytrain)$p.value)
Xtrain = Xtrain[, which(pvals <= .05)]
print(dim(Xtrain))

# now we can remove correlated and non-informative variables
nzv = nearZeroVar(Xtrain)
print(nzv)
if (length(nzv) > 0) {
  Xtrain = Xtrain[, -nzv]
}
correlations = cor(Xtrain, use='na.or.complete')
# # library(corrplot)
# # corrplot(correlations, order = "hclust")
highCorr = findCorrelation(correlations, cutoff=.75)
print(length(highCorr))
if (length(highCorr) > 0) {
  Xtrain = Xtrain[, -highCorr]
}
print(dim(Xtrain))

keep_me = sapply(colnames(Xtrain), function(d) which(colnames(Xtest) == d))
Xtest = Xtest[, keep_me]

tuneLength=10
set.seed(myseed)
index <- createMultiFolds(ytrain, k = 5, times = 5)

set.seed(myseed)
fullCtrl <- trainControl(method = "repeatedcv",
                         index = index,
                         savePredictions="final",
                         classProbs=TRUE,
                         preProcOptions = c("center", 'scale'))

library(caretEnsemble)
model_list <- caretList(
  Xtrain, ytrain,
  tuneLength=10,
  trControl=fullCtrl,
  methodList=c('rf', 'kernelpls', 'svmRadial', 'knn', 'rpart', 'bagEarthGCV', 'LogitBoost', 'lda', 'nb')
  )

greedy_ensemble <- caretEnsemble(
  model_list, 
  trControl=trainControl(
    number=2
    ))

glm_ensemble <- caretStack(
  model_list,
  method="glm",
  trControl=trainControl(
    method="boot",
    number=10,
    savePredictions="final"
  )
)

gbm_ensemble <- caretStack(
  model_list,
  method="gbm",
  verbose=FALSE,
  tuneLength=10,
  trControl=trainControl(
    method="boot",
    number=10,
    savePredictions="final"
  )
)
summary(greedy_ensemble)
model_preds <- lapply(model_list, predict, newdata=Xtest)
model_preds <- lapply(model_preds, function(d, obs) postResample(d, obs)[1], obs=ytest)
model_preds <- data.frame(model_preds)
ens_preds <- predict(greedy_ensemble, newdata=Xtest)
model_preds$greedyE <- postResample(ens_preds, ytest)[1]
ens_preds <- predict(glm_ensemble, newdata=Xtest)
model_preds$glmE <- postResample(ens_preds, ytest)[1]
ens_preds <- predict(gbm_ensemble, newdata=Xtest)
model_preds$gbmE <- postResample(ens_preds, ytest)[1]
print(model_preds)
print(sprintf('No information rate: Accuracy=%f', max(table(ytrain)/length(ytrain))))
```

```
The following models were ensembled: rf, kernelpls, svmRadial, knn, rpart, bagEarthGCV, LogitBoost, lda, nb
They were weighted:
9.0871 15.894 -18.6064 -6.3478 -3.2751 -0.6511 2.1062 0.7845 -2.7625 -1.5392
The resulting Accuracy is: 0.91
The fit for each individual model on the Accuracy is:
      method  Accuracy AccuracySD
          rf 0.7683871 0.08216307
   kernelpls 0.9067339 0.04242475
   svmRadial 0.8901613 0.05725777
         knn 0.8274597 0.07139588
       rpart 0.5621774 0.08488504
 bagEarthGCV 0.7508871 0.08149768
  LogitBoost 0.7227823 0.08307771
         lda 0.8376210 0.04961405
          nb 0.7670161 0.07758786
> model_preds <- lapply(model_list, predict, newdata=Xtest)
Warning messages:
1: In FUN(X[[i]], ...) :
  Numerical 0 probability for all classes with observation 3
2: In FUN(X[[i]], ...) :
  Numerical 0 probability for all classes with observation 4
> model_preds <- lapply(model_preds, function(d, obs) postResample(d, obs)[1], obs=ytest)
> model_preds <- data.frame(model_preds)
> ens_preds <- predict(greedy_ensemble, newdata=Xtest)
Warning messages:
1: In FUN(X[[i]], ...) :
  Numerical 0 probability for all classes with observation 3
2: In FUN(X[[i]], ...) :
  Numerical 0 probability for all classes with observation 4
> model_preds$greedyE <- postResample(ens_preds, ytest)[1]
> ens_preds <- predict(glm_ensemble, newdata=Xtest)
Warning messages:
1: In FUN(X[[i]], ...) :
  Numerical 0 probability for all classes with observation 3
2: In FUN(X[[i]], ...) :
  Numerical 0 probability for all classes with observation 4
> model_preds$glmE <- postResample(ens_preds, ytest)[1]
> ens_preds <- predict(gbm_ensemble, newdata=Xtest)
Warning messages:
1: In FUN(X[[i]], ...) :
  Numerical 0 probability for all classes with observation 3
2: In FUN(X[[i]], ...) :
  Numerical 0 probability for all classes with observation 4
> model_preds$gbmE <- postResample(ens_preds, ytest)[1]
> print(model_preds)
                rf kernelpls svmRadial       knn     rpart bagEarthGCV
Accuracy 0.5641026 0.5897436 0.6153846 0.5384615 0.5897436   0.6153846
         LogitBoost       lda        nb   greedyE      glmE      gbmE
Accuracy  0.5641026 0.5641026 0.5897436 0.6666667 0.6666667 0.4102564
> print(sprintf('No information rate: Accuracy=%f', max(table(ytrain)/length(ytrain))))
[1] "No information rate: Accuracy=0.534591"
> 
```
As usual, DTI quite overfits it, so there's gotta be a way to fix this a bit. Maybe the PCA solution? 

```{r}
X = dti_base_vdata[, 2:ncol(dti_base_vdata)]
rm_me = colSums(is.na(X)) > 0
X = X[, !rm_me]

keep_me = merged$age <= 12
X = X[keep_me, ]
y = merged$DX_BASELINE
y[y!='NV'] = 'ADHD'
y = factor(y, levels=c('NV', 'ADHD'))
y = y[keep_me]

myseed = 1234
default_preproc = c("center", 'scale')
set.seed(myseed)
split <- createDataPartition(y, p = .8, list = FALSE)
Xtrain <- X[ split, ]
ytrain <- y[ split ]
Xtest  <- X[-split, ]
ytest = y[-split]

# in voxel analysis it's actually more costly to do the correlation in the entire set of variables
# then to run the for loop, so let's first reduce the variables to only the univariate ones
pvals = sapply(Xtrain, function(d) t.test(d ~ ytrain)$p.value)
Xtrain = Xtrain[, which(pvals <= .05)]
print(dim(Xtrain))

keep_me = sapply(colnames(Xtrain), function(d) which(colnames(Xtest) == d))
Xtest = Xtest[, keep_me]

pp <- preProcess(Xtrain, method = c('BoxCox', 'center', 'scale', 'pca'), thresh=.9)
filtXtrain<- predict(pp, Xtrain)
filtXtest <- predict(pp, Xtest)
print(dim(filtXtrain))

tuneLength=10
set.seed(myseed)
index <- createMultiFolds(ytrain, k = 5, times = 5)

set.seed(myseed)
fullCtrl <- trainControl(method = "repeatedcv",
                         index = index,
                         savePredictions="final",
                         classProbs=TRUE)

library(caretEnsemble)
model_list <- caretList(
  filtXtrain, ytrain,
  tuneLength=10,
  trControl=fullCtrl,
  methodList=c('rf', 'kernelpls', 'svmRadial', 'knn', 'rpart', 'bagEarthGCV', 'LogitBoost', 'lda', 'nb')
  )

greedy_ensemble <- caretEnsemble(
  model_list, 
  trControl=trainControl(
    number=2
    ))

glm_ensemble <- caretStack(
  model_list,
  method="glm",
  trControl=trainControl(
    method="boot",
    number=10,
    savePredictions="final"
  )
)

gbm_ensemble <- caretStack(
  model_list,
  method="gbm",
  verbose=FALSE,
  tuneLength=10,
  trControl=trainControl(
    method="boot",
    number=10,
    savePredictions="final"
  )
)
summary(greedy_ensemble)
model_preds <- lapply(model_list, predict, newdata=filtXtest)
model_preds <- lapply(model_preds, function(d, obs) postResample(d, obs)[1], obs=ytest)
model_preds <- data.frame(model_preds)
ens_preds <- predict(greedy_ensemble, newdata=filtXtest)
model_preds$greedyE <- postResample(ens_preds, ytest)[1]
ens_preds <- predict(glm_ensemble, newdata=filtXtest)
model_preds$glmE <- postResample(ens_preds, ytest)[1]
ens_preds <- predict(gbm_ensemble, newdata=filtXtest)
model_preds$gbmE <- postResample(ens_preds, ytest)[1]
print(model_preds)
print(sprintf('No information rate: Accuracy=%f', max(table(ytrain)/length(ytrain))))
```
Using PCA we go from 1251 good variables at .05 to 106. 
```
> summary(greedy_ensemble)
The following models were ensembled: rf, kernelpls, svmRadial, knn, rpart, bagEarthGCV, LogitBoost, lda, nb
They were weighted:
9.4559 4.7429 -23.8689 5.5002 -1.3188 -1.9296 -0.5221 -0.3011 -0.2076 -0.498
The resulting Accuracy is: 0.8636
The fit for each individual model on the Accuracy is:
      method  Accuracy AccuracySD
          rf 0.7281855 0.08362475
   kernelpls 0.8639516 0.05336912
   svmRadial 0.7082258 0.06965322
         knn 0.7533468 0.07695189
       rpart 0.7356855 0.08848171
 bagEarthGCV 0.7381452 0.07494294
  LogitBoost 0.6995161 0.07033789
         lda 0.6765323 0.08063745
          nb 0.6375806 0.08854109
> model_preds <- lapply(model_list, predict, newdata=filtXtest)
Error in predict.randomForest(modelFit, newdata) :
  NA/NaN/Inf in foreign function call (arg 7)
> model_preds <- lapply(model_preds, function(d, obs) postResample(d, obs)[1], obs=ytest)
> model_preds <- data.frame(model_preds)
> ens_preds <- predict(greedy_ensemble, newdata=filtXtest)
Error in predict.randomForest(modelFit, newdata, type = "prob") :
  NA/NaN/Inf in foreign function call (arg 7)
> model_preds$greedyE <- postResample(ens_preds, ytest)[1]
> ens_preds <- predict(glm_ensemble, newdata=filtXtest)
Error in predict.randomForest(modelFit, newdata, type = "prob") :
  NA/NaN/Inf in foreign function call (arg 7)
> model_preds$glmE <- postResample(ens_preds, ytest)[1]
> ens_preds <- predict(gbm_ensemble, newdata=filtXtest)
Error in predict.randomForest(modelFit, newdata, type = "prob") :
  NA/NaN/Inf in foreign function call (arg 7)
> model_preds$gbmE <- postResample(ens_preds, ytest)[1]
> print(model_preds)
                rf kernelpls svmRadial       knn     rpart bagEarthGCV
Accuracy 0.5128205 0.7435897 0.7179487 0.5897436 0.5128205   0.6410256
         LogitBoost       lda        nb   greedyE      glmE      gbmE
Accuracy  0.4358974 0.7179487 0.7435897 0.3333333 0.3333333 0.3333333
> print(sprintf('No information rate: Accuracy=%f', max(table(ytrain)/length(ytrain))))
[1] "No information rate: Accuracy=0.534591"
```
Now we overfit a bit less, and generalize better, especially using PLS, SVM, lda, and nb.

```{r}
X = dti_base_vdata[, 2:ncol(dti_base_vdata)]
rm_me = colSums(is.na(X)) > 0
X = X[, !rm_me]

keep_me = merged$age <= 12
X = X[keep_me, ]
y = merged$DX_BASELINE
y[y!='NV'] = 'ADHD'
# twoClassSummary uses the first level as the case of interest (https://topepo.github.io/caret/measuring-performance.html#measures-for-class-probabilities)
y = factor(y, levels=c('ADHD', 'NV'))
y = y[keep_me]

myseed = 1234
set.seed(myseed)
split <- createDataPartition(y, p = .8, list = FALSE)
Xtrain <- X[ split, ]
ytrain <- y[ split ]
Xtest  <- X[-split, ]
ytest = y[-split]

library(parallel)
cl <- makeCluster(8)
pvals = parSapply(cl, Xtrain, function(d, ytrain) t.test(d ~ ytrain)$p.value, ytrain)
Xtrain = Xtrain[, which(pvals <= .05)]
print(dim(Xtrain))

keep_me = sapply(colnames(Xtrain), function(d) which(colnames(Xtest) == d))
Xtest = Xtest[, keep_me]

pp <- preProcess(Xtrain, method = c('BoxCox', 'center', 'scale', 'pca'), thresh=.9)
filtXtrain<- predict(pp, Xtrain)
filtXtest <- predict(pp, Xtest)
print(dim(filtXtrain))

tuneLength=10
set.seed(myseed)
index <- createMultiFolds(ytrain, k = 5, times = 5)

set.seed(myseed)
fullCtrl <- trainControl(method = "repeatedcv",
                         index = index,
                         savePredictions="final",
                         classProbs=TRUE,
                         summaryFunction=twoClassSummary)

library(caretEnsemble)
model_list <- caretList(
  filtXtrain, ytrain,
  tuneLength=10,
  trControl=fullCtrl,
  metric='ROC',
  methodList=c('rf', 'kernelpls', 'svmRadial', 'knn', 'rpart', 'bagEarthGCV', 'LogitBoost', 'lda', 'nb')
  )

greedy_ensemble <- caretEnsemble(
  model_list,
  metric='ROC',
  trControl=trainControl(
    number=2,
    summaryFunction=twoClassSummary,
    classProbs=TRUE
    ))
# ROC stats
summary(greedy_ensemble)
model_preds <- lapply(model_list, function(d, x, y) eval_model(d, x, y, c('ADHD', 'NV'))['ROC'], filtXtest, ytest)
model_preds <- data.frame(model_preds)
print(model_preds)
# sensitivity stats
model_preds <- lapply(model_list, function(d, x, y) eval_model(d, x, y, c('ADHD', 'NV'))['Sens'], filtXtest, ytest)
model_preds <- data.frame(model_preds)
print(model_preds)
# specificity stats
model_preds <- lapply(model_list, function(d, x, y) eval_model(d, x, y, c('ADHD', 'NV'))['Spec'], filtXtest, ytest)
model_preds <- data.frame(model_preds)
print(model_preds)
# Accuracy stats
model_preds <- lapply(model_list, predict, newdata=filtXtest)
model_preds <- lapply(model_preds, function(d, obs) postResample(d, obs)[1], obs=ytest)
model_preds <- data.frame(model_preds)
print(model_preds)
print(sprintf('No information rate: Accuracy=%f', max(table(ytrain)/length(ytrain))))
model_preds <- lapply(model_list, predict, newdata=filtXtest)
model_preds <- lapply(model_preds, function(d) confusionMatrix(d, ref=ytest)$overall['AccuracyPValue'])
model_preds <- data.frame(model_preds)
print(model_preds)
```

```
> summary(greedy_ensemble)
The following models were ensembled: rf, kernelpls, svmRadial, knn, rpart, bagEarthGCV, LogitBoost, lda, nb
They were weighted:
-4.949 -9.7783 13.5318 -3.324 4.2696 3.068 0.3514 -0.4207 -0.0082 0.6927
The resulting ROC is: 0.846
The fit for each individual model on the ROC is:
      method       ROC      ROCSD
          rf 0.7344874 0.09255985
   kernelpls 0.8442353 0.06379635
   svmRadial 0.7550476 0.08657993
         knn 0.8096303 0.08033791
       rpart 0.7148796 0.08893351
 bagEarthGCV 0.7475966 0.06702979
  LogitBoost 0.6971092 0.09380428
         lda 0.7158095 0.09112430
          nb 0.6761569 0.08650556
> print(model_preds)
           rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost
ROC 0.6600529 0.6613757 0.6693122 0.5965608 0.5714286   0.5952381  0.6626984
          lda        nb
ROC 0.6746032 0.6507937
> print(model_preds)
            rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost
Sens 0.4285714 0.4761905 0.3809524 0.1904762 0.4761905   0.2857143  0.4761905
           lda        nb
Sens 0.4761905 0.0952381
> print(model_preds)
            rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost
Spec 0.7222222 0.7222222 0.7777778 0.8333333 0.6666667   0.8888889  0.7222222
           lda        nb
Spec 0.7222222 0.9444444
> print(model_preds)
                rf kernelpls svmRadial       knn     rpart bagEarthGCV
Accuracy 0.5641026 0.5897436 0.5641026 0.4871795 0.5641026   0.5641026
         LogitBoost       lda        nb
Accuracy  0.5897436 0.5897436 0.4871795
[1] "No information rate: Accuracy=0.534591"
> print(model_preds)  
                      rf kernelpls svmRadial       knn     rpart bagEarthGCV
AccuracyPValue 0.4379397 0.3164087 0.4379397 0.7892616 0.4379397   0.4379397
               LogitBoost       lda        nb
AccuracyPValue  0.3164087 0.3164087 0.7892616

```

## Builtin feature selection

# DTI, AD
```{r}
source('~/ncr_notebooks/baseline_prediction/src/aux_functions.R')
tract_data = read.csv('~/data/baseline_prediction/stripped/dti.csv')
load('~/data/baseline_prediction/dti/ad_voxelwise.RData')
dti_vdata = cbind(tract_data$maskid, ad_data)
gf_fname = '~/data/baseline_prediction/stripped/clinical.csv'
gf = read.csv(gf_fname)
gf_base = gf[gf$BASELINE=='BASELINE', ]
my_ids = intersect(gf_base$MRN, tract_data$MRN)
merged = mergeOnClosestDate(gf_base, tract_data, my_ids)
rm_me = abs(merged$dateX.minus.dateY.months) > 12
merged = merged[!rm_me, ]
dti_base_vdata = merge(merged$maskid, dti_vdata, by.x=1, by.y=1, all.y=F, all.x=T)
```
## Univariate filtering
```
> summary(greedy_ensemble)
The following models were ensembled: rf, kernelpls, svmRadial, knn, rpart, bagEarthGCV, LogitBoost, lda, nb
They were weighted:
8.6561 -4.3758 -2.639 -5.1809 -0.7389 1.9485 3.9931 0.1585 -4.5103 -3.5427
The resulting Accuracy is: 0.9809
The fit for each individual model on the Accuracy is:
      method  Accuracy AccuracySD
          rf 0.9118952 0.04355628
   kernelpls 0.9647177 0.02286922
   svmRadial 0.9785081 0.02194079
         knn 0.9218952 0.04563197
       rpart 0.5407258 0.09300408
 bagEarthGCV 0.7998387 0.07169557
  LogitBoost 0.7297177 0.07402089
         lda 0.9470565 0.03638601
          nb 0.9583871 0.02846707
> model_preds <- lapply(model_list, predict, newdata=Xtest)
Warning messages:
1: In FUN(X[[i]], ...) :
  Numerical 0 probability for all classes with observation 3
2: In FUN(X[[i]], ...) :
  Numerical 0 probability for all classes with observation 4
3: In FUN(X[[i]], ...) :
  Numerical 0 probability for all classes with observation 10
> model_preds <- lapply(model_preds, function(d, obs) postResample(d, obs)[1], obs=ytest)
> model_preds <- data.frame(model_preds)
> ens_preds <- predict(greedy_ensemble, newdata=Xtest)
Warning messages:
1: In FUN(X[[i]], ...) :
  Numerical 0 probability for all classes with observation 3
2: In FUN(X[[i]], ...) :
  Numerical 0 probability for all classes with observation 4
3: In FUN(X[[i]], ...) :
> model_preds$gbmE <- postResample(ens_preds, ytest)[1]
> print(model_preds)
                rf kernelpls svmRadial       knn     rpart bagEarthGCV
Accuracy 0.6666667 0.7179487 0.7692308 0.6410256 0.5128205   0.6153846
         LogitBoost       lda        nb   greedyE      glmE      gbmE
Accuracy  0.5128205 0.5897436 0.7948718 0.2631579 0.2631579 0.1794872
> print(sprintf('No information rate: Accuracy=%f', max(table(ytrain)/length(ytrain))))
[1] "No information rate: Accuracy=0.534591"
```
Same as above using FA. Now, if we do the PCA filtering we go from 779 to 107 variables:
```
> summary(greedy_ensemble)
The following models were ensembled: rf, kernelpls, svmRadial, knn, rpart, bagEarthGCV, LogitBoost, lda, nb 
They were weighted: 
33.9237 10.6624 -59.2815 13.215 -13.9299 -1.1419 -17.842 -1.1993 -0.4465 5.7821
The resulting Accuracy is: 0.9663
The fit for each individual model on the Accuracy is: 
      method  Accuracy AccuracySD
          rf 0.8917742 0.06544653
   kernelpls 0.9734677 0.02703468
   svmRadial 0.8077016 0.08561641
         knn 0.9483871 0.03119711
       rpart 0.8640323 0.05894184
 bagEarthGCV 0.9458065 0.05053601
  LogitBoost 0.8829435 0.05432049
         lda 0.7798790 0.08292205
          nb 0.7630645 0.07872777
> model_preds <- lapply(model_list, predict, newdata=filtXtest)
There were 39 warnings (use warnings() to see them)
> model_preds <- lapply(model_preds, function(d, obs) postResample(d, obs)[1], obs=ytest)
> model_preds <- data.frame(model_preds)
> ens_preds <- predict(greedy_ensemble, newdata=filtXtest)
There were 39 warnings (use warnings() to see them)
> model_preds$greedyE <- postResample(ens_preds, ytest)[1]
> ens_preds <- predict(glm_ensemble, newdata=filtXtest)
There were 39 warnings (use warnings() to see them)
> model_preds$glmE <- postResample(ens_preds, ytest)[1]
> ens_preds <- predict(gbm_ensemble, newdata=filtXtest)
There were 39 warnings (use warnings() to see them)
> model_preds$gbmE <- postResample(ens_preds, ytest)[1]
> print(model_preds)
                rf kernelpls svmRadial       knn     rpart bagEarthGCV
Accuracy 0.5128205 0.7435897 0.7179487 0.5897436 0.5128205   0.6410256
         LogitBoost       lda        nb   greedyE      glmE      gbmE
Accuracy  0.4358974 0.7179487 0.7435897 0.4871795 0.4871795 0.3333333
> print(sprintf('No information rate: Accuracy=%f', max(table(ytrain)/length(ytrain))))
[1] "No information rate: Accuracy=0.534591"
```
We actually did a bit better without the PCA. I wonder how well we can do if we maximize ROC, as Philip said we need a minimum specificity and sensitivity of .8. 

```
> summary(greedy_ensemble)
The following models were ensembled: rf, kernelpls, svmRadial, knn, rpart, bagEarthGCV, LogitBoost, lda, nb
They were weighted:
-15.2847 5.0599 25.0817 -2.6874 4.693 0.069 -2.7622 0.6201 0.1594 0.4355
The resulting ROC is: 0.9853
The fit for each individual model on the ROC is:
      method       ROC      ROCSD
          rf 0.9657759 0.02916561
   kernelpls 0.9836863 0.01701787
   svmRadial 0.8782745 0.05239643
         knn 0.9664874 0.02496917
       rpart 0.9080224 0.04176572
 bagEarthGCV 0.9691765 0.02479578
  LogitBoost 0.9541064 0.03195763
         lda 0.8275742 0.06684562
          nb 0.7949356 0.06302168
> print(model_preds)
           rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost
ROC 0.6785714 0.7248677 0.7433862 0.5886243 0.5833333   0.6666667  0.6362434
          lda        nb
ROC 0.7380952 0.7328042
> print(model_preds)
            rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost
Sens 0.4285714 0.6666667 0.6190476 0.4761905 0.5238095   0.5238095  0.4761905
           lda        nb
Sens 0.6190476 0.6190476
> print(model_preds)
            rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost
Spec 0.7777778 0.8333333 0.7777778 0.6111111 0.6666667   0.8333333  0.6666667
           lda        nb
Spec 0.6666667 0.7222222
> print(model_preds)
                rf kernelpls svmRadial       knn     rpart bagEarthGCV
Accuracy 0.5897436 0.7435897 0.6923077 0.5384615 0.5897436   0.6666667
         LogitBoost       lda        nb
Accuracy  0.5641026 0.6410256 0.6666667
[1] "No information rate: Accuracy=0.534591"
> print(model_preds)
                      rf   kernelpls  svmRadial      knn     rpart bagEarthGCV
AccuracyPValue 0.3164087 0.006998213 0.03719967 0.565258 0.3164087  0.07304316
               LogitBoost       lda         nb
AccuracyPValue  0.4379397 0.1301001 0.07304316

```
Now, these are some results I can start getting behind. AUC of svmRadial is good and both spe/sen are decent. PLS is not doing badly either. 

Just to be clear, sensitivity here reflects the "interesting case", which is the first factor, ADHD. Specificity relates to the non-case, NV, being called a NV. In other words, sensitivity is the ability of a test to correctly identify those with ADHD (true positive rate), whereas specificity is the ability of the test to correctly identify the NVs (true negative rate).

Does it get better by using smaller p-values?
Does it get better by combining different classifiers?
Does it get better by splitting the data differently?
Does it make sense in the brain?

## Builtin feature selection

# DTI, RD
```{r}
source('~/ncr_notebooks/baseline_prediction/src/aux_functions.R')
tract_data = read.csv('~/data/baseline_prediction/stripped/dti.csv')
load('~/data/baseline_prediction/dti/rd_voxelwise.RData')
dti_vdata = cbind(tract_data$maskid, rd_data)
gf_fname = '~/data/baseline_prediction/stripped/clinical.csv'
gf = read.csv(gf_fname)
gf_base = gf[gf$BASELINE=='BASELINE', ]
my_ids = intersect(gf_base$MRN, tract_data$MRN)
merged = mergeOnClosestDate(gf_base, tract_data, my_ids)
rm_me = abs(merged$dateX.minus.dateY.months) > 12
merged = merged[!rm_me, ]
dti_base_vdata = merge(merged$maskid, dti_vdata, by.x=1, by.y=1, all.y=F, all.x=T)
```
## Univariate filtering
```
> summary(greedy_ensemble)
The following models were ensembled: rf, kernelpls, svmRadial, knn, rpart, bagEarthGCV, LogitBoost, lda, nb 
They were weighted: 
2.4879 4.7091 0.057 -5.0921 -1.1918 0.3064 -2.4179 -0.7845 0.223 -0.2291
The resulting Accuracy is: 0.774
The fit for each individual model on the Accuracy is: 
      method  Accuracy AccuracySD
          rf 0.6912903 0.09172569
   kernelpls 0.7520968 0.07845270
   svmRadial 0.7820565 0.07748010
         knn 0.7083065 0.06797393
       rpart 0.5534677 0.09228310
 bagEarthGCV 0.7156855 0.06382030
  LogitBoost 0.7193145 0.08451830
         lda 0.7331452 0.07237941
          nb 0.7026613 0.10019450
> model_preds <- lapply(model_list, predict, newdata=Xtest)
Warning messages:
1: In FUN(X[[i]], ...) :
  Numerical 0 probability for all classes with observation 4
2: In FUN(X[[i]], ...) :
  Numerical 0 probability for all classes with observation 10
> model_preds <- lapply(model_preds, function(d, obs) postResample(d, obs)[1], obs=ytest)
> model_preds <- data.frame(model_preds)
> ens_preds <- predict(greedy_ensemble, newdata=Xtest)
Warning messages:
1: In FUN(X[[i]], ...) :
  Numerical 0 probability for all classes with observation 4
2: In FUN(X[[i]], ...) :
  Numerical 0 probability for all classes with observation 10
> model_preds$greedyE <- postResample(ens_preds, ytest)[1]
> ens_preds <- predict(glm_ensemble, newdata=Xtest)
Warning messages:
1: In FUN(X[[i]], ...) :
  Numerical 0 probability for all classes with observation 4
2: In FUN(X[[i]], ...) :
  Numerical 0 probability for all classes with observation 10
> model_preds$glmE <- postResample(ens_preds, ytest)[1]
> ens_preds <- predict(gbm_ensemble, newdata=Xtest)
Warning messages:
1: In FUN(X[[i]], ...) :
  Numerical 0 probability for all classes with observation 4
2: In FUN(X[[i]], ...) :
  Numerical 0 probability for all classes with observation 10
> model_preds$gbmE <- postResample(ens_preds, ytest)[1]
> print(model_preds)
                rf kernelpls svmRadial       knn     rpart bagEarthGCV
Accuracy 0.4871795 0.6410256 0.6410256 0.5641026 0.4871795   0.5641026
         LogitBoost       lda        nb greedyE glmE      gbmE
Accuracy  0.5384615 0.5384615 0.5641026     0.4  0.4 0.2820513
> print(sprintf('No information rate: Accuracy=%f', max(table(ytrain)/length(ytrain))))
[1] "No information rate: Accuracy=0.534591"
```
The usual overfitting at training using DTI, but some promising generalization especially using pls and svm. 

Using PCA we go from 1463 good variables at .05 to 103. 
```
> summary(greedy_ensemble)
The following models were ensembled: rf, kernelpls, svmRadial, knn, rpart, bagEarthGCV, LogitBoost, lda, nb
They were weighted:
1.4781 9.6553 -6.981 1.9453 -4.6269 -0.493 -0.2592 -0.4696 0.2538 -0.7587
The resulting Accuracy is: 0.7224
The fit for each individual model on the Accuracy is:
      method  Accuracy AccuracySD
          rf 0.5912903 0.07777795
   kernelpls 0.6829435 0.06233932
   svmRadial 0.5699597 0.07606213
         knn 0.6802419 0.06363794
       rpart 0.5772984 0.08387947
 bagEarthGCV 0.5849194 0.09143766
  LogitBoost 0.5700806 0.07214890
         lda 0.5459677 0.11486481
          nb 0.5563710 0.08551479
> model_preds <- lapply(model_list, predict, newdata=filtXtest)
There were 39 warnings (use warnings() to see them)
> model_preds <- lapply(model_preds, function(d, obs) postResample(d, obs)[1], obs=ytest)
> model_preds <- data.frame(model_preds)
> ens_preds <- predict(greedy_ensemble, newdata=filtXtest)
There were 39 warnings (use warnings() to see them)
> model_preds$greedyE <- postResample(ens_preds, ytest)[1]
> ens_preds <- predict(glm_ensemble, newdata=filtXtest)
There were 39 warnings (use warnings() to see them)
> model_preds$glmE <- postResample(ens_preds, ytest)[1]
> ens_preds <- predict(gbm_ensemble, newdata=filtXtest)
There were 39 warnings (use warnings() to see them)
> model_preds$gbmE <- postResample(ens_preds, ytest)[1]
> print(model_preds)
                rf kernelpls svmRadial       knn     rpart bagEarthGCV
Accuracy 0.5384615 0.5384615 0.5384615 0.5384615 0.4615385   0.5384615
         LogitBoost       lda        nb   greedyE      glmE      gbmE
Accuracy  0.5897436 0.5128205 0.6666667 0.4615385 0.4615385 0.4102564
> print(sprintf('No information rate: Accuracy=%f', max(table(ytrain)/length(ytrain))))
[1] "No information rate: Accuracy=0.534591"
```
Not so great using PCA... nb seems to generalize well, but that's it. What if we maximize on ROC?
```
> summary(greedy_ensemble)
The following models were ensembled: rf, kernelpls, svmRadial, knn, rpart, bagEarthGCV, LogitBoost, lda, nb
They were weighted:
-5.28 4.8584 1.3782 -0.0134 3.9285 -0.9345 -0.5122 0.6926 -0.3043 0.9423
The resulting ROC is: 0.7968
The fit for each individual model on the ROC is:
      method       ROC      ROCSD
          rf 0.7350420 0.07995180
   kernelpls 0.7350028 0.06774671
   svmRadial 0.6340392 0.06406835
         knn 0.7663137 0.07020814
       rpart 0.5972717 0.06718338
 bagEarthGCV 0.7244594 0.07673689
  LogitBoost 0.6896134 0.07381313
         lda 0.5796134 0.07986952
          nb 0.6519664 0.08080691
> print(model_preds)
           rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost
ROC 0.5595238  0.526455 0.4365079 0.4761905 0.4484127    0.484127   0.505291
          lda        nb
ROC 0.4497354 0.3915344
> print(model_preds)
            rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost
Sens 0.3333333 0.4285714 0.1904762 0.0952381 0.2857143   0.1904762  0.6190476
           lda        nb
Sens 0.3333333 0.0952381
> print(model_preds)
            rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost
Spec 0.6111111 0.6111111 0.8888889 0.8333333 0.6111111   0.7777778  0.3888889
           lda        nb
Spec 0.6111111 0.8888889
> print(model_preds)
                rf kernelpls svmRadial       knn     rpart bagEarthGCV
Accuracy 0.4615385 0.5128205 0.5128205 0.4358974 0.4358974   0.4615385
         LogitBoost       lda        nb
Accuracy  0.5128205 0.4615385 0.4615385
[1] "No information rate: Accuracy=0.534591"
> print(model_preds)
                      rf kernelpls svmRadial       knn     rpart bagEarthGCV
AccuracyPValue 0.8694387 0.6858753 0.6858753 0.9256668 0.9256668   0.8694387
               LogitBoost       lda        nb
AccuracyPValue  0.6858753 0.8694387 0.8694387
```
Again, not as good as AD. 

## Builtin feature selection

# Paul Taylor
```{r}
source('~/ncr_notebooks/baseline_prediction/src/aux_functions.R')
tract_data = read.csv('~/data/baseline_prediction/stripped/dti.csv')
load('~/data/baseline_prediction/dti_paul_taylor.RData')
vdata = d2
gf_fname = '~/data/baseline_prediction/stripped/clinical.csv'
gf = read.csv(gf_fname)
gf_base = gf[gf$BASELINE=='BASELINE', ]
my_ids = intersect(gf_base$MRN, tract_data$MRN)
merged = mergeOnClosestDate(gf_base, tract_data, my_ids)
rm_me = abs(merged$dateX.minus.dateY.months) > 12
merged = merged[!rm_me, ]
keep_me = c()
keep_merged = c()
for (i in 1:nrow(vdata)) {
  if (as.numeric(vdata[i, 1]) %in% merged$maskid) {
    keep_me = c(keep_me, i)
    keep_merged = c(keep_merged, which(merged$maskid == as.numeric(vdata[i, 1])))
  }
}
d3 = vdata[keep_me, ]
d4 = as.numeric(d3)
dim(d4) = dim(d3)
dti_base_vdata = as.data.frame(d4)
merged = merged[keep_merged, ]
rm(vdata)
```
## Univariate results
```
> summary(greedy_ensemble)
The following models were ensembled: rf, kernelpls, svmRadial, knn, rpart, bagEarthGCV, LogitBoost, lda, nb 
They were weighted: 
-2.6839 2.7934 1.6551 5.4386 -1.3808 -0.7312 -3.0114 0.2099 -0.0228 0.4851
The resulting Accuracy is: 0.7657
The fit for each individual model on the Accuracy is: 
      method  Accuracy AccuracySD
          rf 0.7201505 0.06174660
   kernelpls 0.6482151 0.07759082
   svmRadial 0.7620457 0.05734014
         knn 0.6058495 0.07901383
       rpart 0.5619167 0.07235335
 bagEarthGCV 0.6812419 0.07885924
  LogitBoost 0.6466640 0.07862463
         lda 0.5672072 0.08036925
          nb 0.7322688 0.05107080
> model_preds <- lapply(model_list, predict, newdata=Xtest)
> model_preds <- lapply(model_preds, function(d, obs) postResample(d, obs)[1], obs=ytest)
> model_preds <- data.frame(model_preds)
> ens_preds <- predict(greedy_ensemble, newdata=Xtest)
> model_preds$greedyE <- postResample(ens_preds, ytest)[1]
> ens_preds <- predict(glm_ensemble, newdata=Xtest)
> model_preds$glmE <- postResample(ens_preds, ytest)[1]
> ens_preds <- predict(gbm_ensemble, newdata=Xtest)
> model_preds$gbmE <- postResample(ens_preds, ytest)[1]
> print(model_preds)
                rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb   greedyE      glmE      gbmE
Accuracy 0.6756757 0.5945946 0.6756757 0.5945946 0.5135135   0.6756757  0.5945946 0.5945946 0.6756757 0.6486486 0.6486486 0.5945946
> print(sprintf('No information rate: Accuracy=%f', max(table(ytrain)/length(ytrain))))
[1] "No information rate: Accuracy=0.535948"
```
PCA goes from 608 to 57 good dimensions.
```
The following models were ensembled: rf, kernelpls, svmRadial, knn, rpart, bagEarthGCV, LogitBoost, lda, nb 
They were weighted: 
-5.0617 -3.0908 13.3651 -2.5649 -2.4325 1.8528 0.5258 0.2763 0.4326 1.7652
The resulting Accuracy is: 0.7953
The fit for each individual model on the Accuracy is: 
      method  Accuracy AccuracySD
          rf 0.7466344 0.06284589
   kernelpls 0.8101048 0.06470884
   svmRadial 0.7605457 0.05503513
         knn 0.6732473 0.07007386
       rpart 0.6891129 0.06621917
 bagEarthGCV 0.7490511 0.06689793
  LogitBoost 0.7042258 0.06900986
         lda 0.7707849 0.07560172
          nb 0.7175215 0.08299757
> model_preds <- lapply(model_list, predict, newdata=filtXtest)
There were 37 warnings (use warnings() to see them)
> model_preds <- lapply(model_preds, function(d, obs) postResample(d, obs)[1], obs=ytest)
> model_preds <- data.frame(model_preds)
> ens_preds <- predict(greedy_ensemble, newdata=filtXtest)
There were 37 warnings (use warnings() to see them)
> model_preds$greedyE <- postResample(ens_preds, ytest)[1]
> ens_preds <- predict(glm_ensemble, newdata=filtXtest)
There were 37 warnings (use warnings() to see them)
> model_preds$glmE <- postResample(ens_preds, ytest)[1]
> ens_preds <- predict(gbm_ensemble, newdata=filtXtest)
There were 37 warnings (use warnings() to see them)
> model_preds$gbmE <- postResample(ens_preds, ytest)[1]
> print(model_preds)
                rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb   greedyE      glmE      gbmE
Accuracy 0.7027027 0.6756757 0.5405405 0.6216216 0.7027027   0.6486486  0.5405405 0.6216216 0.6756757 0.6756757 0.6756757 0.6756757
> print(sprintf('No information rate: Accuracy=%f', max(table(ytrain)/length(ytrain))))
[1] "No information rate: Accuracy=0.535948"
```
We did better with our usual DTI data. 70% is OK, but we're getting close to 74% above.

This is what we get maximizing ROC:

```{r}
X = dti_base_vdata[, 2:ncol(dti_base_vdata)]
rm_me = colSums(is.na(X)) > 0
X = X[, !rm_me]

keep_me = merged$age <= 12
X = X[keep_me, ]
y = merged$DX_BASELINE
y[y!='NV'] = 'ADHD'
# twoClassSummary uses the first level as the case of interest (https://topepo.github.io/caret/measuring-performance.html#measures-for-class-probabilities)
y = factor(y, levels=c('ADHD', 'NV'))
y = y[keep_me]

myseed = 1234
set.seed(myseed)
split <- createDataPartition(y, p = .8, list = FALSE)
Xtrain <- X[ split, ]
ytrain <- y[ split ]
Xtest  <- X[-split, ]
ytest = y[-split]

library(parallel)
cl <- makeCluster(8)
pvals = parSapply(cl, Xtrain, function(d, ytrain) t.test(d ~ ytrain)$p.value, ytrain)
Xtrain = Xtrain[, which(pvals <= .05)]
print(dim(Xtrain))

keep_me = sapply(colnames(Xtrain), function(d) which(colnames(Xtest) == d))
Xtest = Xtest[, keep_me]

pp <- preProcess(Xtrain, method = c('nzv', 'YeoJohnson', 'center', 'scale', 'pca'), thresh=.9)
filtXtrain<- predict(pp, Xtrain)
filtXtest <- predict(pp, Xtest)
print(dim(filtXtrain))

tuneLength=10
set.seed(myseed)
index <- createMultiFolds(ytrain, k = 5, times = 5)

set.seed(myseed)
fullCtrl <- trainControl(method = "repeatedcv",
                         index = index,
                         savePredictions="final",
                         classProbs=TRUE,
                         summaryFunction=twoClassSummary)

library(caretEnsemble)
model_list <- caretList(
  filtXtrain, ytrain,
  tuneLength=10,
  trControl=fullCtrl,
  metric='ROC',
  methodList=c('rf', 'kernelpls', 'svmRadial', 'knn', 'rpart', 'bagEarthGCV', 'LogitBoost', 'lda', 'nb')
  )

greedy_ensemble <- caretEnsemble(
  model_list,
  metric='ROC',
  trControl=trainControl(
    number=2,
    summaryFunction=twoClassSummary,
    classProbs=TRUE
    ))
# ROC stats
summary(greedy_ensemble)
model_preds <- lapply(model_list, function(d, x, y) eval_model(d, x, y, c('ADHD', 'NV'))['ROC'], filtXtest, ytest)
model_preds <- data.frame(model_preds)
print(model_preds)
# sensitivity stats
model_preds <- lapply(model_list, function(d, x, y) eval_model(d, x, y, c('ADHD', 'NV'))['Sens'], filtXtest, ytest)
model_preds <- data.frame(model_preds)
print(model_preds)
# specificity stats
model_preds <- lapply(model_list, function(d, x, y) eval_model(d, x, y, c('ADHD', 'NV'))['Spec'], filtXtest, ytest)
model_preds <- data.frame(model_preds)
print(model_preds)
# Accuracy stats
model_preds <- lapply(model_list, predict, newdata=filtXtest)
model_preds <- lapply(model_preds, function(d, obs) postResample(d, obs)[1], obs=ytest)
model_preds <- data.frame(model_preds)
print(model_preds)
print(sprintf('No information rate: Accuracy=%f', max(table(ytrain)/length(ytrain))))
model_preds <- lapply(model_list, predict, newdata=filtXtest)
model_preds <- lapply(model_preds, function(d) confusionMatrix(d, ref=ytest)$overall['AccuracyPValue'])
model_preds <- data.frame(model_preds)
print(model_preds)
```

```
> summary(greedy_ensemble)
The following models were ensembled: rf, kernelpls, svmRadial, knn, rpart, bagEarthGCV, LogitBoost, lda, nb 
They were weighted: 
3.997 1.127 -3.9563 -1.9596 1.148 -1.8261 -3.7097 0.3674 -0.4314 1.0648
The resulting ROC is: 0.8523
The fit for each individual model on the ROC is: 
      method       ROC      ROCSD
          rf 0.8275777 0.07860510
   kernelpls 0.8464818 0.07217518
   svmRadial 0.8296078 0.07652514
         knn 0.7649797 0.07212912
       rpart 0.7472864 0.08297590
 bagEarthGCV 0.8463501 0.06110585
  LogitBoost 0.7612665 0.06438682
         lda 0.8022626 0.08659123
          nb 0.6915833 0.08075707
> print(model_preds)
           rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost
ROC 0.5867647 0.5617647 0.5205882 0.5426471 0.6029412   0.5529412  0.5735294
          lda        nb
ROC 0.4911765 0.5382353
> print(model_preds)
       rf kernelpls svmRadial knn rpart bagEarthGCV LogitBoost lda   nb
Sens 0.55       0.4      0.45 0.3   0.5        0.65       0.55 0.4 0.45
> print(model_preds)
            rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost
Spec 0.5882353 0.6470588 0.5294118 0.6470588 0.7058824   0.5294118  0.5294118
           lda        nb
Spec 0.6470588 0.5882353
> print(model_preds)
                rf kernelpls svmRadial       knn     rpart bagEarthGCV
Accuracy 0.5675676 0.5135135 0.4864865 0.4594595 0.5945946   0.5945946
         LogitBoost       lda        nb
Accuracy  0.5405405 0.5135135 0.5135135
[1] "No information rate: Accuracy=0.535948"
> print(model_preds)
                      rf kernelpls svmRadial       knn     rpart bagEarthGCV
AccuracyPValue 0.4363729 0.6905328 0.7954645 0.8757373 0.3118864   0.3118864
               LogitBoost       lda        nb
AccuracyPValue  0.5670837 0.6905328 0.6905328
```