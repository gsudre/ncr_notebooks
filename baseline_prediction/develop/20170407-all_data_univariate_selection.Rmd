---
title: "Benefits of univariate selection"
output: html_notebook
---

Let's investigate how well we can do using all our variables, and then some univariate feature selection. Note that I'll be running this in Biowulf, because at first I'll keep the NAs, and the ADA models take forever to run locally.

```{r}
source('~/ncr_notebooks/baseline_prediction/src/aux_functions.R')

beery_data = read.csv('~/data/baseline_prediction/stripped/beeryVMI.csv')
gf_fname = '~/data/baseline_prediction/stripped/clinical.csv'
gf = read.csv(gf_fname)

gf = gf[gf$BASELINE=='BASELINE', ]
# we only need to keep MRN and DOA for now, to avoid duplicated
gf = gf[, c('MRN', 'DOA')]
my_ids = intersect(gf$MRN, beery_data$Medical.Record...MRN)
mbeery = mergeOnClosestDate(gf, beery_data, my_ids, y.date='record.date.collected', y.id='Medical.Record...MRN')
rm_me = abs(mbeery$dateX.minus.dateY.months) > 12
print(sprintf('Reducing from %d to %d tests', nrow(mbeery), nrow(mbeery)-sum(rm_me)))
mbeery = mbeery[!rm_me, ]
mbeery$dateClinical.minus.dateBeery.months = mbeery$dateX.minus.dateY.months
mbeery$dateX.minus.dateY.months = NULL

cpt_data = read.csv('~/data/baseline_prediction/stripped/cpt.csv')
my_ids = intersect(gf$MRN, cpt_data$MRN)
mcpt = mergeOnClosestDate(gf, cpt_data, my_ids)
rm_me = abs(mcpt$dateX.minus.dateY.months) > 12
print(sprintf('Reducing from %d to %d tests', nrow(mcpt), nrow(mcpt)-sum(rm_me)))
mcpt = mcpt[!rm_me, ]
mcpt$dateClinical.minus.dateCPT.months = mcpt$dateX.minus.dateY.months
mcpt$dateX.minus.dateY.months = NULL

iq_data = read.csv('~/data/baseline_prediction/stripped/iq.csv')
my_ids = intersect(gf$MRN, iq_data$Medical.Record...MRN)
miq = mergeOnClosestDate(gf, iq_data, my_ids, y.id='Medical.Record...MRN', y.date='record.date.collected')
rm_me = abs(miq$dateX.minus.dateY.months) > 12
print(sprintf('Reducing from %d to %d tests', nrow(miq), nrow(miq)-sum(rm_me)))
miq = miq[!rm_me, ]
miq$dateClinical.minus.dateIQ.months = miq$dateX.minus.dateY.months
miq$dateX.minus.dateY.months = NULL

wisc_data = read.csv('~/data/baseline_prediction/stripped/wisc.csv')
my_ids = intersect(gf$MRN, wisc_data$Medical.Record...MRN)
mwisc = mergeOnClosestDate(gf, wisc_data, my_ids, y.id='Medical.Record...MRN', y.date='record.date.collected')
rm_me = abs(mwisc$dateX.minus.dateY.months) > 12
print(sprintf('Reducing from %d to %d tests', nrow(mwisc), nrow(mwisc)-sum(rm_me)))
mwisc = mwisc[!rm_me, ]
mwisc$dateClinical.minus.dateWISC.months = mwisc$dateX.minus.dateY.months
mwisc$dateX.minus.dateY.months = NULL

wj_data = read.csv('~/data/baseline_prediction/stripped/wj.csv')
my_ids = intersect(gf$MRN, wj_data$Medical.Record...MRN)
mwj = mergeOnClosestDate(gf, wj_data, my_ids, y.id='Medical.Record...MRN', y.date='record.date.collected')
rm_me = abs(mwj$dateX.minus.dateY.months) > 12
print(sprintf('Reducing from %d to %d tests', nrow(mwj), nrow(mwj)-sum(rm_me)))
mwj = mwj[!rm_me, ]
mwj$dateClinical.minus.dateWJ.months = mwj$dateX.minus.dateY.months
mwj$dateX.minus.dateY.months = NULL

merged = merge(mwj, mbeery, by='MRN', all.x = T, all.y = T)
merged = merge(merged, miq, by='MRN', all.x = T, all.y = T)
merged = merge(merged, mwisc, by='MRN', all.x = T, all.y = T)
merged = merge(merged, mcpt, by='MRN', all.x = T, all.y = T)

tract_data = read.csv('~/data/baseline_prediction/stripped/dti.csv')
rm_me = (tract_data$fa_avg < .4 | tract_data$ad_avg < 1.18 | tract_data$rd_avg > .65 | tract_data$rd_avg < .5 |
tract_data$norm.trans > .45 | tract_data$norm.rot > .008 | tract_data$goodSlices < 45 | tract_data$goodSlices > 70)
print(sprintf('Reducing from %d to %d scans', nrow(tract_data), nrow(tract_data)-sum(rm_me)))
tract_data = tract_data[!rm_me, ]
my_ids = intersect(gf$MRN, tract_data$MRN)
mdti = mergeOnClosestDate(gf, tract_data, my_ids)
rm_me = abs(mdti$dateX.minus.dateY.months) > 12
print(sprintf('Reducing from %d to %d scans', nrow(mdti), nrow(mdti)-sum(rm_me)))
mdti = mdti[!rm_me, ]

geo_data = read.csv('~/data/baseline_prediction/stripped/geospatial.csv')
mgeo = merge(gf, geo_data, by='MRN')
# some variables are being read as numeric...
mgeo$Home_Price = as.numeric(mgeo$Home_Price)
mgeo$Fam_Income = as.numeric(mgeo$Fam_Income)
mgeo$Crime_Rate = as.numeric(mgeo$Crime_Rate)

merged = merge(merged, mdti, by='MRN', all.x = T, all.y = T)
merged = merge(merged, mgeo, by='MRN', all.x = T, all.y = T)

struct_data = read.csv('~/data/baseline_prediction/stripped/structural.csv')
rm_me = (struct_data$mprage_score > 2)
struct_data = struct_data[!rm_me, ]
my_ids = intersect(gf$MRN, struct_data$MRN)
mstruct = mergeOnClosestDate(gf, struct_data, my_ids)
rm_me = abs(mstruct$dateX.minus.dateY.months) > 12
print(sprintf('Reducing from %d to %d scans', nrow(mstruct), nrow(mstruct)-sum(rm_me)))
mstruct = mstruct[!rm_me, ]
merged = merge(merged, mstruct, by='MRN', all.x = T, all.y = T)

# putting back clinical data
clin = read.csv(gf_fname)
my_ids = gf$MRN
merged = mergeOnClosestDate(merged, clin, my_ids)

y = merged$DX_BASELINE
y[y != 'NV'] = 'ADHD'
y = factor(y, levels = c('NV', 'ADHD'))
```

Removing the date variables:

```{r}
X = merged[, 4:443]
phen_vars = c(which(grepl("^DOA", colnames(X))),
              which(grepl("^date", colnames(X))),
              which(grepl("^record", colnames(X))),
              which(grepl("^Scaled.Score..DS", colnames(X))),
              which(grepl("^Mask.ID", colnames(X))),
              which(grepl("^maskid", colnames(X))),
              which(grepl("^X_hit_RT_SE_ISI_change_t.scores_for_general_population", colnames(X)))
              )
# removing some extra crap that didn't make sense or broke the residualizing
X = X[, -phen_vars]

# let's go ahead and remove any factors from the data as well
rm_me = sapply(X, function(d) class(d)=='factor')
X = X[, !rm_me]
```

So, at this point X has 410 variables, because we didn't select anything a priori. Let's do our current cleaning pipeline. Note that I'm residualizing based on the age at the clinical assessment, because we always cut off to stuff happening within the year, and the assumption is that nothing changes then. That's what I'm going to do. 

```{r}
library(pROC)

# let's work with residuals
get_needed_residuals = function(y, fm_str, cutoff) {
  fm = as.formula(fm_str)
  if (class(y) != 'factor') {
    fit = lm(fm)
    # selecting which covariates to use
    fm = "y ~ "
    for (r in 2:dim(summary(fit)$coefficients)[1]) {
      if (summary(fit)$coefficients[r, 4] < cutoff) {
        cname = rownames(summary(fit)$coefficients)[r]
        cname = gsub("SEXMale", "SEX", cname)
        fm = sprintf('%s + %s', fm, cname)
      }
    }
    # don't do anything if no variables were significant
    if (fm != 'y ~ ') {
      idx = !is.na(y)
      opt_fit = lm(as.formula(fm))
      y[idx] = opt_fit$residuals
    }
  }
  return(y)
}

X_resid = sapply(X, get_needed_residuals, 'y ~ merged$age + I(merged$age^2) + merged$SEX', .1)
X_resid = as.data.frame(X_resid)

myseed = 1234

set.seed(myseed)
split <- createDataPartition(y, p = .8, list = FALSE)

Xtrain <- X_resid[ split, ]
ytrain <- y[ split ]
Xtest  <- X_resid[-split, ]
ytest = y[-split]

# now we can remove correlated and non-informative variables
nzv = nearZeroVar(Xtrain)
print(nzv)
if (length(nzv) > 0) {
  Xtrain = Xtrain[, -nzv]
}
correlations = cor(Xtrain, use='na.or.complete')
# library(corrplot)
# corrplot(correlations, order = "hclust")
highCorr = findCorrelation(correlations, cutoff=.75)
print(length(highCorr))
if (length(highCorr) > 0) {
  Xtrain = Xtrain[, -highCorr]
}

# only evaluate variables with p-value < .05
pvals = sapply(Xtrain, function(d) t.test(d ~ ytrain)$p.value)
Xtrain = Xtrain[pvals <= .05]
print(dim(Xtrain))

keep_me = sapply(colnames(Xtrain), function(d) which(colnames(Xtest) == d))
Xtest = Xtest[, keep_me]
```

Now we're down to 46 predictors, all uncorrelated (r < .75), and good at distinguishing the two groups (p < .05). Let's see what we can do without further cleaning:

```{r}
mymod = 'AdaBoost.M1'
tuneLength = 10
cpuDiff = 0

set.seed(myseed)
index <- createMultiFolds(ytrain, k = 5, times = 5)

library(doMC)
ncpus <- detectBatchCPUs()
njobs <- ncpus - cpuDiff
registerDoMC(njobs)
selFunc = 'best'  # maybe try oneSE and tolerance as well?

set.seed(myseed)
fullCtrl <- trainControl(method = "repeatedcv",
                         index = index,
                         search='grid',
                         summaryFunction = twoClassSummary,
                         classProbs = TRUE)
m1 <- train(Xtrain, ytrain,
            method = mymod,
            trControl = fullCtrl,
            tuneLength = tuneLength,
            metric = 'ROC')
print(getTrainPerf(m1))
pred = predict(m1, Xtest)
print(postResample(pred, ytest))
print(roc(as.numeric(ytest), as.numeric(pred)))
```

> print(getTrainPerf(m1))
   TrainROC TrainSens TrainSpec      method
1 0.7372638 0.5388571 0.7825806 AdaBoost.M1
> print(postResample(pred, ytest))
 Accuracy     Kappa 
0.6612903 0.2525832 
Area under the curve: 0.6189

Or doing some extra cleaning:

```{r}
mymod = 'AdaBoost.M1'
tuneLength = 10
cpuDiff = 0

set.seed(myseed)
index <- createMultiFolds(ytrain, k = 5, times = 5)

library(doMC)
ncpus <- detectBatchCPUs()
njobs <- ncpus - cpuDiff
registerDoMC(njobs)
selFunc = 'best'  # maybe try oneSE and tolerance as well?

pp = preProcess(Xtrain, method=c('YeoJohnson', 'center', 'scale'))
filtXtrain = predict(pp, Xtrain)
filtXtest = predict(pp, Xtest)

set.seed(myseed)
fullCtrl <- trainControl(method = "repeatedcv",
                         index = index,
                         search='grid',
                         summaryFunction = twoClassSummary,
                         classProbs = TRUE)
m1 <- train(filtXtrain, ytrain,
            method = mymod,
            trControl = fullCtrl,
            tuneLength = tuneLength,
            metric = 'ROC')
print(getTrainPerf(m1))
pred = predict(m1, filtXtest)
print(postResample(pred, ytest))
print(roc(as.numeric(ytest), as.numeric(pred)))
```

> print(getTrainPerf(m1))
   TrainROC TrainSens TrainSpec      method
1 0.7411747 0.5489524  0.776043 AdaBoost.M1
> print(postResample(pred, ytest))
 Accuracy     Kappa
0.6935484 0.3329558
Area under the curve: 0.6589

As we had seen from before, doing osme cleaning helps it generalize to the test set better, even though it doesn't matter much for the training set.

# Inattention 3 group outcome

Let's do something similar for inattention outcome.

```{r}
y = merged$inatt3_named
y = factor(y, levels=c('low', 'medium', 'high'))

# let's work with residuals
get_needed_residuals = function(y, fm_str, cutoff) {
  fm = as.formula(fm_str)
  if (class(y) != 'factor') {
    fit = lm(fm)
    # selecting which covariates to use
    fm = "y ~ "
    for (r in 2:dim(summary(fit)$coefficients)[1]) {
      if (summary(fit)$coefficients[r, 4] < cutoff) {
        cname = rownames(summary(fit)$coefficients)[r]
        cname = gsub("SEXMale", "SEX", cname)
        fm = sprintf('%s + %s', fm, cname)
      }
    }
    # don't do anything if no variables were significant
    if (fm != 'y ~ ') {
      idx = !is.na(y)
      opt_fit = lm(as.formula(fm))
      y[idx] = opt_fit$residuals
    }
  }
  return(y)
}

X_resid = sapply(X, get_needed_residuals, 'y ~ merged$age + I(merged$age^2) + merged$SEX', .1)
X_resid = as.data.frame(X_resid)

myseed = 1234

set.seed(myseed)
split <- createDataPartition(y, p = .8, list = FALSE)

Xtrain <- X_resid[ split, ]
ytrain <- y[ split ]
Xtest  <- X_resid[-split, ]
ytest = y[-split]

# now we can remove correlated and non-informative variables
nzv = nearZeroVar(Xtrain)
print(nzv)
if (length(nzv) > 0) {
  Xtrain = Xtrain[, -nzv]
}
correlations = cor(Xtrain, use='na.or.complete')
# library(corrplot)
# corrplot(correlations, order = "hclust")
highCorr = findCorrelation(correlations, cutoff=.75)
print(length(highCorr))
if (length(highCorr) > 0) {
  Xtrain = Xtrain[, -highCorr]
}

# only evaluate variables with p-value < .05
pvals = sapply(Xtrain, function(d) summary(aov(lm(d ~ ytrain)))[[1]]$"Pr(>F)"[1])
Xtrain = Xtrain[pvals <= .05]
print(dim(Xtrain))

keep_me = sapply(colnames(Xtrain), function(d) which(colnames(Xtest) == d))
Xtest = Xtest[, keep_me]

```

In this case, we're down to only 28 predictors.

```{r}
mymod = 'AdaBoost.M1'
tuneLength = 10
cpuDiff = 0

set.seed(myseed)
index <- createMultiFolds(ytrain, k = 5, times = 5)

library(doMC)
ncpus <- detectBatchCPUs()
njobs <- ncpus - cpuDiff
registerDoMC(njobs)
selFunc = 'best'  # maybe try oneSE and tolerance as well?

set.seed(myseed)
fullCtrl <- trainControl(method = "repeatedcv",
                         index = index,
                         search='grid',
                         summaryFunction = multiClassSummary,
                         classProbs = TRUE)
m1 <- train(Xtrain, ytrain,
            method = mymod,
            trControl = fullCtrl,
            tuneLength = tuneLength,
            metric = 'Mean_ROC')
print(getTrainPerf(m1))
pred = predict(m1, Xtest)
print(postResample(pred, ytest))
print(roc(as.numeric(ytest), as.numeric(pred)))
```

TrainMean_ROC TrainAccuracy TrainKappa TrainMean_Sensitivity TrainMean_Specificity
0.6739128     0.5336959  0.2496729             0.4667539             0.7511792
> print(postResample(pred, ytest))
 Accuracy     Kappa 
0.5000000 0.1944677 
Multi-class area under the curve: 0.5809

And like the binary test, we try the clean version:

```{r}
mymod = 'AdaBoost.M1'
tuneLength = 10
cpuDiff = 0

set.seed(myseed)
index <- createMultiFolds(ytrain, k = 5, times = 5)

library(doMC)
ncpus <- detectBatchCPUs()
njobs <- ncpus - cpuDiff
registerDoMC(njobs)
selFunc = 'best'  # maybe try oneSE and tolerance as well?

pp = preProcess(Xtrain, method=c('YeoJohnson', 'center', 'scale'))
filtXtrain = predict(pp, Xtrain)
filtXtest = predict(pp, Xtest)

set.seed(myseed)
fullCtrl <- trainControl(method = "repeatedcv",
                         index = index,
                         search='grid',
                         summaryFunction = multiClassSummary,
                         classProbs = TRUE)
m1 <- train(filtXtrain, ytrain,
            method = mymod,
            trControl = fullCtrl,
            tuneLength = tuneLength,
            metric = 'Mean_ROC')
print(getTrainPerf(m1))
pred = predict(m1, filtXtest)
print(postResample(pred, ytest))
print(roc(as.numeric(ytest), as.numeric(pred)))
```

TrainMean_ROC TrainAccuracy TrainKappa TrainMean_Sensitivity TrainMean_Specificity
0.679289      0.537308  0.2545592             0.4704306              0.752479
> print(postResample(pred, ytest))
 Accuracy     Kappa
0.4677419 0.1388889
Multi-class area under the curve: 0.564

In the 3-class case, cleaning didn't help with generalization to test set.