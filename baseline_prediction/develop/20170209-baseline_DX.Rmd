---
title: "CARET"
output: html_notebook
---

Let's try to get baseline DX from DTI. First, some data cleaning:

```{r}
library(caret)
library(doParallel)
# for replication purposes
set.seed(107)
source('~/ncr_notebooks/baseline_prediction//src/aux_functions.R')

gf_fname = '~/data/baseline_prediction/dti_match_02092017.csv'
data_fname = '~/data/baseline_prediction/dti_match_02092017.csv'
gf = read.csv(gf_fname)
group_var = 'DX_A'
all_data = read.csv(data_fname)
mrn_gf = unique(gf[, c('ID', group_var)])

idx = all_data$DTI_select < 2 & all_data$QC_GRADE < 3
data = all_data[idx,]

ldata = merge(mrn_gf, data, by.x='ID', by.y='ID')
eval(parse(text=sprintf('groups = ldata$\"%s.y\"', group_var)))
idx = groups=='NV' | groups=='ADHD'
groups = factor(groups[idx])
ldata = ldata[idx,]

phen_vars = c(which(grepl("MASKID", colnames(ldata))),
              which(grepl("^FA_", colnames(ldata))),
              which(grepl("^AD_", colnames(ldata))),
              which(grepl("^RD_", colnames(ldata)))
              )
ldata = ldata[, phen_vars]

```

At this point we have only clean DTI data in ldata. Let's check whether they have any remaining outliers in the property averages:

```{r}
mean_data = read.csv('~/data/baseline_prediction/dti_tortoiseExported_meanTSA_02012017.csv')
# remove data columns from mean_data to avoid duplicates
phen_idx = grepl("^FA_", colnames(mean_data)) | 
            grepl("^AD_", colnames(mean_data)) |
            grepl("^RD_", colnames(mean_data))

mean_data = mean_data[, !phen_idx]
ldata2 = merge(ldata, mean_data, by.x='MASKID', by.y='Mask.ID...Scan')

par(mfrow=c(1, 3))

nbreaks = 40
hist(ldata2$fa_avg, breaks=nbreaks)
hist(ldata2$ad_avg, breaks=nbreaks)
hist(ldata2$rd_avg, breaks=nbreaks)
```
Cleaning up based on visual cutoffs:

``` {r}
rm_me = ldata2$fa_avg < .4 | ldata2$ad_avg < 1.18 | ldata2$rd_avg > .7
ldata = ldata2[!rm_me, ]
groups = groups[!rm_me]
```

Now we keep only the variables we'll be using for predictions, and start classification:

``` {r}
phen_vars = c(which(grepl("^FA_", colnames(ldata))),
              which(grepl("^AD_", colnames(ldata))),
              which(grepl("^RD_", colnames(ldata)))
              )
ldata = ldata[, phen_vars]

ncv = 5
nrepeatcv = 2
nsplits = 5
train_test_ratio = .85
root_fname = '~/tmp/dti_NVvsADHD_3props_pca'
# saving output
sink(sprintf('%s.log', root_fname), append=FALSE, split=TRUE)
ncores = detectCores(all.tests = FALSE, logical = TRUE)
njobs = ncores - 3
run_models = c('rndForest', 'lr', 'rsvm', 'xgb') #c('rndForest', 'lr', 'lsvm', 'rsvm', 'xgb', 'gbm')

inTrain = createDataPartition(y = groups,
                              times= nsplits,
                              p=train_test_ratio)
metric = "ROC"
default_preproc = c("pca")
default_options = list(n.comp=15)
ctrl_cv <- trainControl(method = "repeatedcv",
                        number = ncv,
                        repeats = nrepeatcv,
                        classProbs = TRUE,
                        returnData = FALSE,
                        summaryFunction = twoClassSummary,
                        preProcOptions = default_options,
                        search='grid')

source('~/ncr_notebooks/baseline_prediction/src/do_classification.R')

```
Let's plot different metrics on the test data:

```{r}
source('~/ncr_notebooks/baseline_prediction/src/do_metrics_plots.R')
```

Looks like XGB does best, mostly because it has a good tradeoff between sensititivy and specificity. We can explore it further too... Also, given the time they're taking to run, gbm and lsvm don't seem to be worth the effort, at least not with their current search grids.

These are the results if we do PCA as a preprocessing step.

```{r}
source('~/ncr_notebooks/baseline_prediction/src/do_metrics_plots.R')
```

And then we run PCA again, just to check if results are consistent (removed center and scale, which are superfluous in this case):

```{r}
source('~/ncr_notebooks/baseline_prediction/src/do_metrics_plots.R')
```

Here's PCA only within FA:

```{r}
phen_vars = c(which(grepl("^FA_", colnames(ldata))))
ldata = ldata[, phen_vars]

ncv = 5
nrepeatcv = 2
nsplits = 5
train_test_ratio = .85
root_fname = '~/tmp/dti_NVvsADHD_3props_pca'
# saving output
sink(sprintf('%s.log', root_fname), append=FALSE, split=TRUE)
ncores = detectCores(all.tests = FALSE, logical = TRUE)
njobs = ncores - 3
run_models = c('rndForest', 'lr', 'rsvm', 'xgb') #c('rndForest', 'lr', 'lsvm', 'rsvm', 'xgb', 'gbm')

inTrain = createDataPartition(y = groups,
                              times= nsplits,
                              p=train_test_ratio)
metric = "ROC"
default_preproc = c("pca")
default_options = list(n.comp=15)
ctrl_cv <- trainControl(method = "repeatedcv",
                        number = ncv,
                        repeats = nrepeatcv,
                        classProbs = TRUE,
                        returnData = FALSE,
                        summaryFunction = twoClassSummary,
                        preProcOptions = default_options,
                        search='grid')

source('~/ncr_notebooks/baseline_prediction/src/do_classification.R')
source('~/ncr_notebooks/baseline_prediction/src/do_metrics_plots.R')
```

And then we run PCA within AD and RD only:

```{r}
phen_vars = c(which(grepl("^AD_", colnames(ldata))),
              which(grepl("^RD_", colnames(ldata))))
ldata = ldata[, phen_vars]

ncv = 5
nrepeatcv = 2
nsplits = 5
train_test_ratio = .85
root_fname = '~/tmp/dti_NVvsADHD_3props_pca'
# saving output
sink(sprintf('%s.log', root_fname), append=FALSE, split=TRUE)
ncores = detectCores(all.tests = FALSE, logical = TRUE)
njobs = ncores - 3
run_models = c('rndForest', 'lr', 'rsvm', 'xgb') #c('rndForest', 'lr', 'lsvm', 'rsvm', 'xgb', 'gbm')

inTrain = createDataPartition(y = groups,
                              times= nsplits,
                              p=train_test_ratio)
metric = "ROC"
default_preproc = c("pca")
default_options = list(n.comp=15)
ctrl_cv <- trainControl(method = "repeatedcv",
                        number = ncv,
                        repeats = nrepeatcv,
                        classProbs = TRUE,
                        returnData = FALSE,
                        summaryFunction = twoClassSummary,
                        preProcOptions = default_options,
                        search='grid')

source('~/ncr_notebooks/baseline_prediction/src/do_classification.R')
source('~/ncr_notebooks/baseline_prediction/src/do_metrics_plots.R')
```
It looks like we should either do PCA using everything, which boosts LR and rndForst, or not do it at all, which helps xgb.