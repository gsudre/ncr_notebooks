---
title: "Voxel baseline multiclass"
output: html_notebook
---

Still trying to predict baseline, can we do a decent job when predicting baseline symtpom categories? We can group them and make it ordinal, just to make it more interesting.

# DTI, FA
```{r}
source('~/ncr_notebooks/baseline_prediction/src/aux_functions.R')
tract_data = read.csv('~/data/baseline_prediction/stripped/dti.csv')
load('~/data/baseline_prediction/dti/fa_voxelwise.RData')
dti_vdata = cbind(tract_data$maskid, fa_data)
gf_fname = '~/data/baseline_prediction/stripped/clinical.csv'
gf = read.csv(gf_fname)
gf_base = gf[gf$BASELINE=='BASELINE', ]
my_ids = intersect(gf_base$MRN, tract_data$MRN)
merged = mergeOnClosestDate(gf_base, tract_data, my_ids)
rm_me = abs(merged$dateX.minus.dateY.months) > 12
merged = merged[!rm_me, ]
dti_base_vdata = merge(merged$maskid, dti_vdata, by.x=1, by.y=1, all.y=F, all.x=T)
```

## Univariate filtering
Inattention code:
```{r}
X = dti_base_vdata[, 2:ncol(dti_base_vdata)]
rm_me = colSums(is.na(X)) > 0
X = X[, !rm_me]

y = rep('low', nrow(merged))
y[merged$SX_inatt>0] = 'medium'
y[merged$SX_inatt>5] = 'high'
y = factor(y, levels=c('low', 'medium', 'high'), ordered = T)

keep_me = merged$age <= 12
X = X[keep_me, ]
y = y[keep_me]
```

HI code:
```{r}
X = dti_base_vdata[, 2:ncol(dti_base_vdata)]
rm_me = colSums(is.na(X)) > 0
X = X[, !rm_me]

y = rep('low', nrow(merged))
y[merged$SX_HI>0] = 'medium'
y[merged$SX_HI>5] = 'high'
y = factor(y, levels=c('low', 'medium', 'high'), ordered = T)

keep_me = merged$age <= 12
X = X[keep_me, ]
y = y[keep_me]
```

The actual decoding:
```{r}
get_needed_residuals = function(y, fm_str, cutoff, merged) {
  fm = as.formula(fm_str)
  fit = lm(fm)
  # selecting which covariates to use
  fm = "y ~ "
  for (r in 2:dim(summary(fit)$coefficients)[1]) {
    if (summary(fit)$coefficients[r, 4] < cutoff) {
      cname = rownames(summary(fit)$coefficients)[r]
      cname = gsub("SEXMale", "SEX", cname)
      fm = sprintf('%s + %s', fm, cname)
    }
  }
  # don't do anything if no variables were significant
  if (fm == 'y ~ ') {
    return(y)
  } else {
    opt_fit = lm(as.formula(fm))
    return(opt_fit$residuals)
  }
}

library(parallel)
cl <- makeCluster(32)
X_resid = parSapply(cl, X, get_needed_residuals, 'y ~ merged$age_at_scan + I(merged$age_at_scan^2) + merged$SEX', .1, merged[keep_me, ])
stopCluster(cl)
X_resid = as.data.frame(X_resid)

myseed = 1234
set.seed(myseed)
split <- createDataPartition(y, p = .8, list = FALSE)
Xtrain <- X_resid[ split, ]
ytrain <- y[ split ]
Xtest  <- X_resid[-split, ]
ytest = y[-split]

# in voxel analysis it's actually more costly to do the correlation in the entire set of variables
# then to run the for loop, so let's first reduce the variables to only the univariate ones
library(parallel)
cl <- makeCluster(32)
pvals = parSapply(cl, Xtrain, function(d, ytrain) summary(aov(lm(d ~ ytrain)))[[1]][[5]][1], ytrain)
Xtrain = Xtrain[, which(pvals <= .05)]
print(dim(Xtrain))

keep_me = parSapply(cl, colnames(Xtrain), function(d, namesXtest) which(namesXtest == d), colnames(Xtest))
Xtest = Xtest[, keep_me]
stopCluster(cl)

pp <- preProcess(Xtrain, method = c('BoxCox', 'center', 'scale', 'pca'), thresh=.9)
filtXtrain<- predict(pp, Xtrain)
filtXtest <- predict(pp, Xtest)
print(dim(filtXtrain))

tuneLength=10
set.seed(myseed)
index <- createMultiFolds(ytrain, k = 5, times = 5)

fullCtrl <- trainControl(method = "repeatedcv",
                         index = index,
                         savePredictions="final")

adaptControl <- trainControl(method = "adaptive_cv",
                             number = 10, repeats = 10,
                             adaptive = list(min = 5, alpha = 0.05, 
                                             method = "gls", complete = TRUE),
                             classProbs = TRUE,
                             search = "random")
require(doMC)
registerDoMC(cores=32)
set.seed(myseed)
onAdapt <- train(filtXtrain, ytrain,
                  method = "svmRadial", 
                  trControl = adaptControl,
                  metric = "ROC",
                  tuneLength = 15)

library(caretEnsemble)
# some of these models don't use class probabilities, so we can't do ROC
model_list <- caretList(
  filtXtrain, ytrain,
  tuneLength=15,
  trControl=adaptCtrl,
  methodList=c('rpartScore', 'svmRadial')
  # tuneList=list(on=caretModelSpec(method='ordinalNet', tuneGrid=expand.grid(alpha = seq(0.1, 1, length = 15),
  #                                                                           criteria = "aic",
  #                                                                           link = c("logit", "probit", "cloglog", "cauchit"))))
  )

model_perf = data.frame(lapply(model_list, function(d) getTrainPerf(d)[1]))
names(model_perf) = names(model_list)
print(model_perf)
# ROC stats
model_preds <- lapply(model_list, function(d, x, y) eval_model(d, x, y, c('ADHD', 'NV'))['ROC'], filtXtest, ytest)
model_preds <- data.frame(model_preds)
print(model_preds)
# sensitivity stats
model_preds <- lapply(model_list, function(d, x, y) eval_model(d, x, y, c('ADHD', 'NV'))['Sens'], filtXtest, ytest)
model_preds <- data.frame(model_preds)
print(model_preds)
# specificity stats
model_preds <- lapply(model_list, function(d, x, y) eval_model(d, x, y, c('ADHD', 'NV'))['Spec'], filtXtest, ytest)
model_preds <- data.frame(model_preds)
print(model_preds)
# Accuracy stats
model_preds <- lapply(model_list, predict, newdata=filtXtest)
model_preds <- lapply(model_preds, function(d, obs) postResample(d, obs)[1], obs=ytest)
model_preds <- data.frame(model_preds)
print(model_preds)
print(sprintf('No information rate: Accuracy=%f', max(table(ytrain)/length(ytrain))))
model_preds <- lapply(model_list, predict, newdata=filtXtest)
model_preds <- lapply(model_preds, function(d) confusionMatrix(d, ref=ytest)$overall['AccuracyPValue'])
model_preds <- data.frame(model_preds)
print(model_preds)
```

This was taking so much time that I wasn't sure whether it's worth to continue it. Even with a 32 multicore, it was taking several days for a single seed. Let me play a bit with adaptive control, though. It might help the speed.

But it turns out that even with the biggest single computer I can get, and using adaptive CV, it still doesn't complete. It's probably buggy. Let's go ahead and evaluate the 2 other algorithms we can play with. 

I can tell you that neither svmRadial nor rpartScore get affected by the outcome being ordinal. No change in training performance whatsoever. 

```
> postResample(predict(model_list[[1]], filtXtest), ytest)
  Accuracy      Kappa 
 0.3333333 -0.0400000 
> postResample(predict(model_list[[2]], filtXtest), ytest)
  Accuracy      Kappa 
0.35897436 0.01215805 
```

Still overfitting though.


# DTI, AD
```{r}
source('~/ncr_notebooks/baseline_prediction/src/aux_functions.R')
tract_data = read.csv('~/data/baseline_prediction/stripped/dti.csv')
load('~/data/baseline_prediction/dti/ad_voxelwise.RData')
dti_vdata = cbind(tract_data$maskid, ad_data)
gf_fname = '~/data/baseline_prediction/stripped/clinical.csv'
gf = read.csv(gf_fname)
gf_base = gf[gf$BASELINE=='BASELINE', ]
my_ids = intersect(gf_base$MRN, tract_data$MRN)
merged = mergeOnClosestDate(gf_base, tract_data, my_ids)
rm_me = abs(merged$dateX.minus.dateY.months) > 12
merged = merged[!rm_me, ]
dti_base_vdata = merge(merged$maskid, dti_vdata, by.x=1, by.y=1, all.y=F, all.x=T)
```

## Univariate filtering


# DTI, RD
```{r}
source('~/ncr_notebooks/baseline_prediction/src/aux_functions.R')
tract_data = read.csv('~/data/baseline_prediction/stripped/dti.csv')
load('~/data/baseline_prediction/dti/rd_voxelwise.RData')
dti_vdata = cbind(tract_data$maskid, rd_data)
gf_fname = '~/data/baseline_prediction/stripped/clinical.csv'
gf = read.csv(gf_fname)
gf_base = gf[gf$BASELINE=='BASELINE', ]
my_ids = intersect(gf_base$MRN, tract_data$MRN)
merged = mergeOnClosestDate(gf_base, tract_data, my_ids)
rm_me = abs(merged$dateX.minus.dateY.months) > 12
merged = merged[!rm_me, ]
dti_base_vdata = merge(merged$maskid, dti_vdata, by.x=1, by.y=1, all.y=F, all.x=T)
```

## Univariate filtering

