---
title: "Testing ensembles"
output: html_notebook
---

Let's see if using an ensemble of classifiers helps us results. As usual, we will take DTI as our benchmark. Let's do some baseline classification first:

```{r}
source('~/ncr_notebooks/baseline_prediction/src/aux_functions.R')

tract_data = read.csv('~/data/baseline_prediction/stripped/dti.csv')
rm_me = (tract_data$fa_avg < .4 | tract_data$ad_avg < 1.18 | tract_data$rd_avg > .65 | tract_data$rd_avg < .5 |
         tract_data$norm.trans > .45 | tract_data$norm.rot > .008 | tract_data$goodSlices < 45 | tract_data$goodSlices > 70)
print(sprintf('Reducing from %d to %d scans', nrow(tract_data), nrow(tract_data)-sum(rm_me)))
tract_data = tract_data[!rm_me, ]
gf_fname = '~/data/baseline_prediction/stripped/clinical.csv'
gf = read.csv(gf_fname)
gf = gf[gf$BASELINE=='BASELINE', ]
my_ids = intersect(gf$MRN, tract_data$MRN)
merged = mergeOnClosestDate(gf, tract_data, my_ids)
rm_me = abs(merged$dateX.minus.dateY.months) > 12
print(sprintf('Reducing from %d to %d scans', nrow(merged), nrow(merged)-sum(rm_me)))
merged = merged[!rm_me, ]

phen_vars = c(which(grepl("^FA_", colnames(merged))),
              which(grepl("^AD_", colnames(merged))),
              which(grepl("^RD_", colnames(merged))),
              which(grepl("^MO_", colnames(merged)))
              )
X = merged[, phen_vars]
y = merged$DX_BASELINE
y[y != 'NV'] = 'ADHD'
y = factor(y)
rm_me = rowSums(is.na(X)) > 0
X = X[!rm_me, ]
merged = merged[!rm_me, ]
y = y[!rm_me]

get_needed_residuals = function(y, fm_str, cutoff) {
  fm = as.formula(fm_str)
  if (class(y) != 'factor') {
    fit = lm(fm)
    # selecting which covariates to use
    fm = "y ~ "
    for (r in 2:dim(summary(fit)$coefficients)[1]) {
      if (summary(fit)$coefficients[r, 4] < cutoff) {
        cname = rownames(summary(fit)$coefficients)[r]
        cname = gsub("SEXMale", "SEX", cname)
        fm = sprintf('%s + %s', fm, cname)
      }
    }
    # don't do anything if no variables were significant
    if (fm != 'y ~ ') {
      idx = !is.na(y)
      opt_fit = lm(as.formula(fm))
      y[idx] = opt_fit$residuals
    }
  }
  return(y)
}

X_resid = sapply(X, get_needed_residuals, 'y ~ merged$age + I(merged$age^2) + merged$SEX', .1)
X_resid = as.data.frame(X_resid)
```

```{r}
library(pROC)
myseed = 1234
cpuDiff = 1
tuneLength = 10
default_preproc = c("center", 'scale')
set.seed(myseed)
split <- createDataPartition(y, p = .8, list = FALSE)

Xtrain <- X_resid[ split, ]
ytrain <- y[ split ]
Xtest  <- X_resid[-split, ]
ytest = y[-split]

set.seed(myseed)
index <- createMultiFolds(ytrain, k = 5, times = 5)

library(doMC)
ncpus <- detectBatchCPUs()
njobs <- ncpus - cpuDiff
registerDoMC(njobs)
selFunc = 'best'  # maybe try oneSE and tolerance as well?

set.seed(myseed)
fullCtrl <- trainControl(method = "repeatedcv",
                         index = index,
                         search='grid',
                         summaryFunction = twoClassSummary,
                         classProbs = TRUE)
rndForestGrid <- expand.grid(.mtry=c(1:sqrt(ncol(Xtrain))))
rfFull <- train(Xtrain, ytrain,
                method = "rf",
                trControl = fullCtrl,
                tuneGrid=rndForestGrid,
                metric = 'ROC',
                preProcess = default_preproc)
  
getTrainPerf(rfFull)
pred = predict(rfFull, Xtest)
postResample(pred, ytest)
roc(as.numeric(ytest), as.numeric(pred))
```

And I think we can improve even a bit more by using only univariate related variables:

```{r}
Xtrain <- X_resid[ split, ]
ytrain <- y[ split ]
Xtest  <- X_resid[-split, ]
ytest = y[-split]

# now we can remove correlated and non-informative variables
nzv = nearZeroVar(Xtrain)
print(nzv)
if (length(nzv) > 0) {
  Xtrain = Xtrain[, -nzv]
}
correlations = cor(Xtrain, use='na.or.complete')
# library(corrplot)
# corrplot(correlations, order = "hclust")
highCorr = findCorrelation(correlations, cutoff=.75)
print(length(highCorr))
if (length(highCorr) > 0) {
  Xtrain = Xtrain[, -highCorr]
}

# only evaluate variables with p-value < .05
pvals = sapply(Xtrain, function(d) t.test(d ~ ytrain)$p.value)
Xtrain = Xtrain[pvals <= .05]
print(dim(Xtrain))

keep_me = sapply(colnames(Xtrain), function(d) which(colnames(Xtest) == d))
Xtest = Xtest[, keep_me]

rfFull <- train(Xtrain, ytrain,
                method = "rf",
                trControl = fullCtrl,
                tuneGrid=rndForestGrid,
                metric = 'ROC',
                preProcess = default_preproc)
  
getTrainPerf(rfFull)
pred = predict(rfFull, Xtest)
postResample(pred, ytest)
roc(as.numeric(ytest), as.numeric(pred))
```

Not really... oh well. Can we do better if we use an ensemble of classifiers?

```{r}
Xtrain <- X_resid[ split, ]
ytrain <- y[ split ]
Xtest  <- X_resid[-split, ]
ytest = y[-split]

set.seed(myseed)
index <- createResample(ytrain, 25)

my_control <- trainControl(
  method="boot",
  number=25,
  savePredictions="final",
  classProbs=TRUE,
  index=index,
  summaryFunction=defaultSummary
  )

library("rpart")
library("caretEnsemble")
model_list <- caretList(
  Xtrain, ytrain,
  tuneLength=10,
  trControl=my_control,
  methodList=c('lda', 'rpart', 'glm', 'knn', 'svmRadial')
  )
```
```{r}
results <- resamples(model_list)
summary(results)
dotplot(results)
modelCor(results)
splom(results)
```

```{r}
greedy_ensemble <- caretEnsemble(
  model_list, 
  metric="Accuracy",
  trControl=trainControl(
    number=2,
    summaryFunction=defaultSummary,
    classProbs=TRUE
    ))
summary(greedy_ensemble)
```

Thsi seems to have worked better than each individual model on its own. How does it look in the test set?

```{r}
library("caTools")
model_preds <- lapply(model_list, predict, newdata=Xtest, type="prob")
model_preds <- lapply(model_preds, function(x) x[,"ADHD"])
model_preds <- data.frame(model_preds)
ens_preds <- predict(greedy_ensemble, newdata=Xtest, type="prob")
model_preds$ensemble <- ens_preds
caTools::colAUC(model_preds, ytest)
```

svmRadial actually does better... that's AUC though.

What if we combine them differently?

```{r}
glm_ensemble <- caretStack(
  model_list,
  method="glm",
  metric="Accuracy",
  trControl=trainControl(
    method="boot",
    number=10,
    savePredictions="final",
    classProbs=TRUE,
    summaryFunction=defaultSummary
  )
)
model_preds2 <- model_preds
model_preds2$ensemble <- predict(glm_ensemble, newdata=Xtest, type="prob")
CF <- coef(glm_ensemble$ens_model$finalModel)[-1]
colAUC(model_preds2, ytest)
CF/sum(CF)
```

Not really an improvement. What if we combine them using gbm instead?

```{r}
library("gbm")
gbm_ensemble <- caretStack(
  model_list,
  method="gbm",
  verbose=FALSE,
  tuneLength=10,
  metric="Accuracy",
  trControl=trainControl(
    method="boot",
    number=10,
    savePredictions="final",
    classProbs=TRUE,
    summaryFunction=defaultSummary
  )
)
model_preds3 <- model_preds
model_preds3$ensemble <- predict(gbm_ensemble, newdata=Xtest, type="prob")
colAUC(model_preds3, ytest)
```

OK, now we're talking. We're finally getting some improvement by combining classifiers.

The code is working, so let's fine tune it!

First, a few questions:

* should we tune on accuracy or ROC?
* should we use all these classifiers? Add more?
* use bootstrapping is better than K-fold?
* should we clean the data?
* should we downsize the set to only univariate good variables?
* if a classifier sucks, does it automatically get bad weights, or should we go ahead and remove it?

# ROC or Accuracy?

```{r}
table(y)
```

The split is quite even, so I wouldn't worry too much about that. Let's see if we get an improvement using ROC as well:

```{r}
mymetric='ROC'
smFunc = twoClassSummary

my_control <- trainControl(
  method="boot",
  number=25,
  savePredictions="final",
  classProbs=TRUE,
  index=index,
  summaryFunction=smFunc
  )

library("rpart")
library("caretEnsemble")
model_list <- caretList(
  Xtrain, ytrain,
  tuneLength=10,
  trControl=my_control,
  metric = mymetric,
  methodList=c('lda', 'rpart', 'glm', 'knn', 'svmRadial')
  )

greedy_ensemble <- caretEnsemble(
  model_list, 
  metric=mymetric,
  trControl=trainControl(
    number=2,
    summaryFunction=smFunc,
    classProbs=TRUE
    ))
```

```{r}
summary(greedy_ensemble)
model_preds <- lapply(model_list, predict, newdata=Xtest, type="prob")
model_preds <- lapply(model_preds, function(x) x[,"ADHD"])
model_preds <- data.frame(model_preds)
ens_preds <- predict(greedy_ensemble, newdata=Xtest, type="prob")
model_preds$ensemble <- ens_preds
caTools::colAUC(model_preds, ytest)
```

This is very poor. The ensemble is actually much worse than each model individually, especially in the test set. Let's test this on accuracy:

```{r}
mymetric='Accuracy'
smFunc = defaultSummary

my_control <- trainControl(
  method="boot",
  number=25,
  savePredictions="final",
  classProbs=TRUE,
  index=index,
  summaryFunction=smFunc
  )

model_list <- caretList(
  Xtrain, ytrain,
  tuneLength=10,
  trControl=my_control,
  metric = mymetric,
  methodList=c('lda', 'rpart', 'glm', 'knn', 'svmRadial')
  )

greedy_ensemble <- caretEnsemble(
  model_list, 
  metric=mymetric,
  trControl=trainControl(
    number=2,
    summaryFunction=smFunc,
    classProbs=TRUE
    ))
```

```{r}
summary(greedy_ensemble)
model_preds <- lapply(model_list, predict, newdata=Xtest)
model_preds <- lapply(model_preds, function(x) postResample(x, ytest)[1])
model_preds <- data.frame(model_preds)
ens_preds <- predict(greedy_ensemble, newdata=Xtest)
model_preds$ensemble <- postResample(ens_preds, ytest)[1]
print(model_preds)
```

Same here. But even though the accuracy is indeed better by combining the classifiers in the training set, it doesn't generalize well in test. Is it just a matter of how we're combining them?

```{r}
mymetric='ROC'
smFunc = twoClassSummary

my_control <- trainControl(
  method="boot",
  number=25,
  savePredictions="final",
  classProbs=TRUE,
  index=index,
  summaryFunction=smFunc
  )

library("rpart")
library("caretEnsemble")
model_list <- caretList(
  Xtrain, ytrain,
  tuneLength=10,
  trControl=my_control,
  metric = mymetric,
  methodList=c('lda', 'rpart', 'glm', 'knn', 'svmRadial')
  )

greedy_ensemble <- caretEnsemble(
  model_list, 
  metric=mymetric,
  trControl=trainControl(
    number=2,
    summaryFunction=smFunc,
    classProbs=TRUE
    ))

glm_ensemble <- caretStack(
  model_list,
  method="glm",
  metric=mymetric,
  trControl=trainControl(
    method="boot",
    number=10,
    savePredictions="final",
    classProbs=TRUE,
    summaryFunction=smFunc
  )
)

gbm_ensemble <- caretStack(
  model_list,
  method="gbm",
  verbose=FALSE,
  tuneLength=10,
  metric=mymetric,
  trControl=trainControl(
    method="boot",
    number=10,
    savePredictions="final",
    classProbs=TRUE,
    summaryFunction=smFunc
  )
)
summary(greedy_ensemble)
summary(glm_ensemble)
summary(gbm_ensemble)
model_preds <- lapply(model_list, predict, newdata=Xtest, type="prob")
model_preds <- lapply(model_preds, function(x) x[,"ADHD"])
model_preds <- data.frame(model_preds)
ens_preds <- predict(greedy_ensemble, newdata=Xtest, type="prob")
model_preds$greedyE <- ens_preds
ens_preds <- predict(glm_ensemble, newdata=Xtest, type="prob")
model_preds$glmE <- ens_preds
ens_preds <- predict(gbm_ensemble, newdata=Xtest, type="prob")
model_preds$gbmE <- ens_preds
caTools::colAUC(model_preds, ytest)

```

OK, so no much improvement regardless of how we ensemble the models, if using AUC. How about accuracy?

```{r}
mymetric='Accuracy'
smFunc = defaultSummary

my_control <- trainControl(
  method="boot",
  number=25,
  savePredictions="final",
  classProbs=TRUE,
  index=index,
  summaryFunction=smFunc
  )

model_list <- caretList(
  Xtrain, ytrain,
  tuneLength=10,
  trControl=my_control,
  metric = mymetric,
  methodList=c('lda', 'rpart', 'glm', 'knn', 'svmRadial')
  )

greedy_ensemble <- caretEnsemble(
  model_list, 
  metric=mymetric,
  trControl=trainControl(
    number=2,
    summaryFunction=smFunc,
    classProbs=TRUE
    ))

glm_ensemble <- caretStack(
  model_list,
  method="glm",
  metric=mymetric,
  trControl=trainControl(
    method="boot",
    number=10,
    savePredictions="final",
    classProbs=TRUE,
    summaryFunction=smFunc
  )
)

gbm_ensemble <- caretStack(
  model_list,
  method="gbm",
  verbose=FALSE,
  tuneLength=10,
  metric=mymetric,
  trControl=trainControl(
    method="boot",
    number=10,
    savePredictions="final",
    classProbs=TRUE,
    summaryFunction=smFunc
  )
)
summary(greedy_ensemble)
summary(glm_ensemble)
summary(gbm_ensemble)
model_preds <- lapply(model_list, predict, newdata=Xtest)
model_preds <- lapply(model_preds, function(x) postResample(x, ytest)[1])
model_preds <- data.frame(model_preds)
ens_preds <- predict(greedy_ensemble, newdata=Xtest)
model_preds$greedyE <- postResample(ens_preds, ytest)[1]
ens_preds <- predict(glm_ensemble, newdata=Xtest)
model_preds$glmE <- postResample(ens_preds, ytest)[1]
ens_preds <- predict(gbm_ensemble, newdata=Xtest)
model_preds$gbmE <- postResample(ens_preds, ytest)[1]
print(model_preds)

```
That's puzzling... how did I get that improvement before? It looks like I tuned each classifier based on ROC, but used Accuracy for tunning the ensemble, and then later output the AUC... bonkers. The questions above still apply though. So far, not much decided, but it's worth trying it all, since it doesn't take much computational time. 

For now, the no information thresholding is `r max(table(y))/length(y)`.

# Which classifiers to use?

Are all these classifiers performing well in the training set? Also, we should try others that we normally enjoy:

```{r}
mymetric='Accuracy'
smFunc = defaultSummary

Xtrain <- X_resid[ split, ]
ytrain <- y[ split ]
Xtest  <- X_resid[-split, ]
ytest = y[-split]

my_control <- trainControl(
  method="boot",
  number=25,
  savePredictions="final",
  classProbs=TRUE,
  index=index,
  summaryFunction=smFunc,
  preProc=c('YeoJohnson', 'center', 'scale')
  )

model_list <- caretList(
  Xtrain, ytrain,
  tuneLength=10,
  trControl=my_control,
  metric = mymetric,
  methodList=c('lda', 'rpart', 'glm', 'knn', 'svmRadial', 'LogitBoost', 'rf', 'xgbTree')
  )
```

```{r}
results <- resamples(model_list)
dotplot(results)
modelCor(results)
```

None of them is performing too well, but at least they're not too correlated. Does it get a bit better if we clean up the variables?

```{r}
Xtrain <- X_resid[ split, ]
ytrain <- y[ split ]
Xtest  <- X_resid[-split, ]
ytest = y[-split]

# only evaluate variables with p-value < .05
pvals = sapply(Xtrain, function(d) t.test(d ~ ytrain)$p.value)
Xtrain = Xtrain[pvals <= .05]
print(dim(Xtrain))

# now we can remove correlated and non-informative variables
nzv = nearZeroVar(Xtrain)
print(nzv)
if (length(nzv) > 0) {
  Xtrain = Xtrain[, -nzv]
}
correlations = cor(Xtrain, use='na.or.complete')
# library(corrplot)
# corrplot(correlations, order = "hclust")
highCorr = findCorrelation(correlations, cutoff=.75)
print(length(highCorr))
if (length(highCorr) > 0) {
  Xtrain = Xtrain[, -highCorr]
}

keep_me = sapply(colnames(Xtrain), function(d) which(colnames(Xtest) == d))
Xtest = Xtest[, keep_me]
```

```{r}
mymetric='Accuracy'
smFunc = defaultSummary

my_control <- trainControl(
  method="boot",
  number=25,
  savePredictions="final",
  classProbs=TRUE,
  index=index,
  summaryFunction=smFunc,
  preProc=c('YeoJohnson', 'center', 'scale')
  )

model_list <- caretList(
  Xtrain, ytrain,
  tuneLength=10,
  trControl=my_control,
  metric = mymetric,
  methodList=c('lda', 'rpart', 'glm', 'knn', 'svmRadial', 'LogitBoost', 'rf', 'xgbTree')
  )

results <- resamples(model_list)
dotplot(results)
modelCor(results)
```

It looks like there isn't much difference whether we clean it up or not, but the models get a lot more correlated if we have fewer variables. Is that true if we use ROC as well?

```{r}
mymetric='ROC'
smFunc = twoClassSummary

Xtrain <- X_resid[ split, ]
ytrain <- y[ split ]
Xtest  <- X_resid[-split, ]
ytest = y[-split]

my_control <- trainControl(
  method="boot",
  number=25,
  savePredictions="final",
  classProbs=TRUE,
  index=index,
  summaryFunction=smFunc,
  preProc=c('YeoJohnson', 'center', 'scale')
  )

model_list <- caretList(
  Xtrain, ytrain,
  tuneLength=10,
  trControl=my_control,
  metric = mymetric,
  methodList=c('lda', 'rpart', 'glm', 'knn', 'svmRadial', 'LogitBoost', 'rf', 'xgbTree')
  )
results <- resamples(model_list)
summary(results)
dotplot(results)
modelCor(results)
```

```{r}
Xtrain <- X_resid[ split, ]
ytrain <- y[ split ]
Xtest  <- X_resid[-split, ]
ytest = y[-split]

# only evaluate variables with p-value < .05
pvals = sapply(Xtrain, function(d) t.test(d ~ ytrain)$p.value)
Xtrain = Xtrain[pvals <= .05]
print(dim(Xtrain))

# now we can remove correlated and non-informative variables
nzv = nearZeroVar(Xtrain)
print(nzv)
if (length(nzv) > 0) {
  Xtrain = Xtrain[, -nzv]
}
correlations = cor(Xtrain, use='na.or.complete')
# library(corrplot)
# corrplot(correlations, order = "hclust")
highCorr = findCorrelation(correlations, cutoff=.75)
print(length(highCorr))
if (length(highCorr) > 0) {
  Xtrain = Xtrain[, -highCorr]
}

keep_me = sapply(colnames(Xtrain), function(d) which(colnames(Xtest) == d))
Xtest = Xtest[, keep_me]

mymetric='ROC'
smFunc = twoClassSummary

my_control <- trainControl(
  method="boot",
  number=25,
  savePredictions="final",
  classProbs=TRUE,
  index=index,
  summaryFunction=smFunc,
  preProc=c('YeoJohnson', 'center', 'scale')
  )

model_list <- caretList(
  Xtrain, ytrain,
  tuneLength=10,
  trControl=my_control,
  metric = mymetric,
  methodList=c('lda', 'rpart', 'glm', 'knn', 'svmRadial', 'LogitBoost', 'rf', 'xgbTree')
  )

results <- resamples(model_list)
summary(results)
dotplot(results)
modelCor(results)
```

Yep, same thing here. Using the small set of variables just makes the models way more correlated.

But in the results with the entire set of variables, svmRadial, lda, and xgbTree stand out, so maybe we could combine them in different ways?

```{r}
mymetric='ROC'
smFunc = twoClassSummary

Xtrain <- X_resid[ split, ]
ytrain <- y[ split ]
Xtest  <- X_resid[-split, ]
ytest = y[-split]

my_control <- trainControl(
  method="boot",
  number=25,
  savePredictions="final",
  classProbs=TRUE,
  index=index,
  summaryFunction=smFunc,
  preProc=c('YeoJohnson', 'center', 'scale')
  )

model_list <- caretList(
  Xtrain, ytrain,
  tuneLength=10,
  trControl=my_control,
  metric = mymetric,
  methodList=c('lda', 'svmRadial', 'xgbTree')
  )
results <- resamples(model_list)
summary(results)
dotplot(results)
modelCor(results)

greedy_ensemble <- caretEnsemble(
  model_list, 
  metric=mymetric,
  trControl=trainControl(
    number=2,
    summaryFunction=smFunc,
    classProbs=TRUE
    ))

glm_ensemble <- caretStack(
  model_list,
  method="glm",
  metric=mymetric,
  trControl=trainControl(
    method="boot",
    number=10,
    savePredictions="final",
    classProbs=TRUE,
    summaryFunction=smFunc
  )
)

gbm_ensemble <- caretStack(
  model_list,
  method="gbm",
  verbose=FALSE,
  tuneLength=10,
  metric=mymetric,
  trControl=trainControl(
    method="boot",
    number=50,
    savePredictions="final",
    classProbs=TRUE,
    summaryFunction=smFunc
  )
)
summary(greedy_ensemble)
summary(glm_ensemble)
summary(gbm_ensemble)
model_preds <- lapply(model_list, predict, newdata=Xtest, type="prob")
model_preds <- lapply(model_preds, function(x) x[,"ADHD"])
model_preds <- data.frame(model_preds)
ens_preds <- predict(greedy_ensemble, newdata=Xtest, type="prob")
model_preds$greedyE <- ens_preds
ens_preds <- predict(glm_ensemble, newdata=Xtest, type="prob")
model_preds$glmE <- ens_preds
ens_preds <- predict(gbm_ensemble, newdata=Xtest, type="prob")
model_preds$gbmE <- ens_preds
caTools::colAUC(model_preds, ytest)

```

Unfortunately, this is not working either... the combination doesn't do better in terms of AUC then the individual classifiers. Should we take a look at accuracy?

```{r}
Xtrain <- X_resid[ split, ]
ytrain <- y[ split ]
Xtest  <- X_resid[-split, ]
ytest = y[-split]

mymetric='Accuracy'
smFunc = defaultSummary

my_control <- trainControl(
  method="boot",
  number=25,
  savePredictions="final",
  classProbs=TRUE,
  index=index,
  summaryFunction=smFunc,
  preProc=c('YeoJohnson', 'center', 'scale')
  )

model_list <- caretList(
  Xtrain, ytrain,
  tuneLength=10,
  trControl=my_control,
  metric = mymetric,
  methodList=c('lda', 'rpart', 'glm', 'knn', 'svmRadial', 'LogitBoost', 'rf', 'xgbTree')
  )

results <- resamples(model_list)
dotplot(results)
modelCor(results)
```

There are very similar models, but not necessarily highly correlated. And many that are simply at chance. So, let's just pick the top 3 then:

