---
title: "Probing AD result"
output: html_notebook
---

We got an interesting result predicting baseline DX using AD voxelwise. Let's do some probing to see what's all about. For example:

* Is it still there after removing covariates?
* Does it get better by using smaller p-values?
* Does it get better by combining different classifiers?
* Does it get better by splitting the data differently?
* Does it make sense in the brain?

First, let's replicate the result:

```{r}
source('~/ncr_notebooks/baseline_prediction/src/aux_functions.R')
tract_data = read.csv('~/data/baseline_prediction/stripped/dti.csv')
load('~/data/baseline_prediction/dti/ad_voxelwise.RData')
dti_vdata = cbind(tract_data$maskid, ad_data)
gf_fname = '~/data/baseline_prediction/stripped/clinical.csv'
gf = read.csv(gf_fname)
gf_base = gf[gf$BASELINE=='BASELINE', ]
my_ids = intersect(gf_base$MRN, tract_data$MRN)
merged = mergeOnClosestDate(gf_base, tract_data, my_ids)
rm_me = abs(merged$dateX.minus.dateY.months) > 12
merged = merged[!rm_me, ]
dti_base_vdata = merge(merged$maskid, dti_vdata, by.x=1, by.y=1, all.y=F, all.x=T)

X = dti_base_vdata[, 2:ncol(dti_base_vdata)]
rm_me = colSums(is.na(X)) > 0
X = X[, !rm_me]

keep_me = merged$age <= 12
X = X[keep_me, ]
y = merged$DX_BASELINE
y[y!='NV'] = 'ADHD'
# twoClassSummary uses the first level as the case of interest (https://topepo.github.io/caret/measuring-performance.html#measures-for-class-probabilities)
y = factor(y, levels=c('ADHD', 'NV'))
y = y[keep_me]

myseed = 1234
set.seed(myseed)
split <- createDataPartition(y, p = .8, list = FALSE)
Xtrain <- X[ split, ]
ytrain <- y[ split ]
Xtest  <- X[-split, ]
ytest = y[-split]

library(parallel)
cl <- makeCluster(8)
pvals = parSapply(cl, Xtrain, function(d, ytrain) t.test(d ~ ytrain)$p.value, ytrain)
stopCluster(cl)
Xtrain = Xtrain[, which(pvals <= .05)]
print(dim(Xtrain))

keep_me = sapply(colnames(Xtrain), function(d) which(colnames(Xtest) == d))
Xtest = Xtest[, keep_me]

pp <- preProcess(Xtrain, method = c('BoxCox', 'center', 'scale', 'pca'), thresh=.9)
filtXtrain<- predict(pp, Xtrain)
filtXtest <- predict(pp, Xtest)
print(dim(filtXtrain))

tuneLength=10
set.seed(myseed)
index <- createMultiFolds(ytrain, k = 5, times = 5)

set.seed(myseed)
fullCtrl <- trainControl(method = "repeatedcv",
                         index = index,
                         savePredictions="final",
                         classProbs=TRUE,
                         summaryFunction=twoClassSummary)

library(caretEnsemble)
model_list <- caretList(
  filtXtrain, ytrain,
  tuneLength=10,
  trControl=fullCtrl,
  metric='ROC',
  methodList=c('rf', 'kernelpls', 'svmRadial', 'knn', 'rpart', 'bagEarthGCV', 'LogitBoost', 'lda', 'nb')
  )

greedy_ensemble <- caretEnsemble(
  model_list,
  metric='ROC',
  trControl=trainControl(
    number=2,
    summaryFunction=twoClassSummary,
    classProbs=TRUE
    ))
# ROC stats
summary(greedy_ensemble)
model_preds <- lapply(model_list, function(d, x, y) eval_model(d, x, y, c('ADHD', 'NV'))['ROC'], filtXtest, ytest)
model_preds <- data.frame(model_preds)
print(model_preds)
# sensitivity stats
model_preds <- lapply(model_list, function(d, x, y) eval_model(d, x, y, c('ADHD', 'NV'))['Sens'], filtXtest, ytest)
model_preds <- data.frame(model_preds)
print(model_preds)
# specificity stats
model_preds <- lapply(model_list, function(d, x, y) eval_model(d, x, y, c('ADHD', 'NV'))['Spec'], filtXtest, ytest)
model_preds <- data.frame(model_preds)
print(model_preds)
# Accuracy stats
model_preds <- lapply(model_list, predict, newdata=filtXtest)
model_preds <- lapply(model_preds, function(d, obs) postResample(d, obs)[1], obs=ytest)
model_preds <- data.frame(model_preds)
print(model_preds)
print(sprintf('No information rate: Accuracy=%f', max(table(ytrain)/length(ytrain))))
model_preds <- lapply(model_list, predict, newdata=filtXtest)
model_preds <- lapply(model_preds, function(d) confusionMatrix(d, ref=ytest)$overall['AccuracyPValue'])
model_preds <- data.frame(model_preds)
print(model_preds)
```

```
> summary(greedy_ensemble)
The following models were ensembled: rf, kernelpls, svmRadial, knn, rpart, bagEarthGCV, LogitBoost, lda, nb
They were weighted:
-15.2847 5.0599 25.0817 -2.6874 4.693 0.069 -2.7622 0.6201 0.1594 0.4355
The resulting ROC is: 0.9853
The fit for each individual model on the ROC is:
      method       ROC      ROCSD
          rf 0.9657759 0.02916561
   kernelpls 0.9836863 0.01701787
   svmRadial 0.8782745 0.05239643
         knn 0.9664874 0.02496917
       rpart 0.9080224 0.04176572
 bagEarthGCV 0.9691765 0.02479578
  LogitBoost 0.9541064 0.03195763
         lda 0.8275742 0.06684562
          nb 0.7949356 0.06302168
> print(model_preds)
           rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
ROC 0.6785714 0.7248677 0.7433862 0.5886243 0.5833333   0.6666667  0.6362434 0.7380952 0.7328042
> print(model_preds)
            rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
Sens 0.4285714 0.6666667 0.6190476 0.4761905 0.5238095   0.5238095  0.4761905 0.6190476 0.6190476
> print(model_preds)
            rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
Spec 0.7777778 0.8333333 0.7777778 0.6111111 0.6666667   0.8333333  0.6666667 0.6666667 0.7222222
> print(model_preds)
                rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
Accuracy 0.5897436 0.7435897 0.6923077 0.5384615 0.5897436   0.6666667  0.5641026 0.6410256 0.6666667
[1] "No information rate: Accuracy=0.534591"
> print(model_preds)  
                      rf   kernelpls  svmRadial      knn     rpart bagEarthGCV LogitBoost       lda         nb
AccuracyPValue 0.3164087 0.006998213 0.03719967 0.565258 0.3164087  0.07304316  0.4379397 0.1301001 0.07304316

```

# Covariates
Let's remove some covariates:

```{r}
get_needed_residuals = function(y, fm_str, cutoff, merged) {
  fm = as.formula(fm_str)
  fit = lm(fm)
  # selecting which covariates to use
  fm = "y ~ "
  for (r in 2:dim(summary(fit)$coefficients)[1]) {
    if (summary(fit)$coefficients[r, 4] < cutoff) {
      cname = rownames(summary(fit)$coefficients)[r]
      cname = gsub("SEXMale", "SEX", cname)
      fm = sprintf('%s + %s', fm, cname)
    }
  }
  # don't do anything if no variables were significant
  if (fm == 'y ~ ') {
    return(y)
  } else {
    opt_fit = lm(as.formula(fm))
    return(opt_fit$residuals)
  }
}

X = dti_base_vdata[, 2:ncol(dti_base_vdata)]
rm_me = colSums(is.na(X)) > 0
X = X[, !rm_me]
keep_me = merged$age <= 12
X = X[keep_me, ]
y = merged$DX_BASELINE
y[y!='NV'] = 'ADHD'
y = factor(y, levels=c('ADHD', 'NV'))
y = y[keep_me]
library(parallel)
cl <- makeCluster(8)
X_resid = parSapply(cl, X, get_needed_residuals, 'y ~ merged$age_at_scan + I(merged$age_at_scan^2) + merged$SEX', .1, merged[keep_me, ])
stopCluster(cl)
X_resid = as.data.frame(X_resid)

myseed = 1234
set.seed(myseed)
split <- createDataPartition(y, p = .8, list = FALSE)
Xtrain <- X_resid[ split, ]
ytrain <- y[ split ]
Xtest  <- X_resid[-split, ]
ytest = y[-split]
```

```
> summary(greedy_ensemble)
The following models were ensembled: rf, kernelpls, svmRadial, knn, rpart, bagEarthGCV, LogitBoost, lda, nb 
They were weighted: 
-14.3234 4.375 20.3875 -2.3583 5.1301 0.0272 -1.7721 0.6523 0.7884 1.4327
The resulting ROC is: 0.9904
The fit for each individual model on the ROC is: 
      method       ROC      ROCSD
          rf 0.9602017 0.03294265
   kernelpls 0.9826667 0.01854713
   svmRadial 0.8874510 0.05135665
         knn 0.9652213 0.03138885
       rpart 0.8868347 0.05869530
 bagEarthGCV 0.9702073 0.02930797
  LogitBoost 0.9407563 0.03274334
         lda 0.8485882 0.07011661
          nb 0.8469804 0.07398235
> print(model_preds)
           rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
ROC 0.6891534 0.7301587 0.7037037 0.6521164 0.5753968   0.6772487  0.6772487 0.7222222 0.7089947
> print(model_preds)
            rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
Sens 0.3809524 0.6190476 0.4761905 0.5714286 0.2857143   0.6190476  0.5714286 0.6190476 0.3809524
> print(model_preds)
            rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
Spec 0.7222222 0.7222222 0.7777778 0.6111111 0.7777778   0.7777778  0.6666667 0.7777778 0.8333333
> print(model_preds)
                rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
Accuracy 0.5384615 0.6666667 0.6153846 0.5897436 0.5128205   0.6923077  0.6153846 0.6923077 0.5897436
> print(model_preds)
                     rf  kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost        lda        nb
AccuracyPValue 0.565258 0.07304316 0.2116101 0.3164087 0.6858753  0.03719967  0.2116101 0.03719967 0.3164087
```
The two best models take a hit, but other two step up (lda and MARS). That makes me wonder whether other models might do better with the covariates... let's try it in a different note.

# smaller p-values
Let's try the cutoff  in residuals with p<.01:
```
> summary(greedy_ensemble)
The following models were ensembled: rf, kernelpls, svmRadial, knn, rpart, bagEarthGCV, LogitBoost, lda, nb
They were weighted:
-9.8693 -3.5951 14.7185 -3.4179 6.3666 1.8191 0.826 0.3428 0.9832 1.6655
The resulting ROC is: 0.9567
The fit for each individual model on the ROC is:
      method       ROC      ROCSD
          rf 0.9101625 0.04948903
   kernelpls 0.9572773 0.03749781
   svmRadial 0.8959888 0.05653903
         knn 0.9440392 0.03622723
       rpart 0.8174062 0.09082599
 bagEarthGCV 0.9354174 0.04059105
  LogitBoost 0.8910756 0.04525996
         lda 0.8950700 0.05319536
          nb 0.8518768 0.06068958
> print(model_preds)
           rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
ROC 0.5674603 0.6243386 0.6587302 0.6812169 0.5039683   0.6719577  0.6177249 0.6455026 0.6031746
> print(model_preds)
            rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
Sens 0.4285714 0.4285714 0.5238095 0.7619048 0.3809524   0.5714286  0.4761905 0.6190476 0.4761905
> print(model_preds)
            rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
Spec 0.7777778 0.6666667 0.6111111 0.6111111 0.6111111   0.6666667  0.6111111 0.6666667 0.5555556
> print(model_preds)
                rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
Accuracy 0.5897436 0.5384615 0.5641026 0.6923077 0.4871795   0.6153846  0.5384615 0.6410256 0.5128205
[1] "No information rate: Accuracy=0.534591"
> print(model_preds)  
                      rf kernelpls svmRadial        knn     rpart bagEarthGCV LogitBoost       lda        nb
AccuracyPValue 0.3164087  0.565258 0.4379397 0.03719967 0.7892616   0.2116101   0.565258 0.1301001 0.6858753
```
knn seems to benefit from it, but that's pretty much it. 

# different data splits
Maybe the 80-20 split is not the best thing to do. Let's see what try more or less:
## .9
```
> summary(greedy_ensemble)
The following models were ensembled: rf, kernelpls, svmRadial, knn, rpart, bagEarthGCV, LogitBoost, lda, nb
They were weighted:
-16.3196 -0.8059 33.6805 0.0088 2.7274 1.1607 -4.8603 0.5006 0.13 0.019
The resulting ROC is: 0.9812
The fit for each individual model on the ROC is:
      method       ROC      ROCSD
          rf 0.9421304 0.03182460
   kernelpls 0.9841970 0.01469873
   svmRadial 0.8741842 0.05808317
         knn 0.9599547 0.02549625
       rpart 0.8841347 0.07310946
 bagEarthGCV 0.9557368 0.03345064
  LogitBoost 0.9361633 0.02997070
         lda 0.8306885 0.07427827
          nb 0.8229002 0.04417482
> print(model_preds)
           rf kernelpls svmRadial knn     rpart bagEarthGCV LogitBoost       lda        nb
ROC 0.4444444 0.5888889 0.6222222 0.7 0.3722222   0.5111111  0.4277778 0.6222222 0.5888889
> print(model_preds)
      rf kernelpls svmRadial knn rpart bagEarthGCV LogitBoost lda  nb
Sens 0.6       0.6       0.5 0.7   0.6         0.5        0.4 0.7 0.5
> print(model_preds)
            rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
Spec 0.2222222 0.4444444 0.6666667 0.6666667 0.3333333   0.4444444  0.3333333 0.5555556 0.3333333
> print(model_preds)
                rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
Accuracy 0.4210526 0.5263158 0.5789474 0.6842105 0.4736842   0.4736842  0.3684211 0.6315789 0.4210526
[1] "No information rate: Accuracy=0.536313"
> print(model_preds)  
                      rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
AccuracyPValue 0.8746633 0.5919697 0.4110722 0.1248033 0.7547775   0.7547775  0.9465947 0.2466198 0.8746633
> 
```
## .7
```
> summary(greedy_ensemble)
The following models were ensembled: rf, kernelpls, svmRadial, knn, rpart, bagEarthGCV, LogitBoost, lda, nb 
They were weighted: 
-18.5463 5.4076 28.2392 -4.3276 12.193 -0.5249 -3.5551 -0.3097 -0.2054 -0.2827
The resulting ROC is: 0.9868
The fit for each individual model on the ROC is: 
      method       ROC      ROCSD
          rf 0.9408205 0.04222940
   kernelpls 0.9870769 0.01373810
   svmRadial 0.8637949 0.07431593
         knn 0.9828718 0.01973304
       rpart 0.7772308 0.06573278
 bagEarthGCV 0.9743590 0.02741712
  LogitBoost 0.9128205 0.04441156
         lda 0.8051282 0.06910799
          nb 0.6699487 0.10173683
> print(model_preds)
           rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
ROC 0.6129032 0.7753883 0.7598566 0.7144564 0.5609319   0.7228196  0.5896057 0.7491039 0.8016726
> print(model_preds)
            rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
Sens 0.6774194 0.7096774 0.7096774 0.5806452 0.6129032   0.6129032  0.6129032 0.7096774 0.5806452
> print(model_preds)
            rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
Spec 0.3703704 0.7407407 0.7407407 0.7037037 0.4444444   0.6666667  0.4814815 0.7407407 0.8518519
> print(model_preds)
                rf kernelpls svmRadial      knn     rpart bagEarthGCV LogitBoost       lda        nb
Accuracy 0.5344828 0.7241379 0.7241379 0.637931 0.5344828    0.637931  0.5517241 0.7241379 0.7068966
[1] "No information rate: Accuracy=0.535714"
> print(model_preds)
                      rf   kernelpls   svmRadial        knn     rpart bagEarthGCV LogitBoost         lda          nb
AccuracyPValue 0.5534668 0.002447212 0.002447212 0.07303894 0.5534668  0.07303894  0.4489039 0.002447212 0.005579516
```
## .6
```
> summary(greedy_ensemble)
The following models were ensembled: rf, kernelpls, svmRadial, knn, rpart, bagEarthGCV, LogitBoost, lda, nb 
They were weighted: 
-21.3862 2.0764 34.5666 -1.7804 8.7175 0.5419 -3.6783 0.8752 0.049 2.4444
The resulting ROC is: 0.9916
The fit for each individual model on the ROC is: 
      method       ROC      ROCSD
          rf 0.9697960 0.02865297
   kernelpls 0.9918939 0.01006466
   svmRadial 0.8945688 0.06521925
         knn 0.9830390 0.01928645
       rpart 0.8993871 0.06396391
 bagEarthGCV 0.9788034 0.01627045
  LogitBoost 0.9454507 0.03924833
         lda 0.8351787 0.10288280
          nb 0.8666628 0.06111336
> print(model_preds)
           rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
ROC 0.5806878 0.6507937 0.6309524 0.6521164 0.5181878   0.6329365  0.5906085 0.6164021 0.6488095
> print(model_preds)
            rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost      lda        nb
Sens 0.6904762       0.5  0.547619 0.6904762 0.5952381    0.547619  0.7142857 0.452381 0.6666667
> print(model_preds)
            rf kernelpls svmRadial knn     rpart bagEarthGCV LogitBoost       lda        nb
Spec 0.4444444 0.6388889 0.5555556 0.5 0.4444444   0.6666667  0.4722222 0.5833333 0.4444444
> print(model_preds)
                rf kernelpls svmRadial       knn    rpart bagEarthGCV LogitBoost       lda        nb
Accuracy 0.5769231 0.5641026 0.5512821 0.6025641 0.525641   0.6025641  0.6025641 0.5128205 0.5641026
[1] "No information rate: Accuracy=0.533333"
> print(model_preds)
                      rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
AccuracyPValue 0.2858988 0.3677682 0.4559866 0.1533324 0.6341833   0.1533324  0.1533324 0.7154538 0.3677682

```
## .5
```
> summary(greedy_ensemble)
The following models were ensembled: rf, kernelpls, svmRadial, knn, rpart, bagEarthGCV, LogitBoost, lda, nb
They were weighted:
-28.8512 -9.6025 58.5978 -7.1362 6.7822 1.9862 1.626 2.2839 0.3971 2.5153
The resulting ROC is: 0.9746
The fit for each individual model on the ROC is:
      method       ROC       ROCSD
          rf 0.9360727 0.076212681
   kernelpls 0.9979475 0.005091644
   svmRadial 0.9232364 0.053109324
         knn 0.9775091 0.036317207
       rpart 0.7690182 0.134311453
 bagEarthGCV 0.9801737 0.032568246
  LogitBoost 0.9471394 0.053366017
         lda 0.8478687 0.109181331
          nb 0.7465495 0.084548922
> print(model_preds)
          rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
ROC 0.590443 0.6341263 0.5910582 0.5687039 0.5319934   0.6279737  0.6058244 0.5968007 0.6415094
> print(model_preds)
            rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
Sens 0.6981132 0.5849057 0.6037736 0.6226415 0.6415094   0.7169811  0.6415094 0.5283019 0.7358491
> print(model_preds)
            rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
Spec 0.3478261 0.6086957       0.5 0.4565217 0.3913043   0.5434783  0.5217391 0.5652174 0.4130435
> print(model_preds)
                rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
Accuracy 0.5353535 0.5959596 0.5555556 0.5353535 0.5252525   0.6363636  0.5858586 0.5454545 0.5858586
[1] "No information rate: Accuracy=0.535354"
> print(model_preds)  
                      rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
AccuracyPValue 0.5410311 0.1336995  0.382141 0.4608436 0.6195125  0.02709875  0.1824064 0.4608436 0.1824064
```

It looks like .7 improves it when compared to .8, and .9 is bad. So, the less data for training, the better. But after .6 it's too little.

# Different cross-validation
## Bootstrap
```{r}
X = dti_base_vdata[, 2:ncol(dti_base_vdata)]
rm_me = colSums(is.na(X)) > 0
X = X[, !rm_me]
keep_me = merged$age <= 12
X = X[keep_me, ]
y = merged$DX_BASELINE
y[y!='NV'] = 'ADHD'
y = factor(y, levels=c('ADHD', 'NV'))
y = y[keep_me]
library(parallel)
cl <- makeCluster(8)
X_resid = parSapply(cl, X, get_needed_residuals, 'y ~ merged$age_at_scan + I(merged$age_at_scan^2) + merged$SEX', .1, merged[keep_me, ])
stopCluster(cl)
X_resid = as.data.frame(X_resid)

myseed = 1234
set.seed(myseed)
split <- createDataPartition(y, p = .7, list = FALSE)
Xtrain <- X_resid[ split, ]
ytrain <- y[ split ]
Xtest  <- X_resid[-split, ]
ytest = y[-split]

library(parallel)
cl <- makeCluster(8)
pvals = parSapply(cl, Xtrain, function(d, ytrain) t.test(d ~ ytrain)$p.value, ytrain)
stopCluster(cl)
Xtrain = Xtrain[, which(pvals <= .05)]
print(dim(Xtrain))

keep_me = sapply(colnames(Xtrain), function(d) which(colnames(Xtest) == d))
Xtest = Xtest[, keep_me]

pp <- preProcess(Xtrain, method = c('BoxCox', 'center', 'scale', 'pca'), thresh=.9)
filtXtrain<- predict(pp, Xtrain)
filtXtest <- predict(pp, Xtest)
print(dim(filtXtrain))

tuneLength=10
set.seed(myseed)
index <- createResample(ytrain, times=20)

set.seed(myseed)
fullCtrl <- trainControl(method = "boot",
                         index = index,
                         savePredictions="final",
                         classProbs=TRUE,
                         summaryFunction=twoClassSummary)

library(caretEnsemble)
model_list <- caretList(
  filtXtrain, ytrain,
  tuneLength=10,
  trControl=fullCtrl,
  metric='ROC',
  methodList=c('rf', 'kernelpls', 'svmRadial', 'knn', 'rpart', 'bagEarthGCV', 'LogitBoost', 'lda', 'nb')
  )

greedy_ensemble <- caretEnsemble(
  model_list,
  metric='ROC',
  trControl=trainControl(
    number=2,
    summaryFunction=twoClassSummary,
    classProbs=TRUE
    ))
# ROC stats
summary(greedy_ensemble)
model_preds <- lapply(model_list, function(d, x, y) eval_model(d, x, y, c('ADHD', 'NV'))['ROC'], filtXtest, ytest)
model_preds <- data.frame(model_preds)
print(model_preds)
# sensitivity stats
model_preds <- lapply(model_list, function(d, x, y) eval_model(d, x, y, c('ADHD', 'NV'))['Sens'], filtXtest, ytest)
model_preds <- data.frame(model_preds)
print(model_preds)
# specificity stats
model_preds <- lapply(model_list, function(d, x, y) eval_model(d, x, y, c('ADHD', 'NV'))['Spec'], filtXtest, ytest)
model_preds <- data.frame(model_preds)
print(model_preds)
# Accuracy stats
model_preds <- lapply(model_list, predict, newdata=filtXtest)
model_preds <- lapply(model_preds, function(d, obs) postResample(d, obs)[1], obs=ytest)
model_preds <- data.frame(model_preds)
print(model_preds)
print(sprintf('No information rate: Accuracy=%f', max(table(ytrain)/length(ytrain))))
model_preds <- lapply(model_list, predict, newdata=filtXtest)
model_preds <- lapply(model_preds, function(d) confusionMatrix(d, ref=ytest)$overall['AccuracyPValue'])
model_preds <- data.frame(model_preds)
print(model_preds)
```

```
> summary(greedy_ensemble)
The following models were ensembled: rf, kernelpls, svmRadial, knn, rpart, bagEarthGCV, LogitBoost, lda, nb
They were weighted:
-14.2382 -2.2002 29.6924 -3.4489 3.1558 0.2231 1.8437 -0.3213 0.0088 -1.0375
The resulting ROC is: 0.9859
The fit for each individual model on the ROC is:
      method       ROC      ROCSD
          rf 0.9147835 0.03748403
   kernelpls 0.9847816 0.01194969
   svmRadial 0.8162486 0.07501250
         knn 0.9622695 0.02045458
       rpart 0.7800822 0.08142968
 bagEarthGCV 0.9726140 0.02467350
  LogitBoost 0.9018980 0.02966611
         lda 0.6305338 0.06247505
          nb 0.6575119 0.07449278
> model_preds <- lapply(model_list, function(d, x, y) eval_model(d, x, y, c('ADHD', 'NV'))['ROC'], filtXtest, ytest)
There were 50 or more warnings (use warnings() to see the first 50)
> model_preds <- data.frame(model_preds)
> print(model_preds)
           rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
ROC 0.6427718 0.7753883 0.7622461 0.7144564 0.5609319   0.6857826  0.5896057 0.7491039 0.8016726
> # sensitivity stats
> model_preds <- lapply(model_list, function(d, x, y) eval_model(d, x, y, c('ADHD', 'NV'))['Sens'], filtXtest, ytest)
There were 50 or more warnings (use warnings() to see the first 50)
> model_preds <- data.frame(model_preds)
> print(model_preds)
            rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
Sens 0.7096774 0.7096774 0.7096774 0.5806452 0.6129032   0.5483871  0.6129032 0.7096774 0.5806452
> # specificity stats
> model_preds <- lapply(model_list, function(d, x, y) eval_model(d, x, y, c('ADHD', 'NV'))['Spec'], filtXtest, ytest)
There were 50 or more warnings (use warnings() to see the first 50)
> model_preds <- data.frame(model_preds)
> print(model_preds)
            rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
Spec 0.3703704 0.7407407 0.7407407 0.7037037 0.4444444   0.6296296  0.4814815 0.7407407 0.8518519
> # Accuracy stats
> model_preds <- lapply(model_list, predict, newdata=filtXtest)
There were 50 or more warnings (use warnings() to see the first 50)
> model_preds <- lapply(model_preds, function(d, obs) postResample(d, obs)[1], obs=ytest)
> model_preds <- data.frame(model_preds)
> print(model_preds)
                rf kernelpls svmRadial      knn     rpart bagEarthGCV LogitBoost       lda        nb
Accuracy 0.5517241 0.7241379 0.7241379 0.637931 0.5344828   0.5862069  0.5517241 0.7241379 0.7068966
> print(sprintf('No information rate: Accuracy=%f', max(table(ytrain)/length(ytrain))))
[1] "No information rate: Accuracy=0.535714"
> model_preds <- lapply(model_list, predict, newdata=filtXtest)
There were 50 or more warnings (use warnings() to see the first 50)
> model_preds <- lapply(model_preds, function(d) confusionMatrix(d, ref=ytest)$overall['AccuracyPValue'])
> model_preds <- data.frame(model_preds)
> print(model_preds)  
                      rf   kernelpls   svmRadial        knn     rpart bagEarthGCV LogitBoost         lda          nb
AccuracyPValue 0.4489039 0.002447212 0.002447212 0.07303894 0.5534668   0.2559767  0.4489039 0.002447212 0.005579516
```
This actually improved it by much! If we increase it to 50 times, is it better? 
```
The following models were ensembled: rf, kernelpls, svmRadial, knn, rpart, bagEarthGCV, LogitBoost, lda, nb
They were weighted:
-13.7443 -0.412 26.746 -3.1017 3.5998 0.23 0.9839 -0.1331 0.1373 -1.0221
The resulting ROC is: 0.987
The fit for each individual model on the ROC is:
      method       ROC      ROCSD
          rf 0.9089250 0.03178764
   kernelpls 0.9836369 0.01119002
   svmRadial 0.8088022 0.06618014
         knn 0.9661846 0.02012005
       rpart 0.7627947 0.07147589
 bagEarthGCV 0.9657260 0.02315925
  LogitBoost 0.8981949 0.03108831
         lda 0.5959503 0.09358236
          nb 0.6487113 0.07590389
> model_preds <- lapply(model_list, function(d, x, y) eval_model(d, x, y, c('ADHD', 'NV'))['ROC'], filtXtest, ytest)
There were 50 or more warnings (use warnings() to see the first 50)
> model_preds <- data.frame(model_preds)
> print(model_preds)
           rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
ROC 0.6326165 0.7753883 0.7598566 0.7144564 0.5609319   0.7037037  0.5591398 0.7491039 0.8016726
> # sensitivity stats
> model_preds <- lapply(model_list, function(d, x, y) eval_model(d, x, y, c('ADHD', 'NV'))['Sens'], filtXtest, ytest)
There were 50 or more warnings (use warnings() to see the first 50)
> model_preds <- data.frame(model_preds)
> print(model_preds)
            rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
Sens 0.6774194 0.7096774 0.7096774 0.5806452 0.6129032   0.6129032  0.5483871 0.7096774 0.5806452
> # specificity stats
> model_preds <- lapply(model_list, function(d, x, y) eval_model(d, x, y, c('ADHD', 'NV'))['Spec'], filtXtest, ytest)
There were 50 or more warnings (use warnings() to see the first 50)
> model_preds <- data.frame(model_preds)
> print(model_preds)
            rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
Spec 0.4444444 0.7407407 0.7407407 0.7037037 0.4444444   0.5555556  0.4814815 0.7407407 0.8518519
> # Accuracy stats
> model_preds <- lapply(model_list, predict, newdata=filtXtest)
There were 50 or more warnings (use warnings() to see the first 50)
> model_preds <- lapply(model_preds, function(d, obs) postResample(d, obs)[1], obs=ytest)
> model_preds <- data.frame(model_preds)
> print(model_preds)
                rf kernelpls svmRadial      knn     rpart bagEarthGCV LogitBoost       lda        nb
Accuracy 0.5689655 0.7241379 0.7241379 0.637931 0.5344828   0.5862069  0.5172414 0.7241379 0.7068966
> print(sprintf('No information rate: Accuracy=%f', max(table(ytrain)/length(ytrain))))
[1] "No information rate: Accuracy=0.535714"
> model_preds <- lapply(model_list, predict, newdata=filtXtest)
There were 50 or more warnings (use warnings() to see the first 50)
> model_preds <- lapply(model_preds, function(d) confusionMatrix(d, ref=ytest)$overall['AccuracyPValue'])
> model_preds <- data.frame(model_preds)
> print(model_preds)  
                      rf   kernelpls   svmRadial        knn     rpart bagEarthGCV LogitBoost         lda          nb
AccuracyPValue 0.3476086 0.002447212 0.002447212 0.07303894 0.5534668   0.2559767  0.6542952 0.002447212 0.005579516
```
Not much difference at all, so let's stick with 20 bootstrap.

# Combining models
Hard to tell. Not really, but then it might be better to run it several times first. 
```{r}
par(mfrow=c(2,2))
acc = read.table('~/tmp/acc_varp9.txt')
colnames(acc) = c('junk', 'rf', 'kernelpls', 'svmRadial', 'knn', 'rpart', 'bagEarthGCV', 'LogitBoost', 'lda', 'nb')
boxplot(acc[,2:ncol(acc)], las=2)
abline(h=.54, col='red')
title('Accuracy')
roc = read.table('~/tmp/roc_varp9.txt')
colnames(roc) = c('junk', 'rf', 'kernelpls', 'svmRadial', 'knn', 'rpart', 'bagEarthGCV', 'LogitBoost', 'lda', 'nb')
boxplot(roc[,2:ncol(roc)], las=2)
title('ROC')
sen = read.table('~/tmp/sen_varp9.txt')
colnames(sen) = c('junk', 'rf', 'kernelpls', 'svmRadial', 'knn', 'rpart', 'bagEarthGCV', 'LogitBoost', 'lda', 'nb')
boxplot(sen[,2:ncol(sen)], las=2)
title('Sensitivity')
spe = read.table('~/tmp/spe_varp9.txt')
colnames(spe) = c('junk', 'rf', 'kernelpls', 'svmRadial', 'knn', 'rpart', 'bagEarthGCV', 'LogitBoost', 'lda', 'nb')
boxplot(spe[,2:ncol(spe)], las=2)
title('Specificity')
```
```{r}
par(mfrow=c(2,2))
acc = read.table('~/tmp/acc_varp6.txt')
colnames(acc) = c('junk', 'rf', 'kernelpls', 'svmRadial', 'knn', 'rpart', 'bagEarthGCV', 'LogitBoost', 'lda', 'nb')
boxplot(acc[,2:ncol(acc)], las=2)
abline(h=.54, col='red')
title('Accuracy')
roc = read.table('~/tmp/roc_varp6.txt')
colnames(roc) = c('junk', 'rf', 'kernelpls', 'svmRadial', 'knn', 'rpart', 'bagEarthGCV', 'LogitBoost', 'lda', 'nb')
boxplot(roc[,2:ncol(roc)], las=2)
title('ROC')
sen = read.table('~/tmp/sen_varp6.txt')
colnames(sen) = c('junk', 'rf', 'kernelpls', 'svmRadial', 'knn', 'rpart', 'bagEarthGCV', 'LogitBoost', 'lda', 'nb')
boxplot(sen[,2:ncol(sen)], las=2)
title('Sensitivity')
spe = read.table('~/tmp/spe_varp6.txt')
colnames(spe) = c('junk', 'rf', 'kernelpls', 'svmRadial', 'knn', 'rpart', 'bagEarthGCV', 'LogitBoost', 'lda', 'nb')
boxplot(spe[,2:ncol(spe)], las=2)
title('Specificity')
```

# How much variance to explain
Philip was wondering how much variance do I need to explain in PCA. HE said I should be able to get by with 50 or 60. Let's go down in increments.

## 80
Now I have 68 PCs, inetad of 93. 
```
> summary(greedy_ensemble)
The following models were ensembled: rf, kernelpls, svmRadial, knn, rpart, bagEarthGCV, LogitBoost, lda, nb
They were weighted:
-13.8002 -1.2117 22.969 0.7351 3.88 -0.0591 3.3907 -0.5994 0.0941 -2.2888
The resulting ROC is: 0.9887
The fit for each individual model on the ROC is:
      method       ROC      ROCSD
          rf 0.9277175 0.03676759
   kernelpls 0.9870265 0.01041459
   svmRadial 0.9311513 0.04153439
         knn 0.9617368 0.02191891
       rpart 0.7780381 0.08621316
 bagEarthGCV 0.9738207 0.02403665
  LogitBoost 0.9238272 0.03470915
         lda 0.8863526 0.05178457
          nb 0.7484108 0.06731551
> model_preds <- lapply(model_list, function(d, x, y) eval_model(d, x, y, c('ADHD', 'NV'))['ROC'], filtXtest, ytest)
There were 50 or more warnings (use warnings() to see the first 50)
> model_preds <- data.frame(model_preds)
> print(model_preds)
           rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost      lda        nb
ROC 0.6224612 0.7646356 0.7514934 0.7682198 0.5609319   0.7108722   0.583632 0.739546 0.7634409
> # sensitivity stats
> model_preds <- lapply(model_list, function(d, x, y) eval_model(d, x, y, c('ADHD', 'NV'))['Sens'], filtXtest, ytest)
There were 50 or more warnings (use warnings() to see the first 50)
> model_preds <- data.frame(model_preds)
> print(model_preds)
            rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda       nb
Sens 0.6774194 0.6774194  0.516129 0.6129032 0.6129032   0.4516129  0.6451613 0.6451613 0.483871
> # specificity stats
> model_preds <- lapply(model_list, function(d, x, y) eval_model(d, x, y, c('ADHD', 'NV'))['Spec'], filtXtest, ytest)
There were 50 or more warnings (use warnings() to see the first 50)
> model_preds <- data.frame(model_preds)
> print(model_preds)
            rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
Spec 0.4444444 0.7777778 0.7777778 0.7037037 0.4444444   0.7037037  0.4444444 0.7777778 0.8148148
> # Accuracy stats
> model_preds <- lapply(model_list, predict, newdata=filtXtest)
There were 50 or more warnings (use warnings() to see the first 50)
> model_preds <- lapply(model_preds, function(d, obs) postResample(d, obs)[1], obs=ytest)
> model_preds <- data.frame(model_preds)
> print(model_preds)
                rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda       nb
Accuracy 0.5689655 0.7241379  0.637931 0.6551724 0.5344828   0.5689655  0.5517241 0.7068966 0.637931
> print(sprintf('No information rate: Accuracy=%f', max(table(ytrain)/length(ytrain))))
[1] "No information rate: Accuracy=0.535714"
> model_preds <- lapply(model_list, predict, newdata=filtXtest)
There were 50 or more warnings (use warnings() to see the first 50)
> model_preds <- lapply(model_preds, function(d) confusionMatrix(d, ref=ytest)$overall['AccuracyPValue'])
> model_preds <- data.frame(model_preds)
> print(model_preds)  
                      rf   kernelpls  svmRadial        knn     rpart bagEarthGCV LogitBoost         lda         nb
AccuracyPValue 0.3476086 0.002447212 0.07303894 0.04253971 0.5534668   0.3476086  0.4489039 0.005579516 0.07303894
```
## 70
Now I have 49 PCS.
```
> summary(greedy_ensemble)
The following models were ensembled: rf, kernelpls, svmRadial, knn, rpart, bagEarthGCV, LogitBoost, lda, nb
They were weighted:
-11.1986 -4.2006 17.1379 1.689 4.0183 0.3218 4.4423 0.3147 -0.3391 -1.9909
The resulting ROC is: 0.9877
The fit for each individual model on the ROC is:
      method       ROC      ROCSD
          rf 0.9287714 0.04099618
   kernelpls 0.9858498 0.01349226
   svmRadial 0.9606497 0.02790422
         knn 0.9634854 0.01631949
       rpart 0.7774469 0.08490535
 bagEarthGCV 0.9784120 0.02188355
  LogitBoost 0.9323185 0.03571818
         lda 0.9377823 0.03960110
          nb 0.8177752 0.06939518
> model_preds <- lapply(model_list, function(d, x, y) eval_model(d, x, y, c('ADHD', 'NV'))['ROC'], filtXtest, ytest)
There were 50 or more warnings (use warnings() to see the first 50)
> model_preds <- data.frame(model_preds)
> print(model_preds)
           rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
ROC 0.6427718 0.7741935 0.7741935 0.7311828 0.5609319   0.7216249  0.6421744 0.7682198 0.7741935
> # sensitivity stats
> model_preds <- lapply(model_list, function(d, x, y) eval_model(d, x, y, c('ADHD', 'NV'))['Sens'], filtXtest, ytest)
There were 50 or more warnings (use warnings() to see the first 50)
> model_preds <- data.frame(model_preds)
> print(model_preds)
            rf kernelpls svmRadial      knn     rpart bagEarthGCV LogitBoost       lda        nb
Sens 0.6774194 0.7419355 0.5806452 0.516129 0.6129032   0.5483871  0.6451613 0.7096774 0.3548387
> # specificity stats
> model_preds <- lapply(model_list, function(d, x, y) eval_model(d, x, y, c('ADHD', 'NV'))['Spec'], filtXtest, ytest)
There were 50 or more warnings (use warnings() to see the first 50)
> model_preds <- data.frame(model_preds)
> print(model_preds)
            rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda       nb
Spec 0.4444444 0.7037037 0.8148148 0.7407407 0.4444444   0.7037037  0.4814815 0.7407407 0.962963
> # Accuracy stats
> model_preds <- lapply(model_list, predict, newdata=filtXtest)
There were 50 or more warnings (use warnings() to see the first 50)
> model_preds <- lapply(model_preds, function(d, obs) postResample(d, obs)[1], obs=ytest)
> model_preds <- data.frame(model_preds)
> print(model_preds)
                rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda       nb
Accuracy 0.5689655 0.7241379 0.6896552 0.6206897 0.5344828   0.6206897  0.5689655 0.7241379 0.637931
> print(sprintf('No information rate: Accuracy=%f', max(table(ytrain)/length(ytrain))))
[1] "No information rate: Accuracy=0.535714"
> model_preds <- lapply(model_list, predict, newdata=filtXtest)
There were 50 or more warnings (use warnings() to see the first 50)
> model_preds <- lapply(model_preds, function(d) confusionMatrix(d, ref=ytest)$overall['AccuracyPValue'])
> model_preds <- data.frame(model_preds)
> print(model_preds)  
                      rf   kernelpls svmRadial        knn     rpart bagEarthGCV LogitBoost         lda         nb
AccuracyPValue 0.3476086 0.002447212 0.0117936 0.07303894 0.5534668   0.1177145  0.3476086 0.002447212 0.07303894

```
## 60
Down to 35 PCs.
```
> summary(greedy_ensemble)
The following models were ensembled: rf, kernelpls, svmRadial, knn, rpart, bagEarthGCV, LogitBoost, lda, nb 
They were weighted: 
-10.7748 -0.4235 13.7844 3.4304 3.2034 -0.508 3.2138 0.5385 0.2056 -2.7118
The resulting ROC is: 0.9876
The fit for each individual model on the ROC is: 
      method       ROC      ROCSD
          rf 0.9374941 0.03613215
   kernelpls 0.9876427 0.01226178
   svmRadial 0.9811865 0.01627397
         knn 0.9597349 0.01989230
       rpart 0.7746915 0.08173485
 bagEarthGCV 0.9796797 0.01675408
  LogitBoost 0.9460923 0.02858001
         lda 0.9801959 0.01713064
          nb 0.8651280 0.06086284
> model_preds <- lapply(model_list, function(d, x, y) eval_model(d, x, y, c('ADHD', 'NV'))['ROC'], filtXtest, ytest)
There were 50 or more warnings (use warnings() to see the first 50)
> model_preds <- data.frame(model_preds)
> print(model_preds)
           rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
ROC 0.6499403 0.7622461 0.7419355 0.7174432 0.5752688   0.7072879  0.6804062 0.7574671 0.7670251
> # sensitivity stats
> model_preds <- lapply(model_list, function(d, x, y) eval_model(d, x, y, c('ADHD', 'NV'))['Sens'], filtXtest, ytest)
There were 50 or more warnings (use warnings() to see the first 50)
> model_preds <- data.frame(model_preds)
> print(model_preds)
            rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
Sens 0.6451613 0.7096774 0.5483871 0.5483871 0.6129032   0.6451613  0.7096774 0.6774194 0.3225806
> # specificity stats
> model_preds <- lapply(model_list, function(d, x, y) eval_model(d, x, y, c('ADHD', 'NV'))['Spec'], filtXtest, ytest)
There were 50 or more warnings (use warnings() to see the first 50)
> model_preds <- data.frame(model_preds)
> print(model_preds)
            rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
Spec 0.4444444 0.7037037 0.7777778 0.7777778 0.4814815   0.5555556  0.5555556 0.6296296 0.9259259
> # Accuracy stats
> model_preds <- lapply(model_list, predict, newdata=filtXtest)
There were 50 or more warnings (use warnings() to see the first 50)
> model_preds <- lapply(model_preds, function(d, obs) postResample(d, obs)[1], obs=ytest)
> model_preds <- data.frame(model_preds)
> print(model_preds)
                rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
Accuracy 0.5517241 0.7068966 0.6551724 0.6551724 0.5517241   0.6034483   0.637931 0.6551724 0.6034483
> print(sprintf('No information rate: Accuracy=%f', max(table(ytrain)/length(ytrain))))
[1] "No information rate: Accuracy=0.535714"
> model_preds <- lapply(model_list, predict, newdata=filtXtest)
There were 50 or more warnings (use warnings() to see the first 50)
> model_preds <- lapply(model_preds, function(d) confusionMatrix(d, ref=ytest)$overall['AccuracyPValue'])
> model_preds <- data.frame(model_preds)
> print(model_preds)
                      rf   kernelpls  svmRadial        knn     rpart bagEarthGCV LogitBoost        lda        nb
AccuracyPValue 0.4489039 0.005579516 0.04253971 0.04253971 0.4489039   0.1786186 0.07303894 0.04253971 0.1786186
```
## 50
Down to 24 PCs.
```
The following models were ensembled: rf, kernelpls, svmRadial, knn, rpart, bagEarthGCV, LogitBoost, lda, nb
They were weighted:
-11.1914 0.4872 16.3603 3.9699 3.9733 -0.3919 1.502 -0.4089 -1.1608 -2.6551
The resulting ROC is: 0.9859
The fit for each individual model on the ROC is:
      method       ROC      ROCSD
          rf 0.9424070 0.02910898
   kernelpls 0.9845347 0.01328483
   svmRadial 0.9821615 0.01359390
         knn 0.9636181 0.01811838
       rpart 0.7771973 0.08421468
 bagEarthGCV 0.9676394 0.02299523
  LogitBoost 0.9292981 0.03244892
         lda 0.9811684 0.01387489
          nb 0.8936462 0.04970732
> model_preds <- lapply(model_list, function(d, x, y) eval_model(d, x, y, c('ADHD', 'NV'))['ROC'], filtXtest, ytest)
There were 50 or more warnings (use warnings() to see the first 50)
> model_preds <- data.frame(model_preds)
> print(model_preds)
           rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
ROC 0.6744325 0.7526882  0.734767 0.7532855 0.5454002   0.7467145  0.6798088 0.7383513 0.7634409
> # sensitivity stats
> model_preds <- lapply(model_list, function(d, x, y) eval_model(d, x, y, c('ADHD', 'NV'))['Sens'], filtXtest, ytest)
There were 50 or more warnings (use warnings() to see the first 50)
> model_preds <- data.frame(model_preds)
> print(model_preds)
            rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
Sens 0.6451613 0.6774194 0.5806452 0.5483871 0.5483871   0.6451613  0.6774194 0.6774194 0.3548387
> # specificity stats
> model_preds <- lapply(model_list, function(d, x, y) eval_model(d, x, y, c('ADHD', 'NV'))['Spec'], filtXtest, ytest)
There were 50 or more warnings (use warnings() to see the first 50)
> model_preds <- data.frame(model_preds)
> print(model_preds)
            rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
Spec 0.4444444 0.6296296 0.7777778 0.8148148 0.5185185   0.6666667  0.4814815 0.6666667 0.8888889
> # Accuracy stats
> model_preds <- lapply(model_list, predict, newdata=filtXtest)
There were 50 or more warnings (use warnings() to see the first 50)
> model_preds <- lapply(model_preds, function(d, obs) postResample(d, obs)[1], obs=ytest)
> model_preds <- data.frame(model_preds)
> print(model_preds)
                rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
Accuracy 0.5517241 0.6551724 0.6724138 0.6896552 0.5344828   0.6551724  0.5862069 0.6724138 0.6034483
> print(sprintf('No information rate: Accuracy=%f', max(table(ytrain)/length(ytrain))))
[1] "No information rate: Accuracy=0.535714"
> model_preds <- lapply(model_list, predict, newdata=filtXtest)
There were 50 or more warnings (use warnings() to see the first 50)
> model_preds <- lapply(model_preds, function(d) confusionMatrix(d, ref=ytest)$overall['AccuracyPValue'])
> model_preds <- data.frame(model_preds)
> print(model_preds)  
                      rf  kernelpls  svmRadial       knn     rpart bagEarthGCV LogitBoost        lda        nb
AccuracyPValue 0.4489039 0.04253971 0.02318786 0.0117936 0.5534668  0.04253971  0.2559767 0.02318786 0.1786186
```
It looks like different models are more succeptible to the different cutoffs than others, but the overall best result didn't change.

# Plotting in the brain
Here, we'll need to have a binary map to go from all voxels to the univariate first. Then, project the PC into those univariate voxels. Finally, multiply by the importance, if necessary.
```{r}
brain = rep(0, ncol(X))
brain2 = rep(0, ncol(X))
brain[pvals<=.05] = pp$rotation[,'PC2']
imp = sort(varImp(model_list$kernelpls)$importance[,1], decreasing=T, index.return=T)
brain2[pvals<=.05] = imp$x[1]*pp$rotation[,imp$ix[1]] + imp$x[2]*pp$rotation[,imp$ix[2]] + imp$x[3]*pp$rotation[,imp$ix[3]]
ijk = read.table('~/data/tmp/2021_fa.txt')
ijk[, 4] = brain
write.table(ijk, row.names=F, col.names=F, file='~/tmp/brain.txt')
# cat brain.txt | 3dUndump -master ~/data/dtitk/analysis/mean_fa_skeleton_mask.nii.gz -datum float -prefix brain.nii -overwrite -
```

# Reduce bias
Talking to Philip, we would want to report different seeds. So, we need to make sure we have a decent looking error (bias + variance). 