---
title: "DTI for 3 group and SX baseline"
output: html_notebook
---

Let's first plot the DTI results we're comparing to, before we do the age restriction analysis:

```{r}
source('~/ncr_notebooks/baseline_prediction/src/aux_functions.R')
root_fname = '~/data/baseline_prediction/results/stripped_allDTI_NVvsADHD_big'
do_metrics_plots(root_fname)
```

Now let's see if separating it into 3 groups makes any difference:

```{r}
tract_data = read.csv('~/data/baseline_prediction/stripped/dti.csv')
rm_me = (tract_data$fa_avg < .4 | tract_data$ad_avg < 1.18 | tract_data$rd_avg > .65 | tract_data$rd_avg < .5 |
         tract_data$norm.trans > .45 | tract_data$norm.rot > .008 | tract_data$goodSlices < 45 | tract_data$goodSlices > 70)
print(sprintf('Reducing from %d to %d scans', nrow(tract_data), nrow(tract_data)-sum(rm_me)))
tract_data = tract_data[!rm_me, ]
gf_fname = '~/data/baseline_prediction/stripped/clinical.csv'
gf = read.csv(gf_fname)
gf = gf[gf$BASELINE=='BASELINE', ]
my_ids = intersect(gf$MRN, tract_data$MRN)
merged = mergeOnClosestDate(gf, tract_data, my_ids)
rm_me = abs(merged$dateX.minus.dateY.months) > 12
print(sprintf('Reducing from %d to %d scans', nrow(merged), nrow(merged)-sum(rm_me)))
merged = merged[!rm_me, ]
```

The distribution looks like this:
```{r}
library(ggplot2)
ggplot(merged) + geom_point(aes(1:nrow(merged), GAS, color=merged$DX_BASELINE))
```
Wendy has already looked over those outliers, so let's just go ahead and split them based on GAS:

```{r}
myc = kmeans(merged[merged$DX_BASELINE != 'NV',]$GAS, 2)
print(sprintf('Cutoff: %.2f', myc$centers[1] + diff(myc$centers)/2))
```

```{r}
library(caret)
library(randomForest)
seed = 107
phen_vars = c(which(grepl("^FA_", colnames(merged))),
              which(grepl("^AD_", colnames(merged))),
              which(grepl("^RD_", colnames(merged))),
              which(grepl("^MO_", colnames(merged)))
              )
X = merged[, phen_vars]
y = as.character(merged$DX_BASELINE)
y[y != 'NV'] = 'ADHD'
y[y == 'ADHD' & merged$GAS > 68.91] = 'ADHD_high'
y[y == 'ADHD'] = 'ADHD_low'
y = factor(y, levels=c('NV', 'ADHD_low', 'ADHD_high'))
# remove subjects with NA
rm_me = rowSums(is.na(X)) > 0
X = X[!rm_me, ]
y = y[!rm_me]
```

Let's do some ML now:

```{r}
# ldata = X
# groups = y
root_fname = '~/data/baseline_prediction/results/stripped_allDTI_3group'
# save(ldata, groups, file=sprintf('%s.RData', root_fname), compress=T)

# runInCluster(root_fname, train_test_ratio=.7, cpuDiff=1, nsplits=5, nrepeatcv=5, run_models=c('rndForest', 'rsvm'))

do_metrics_plots(root_fname)
```

The issue here is that we would have to look at AUC, because accuracy is shot by having almost 50% NVs. Let's take a look at regression:

```{r}
seed = 107
phen_vars = c(which(grepl("^FA_", colnames(merged))),
              which(grepl("^AD_", colnames(merged))),
              which(grepl("^RD_", colnames(merged))),
              which(grepl("^MO_", colnames(merged)))
              )
X = merged[, phen_vars]
y = merged$SX_inatt
# remove subjects with NA
rm_me = rowSums(is.na(X)) > 0
X = X[!rm_me, ]
y = y[!rm_me]
```

```{r}
myseed = 107
cpuDiff = 1
tuneLength = 10
default_preproc = c("center", 'scale')
library(caret)
set.seed(myseed)
split <- createDataPartition(y, p = .8, list = FALSE)

Xtrain <- X[ split, ]
ytrain <- y[ split ]
Xtest  <- X[-split, ]
ytest = y[-split]

set.seed(myseed)
index <- createMultiFolds(ytrain, times = 5)

library(doMC)
ncpus <- detectBatchCPUs()
njobs <- ncpus - cpuDiff
registerDoMC(njobs)
selFunc = 'best'  # maybe try oneSE and tolerance as well?

fullCtrl <- trainControl(method = "repeatedcv",
                         repeats = 5,
                         index = index,
                         search='grid',
                         selectionFunction = selFunc)

set.seed(myseed)
rfFull <- train(Xtrain, ytrain,
                method = "rf",
                tuneLength = tuneLength,
                ntree = 1000,
                trControl = fullCtrl,
                preProcess = default_preproc)
rfFull

set.seed(myseed)
blaFull <- train(Xtrain, ytrain,
                method = "blassoAveraged",
                tuneLength = tuneLength,
                trControl = fullCtrl,
                preProcess = default_preproc)
blaFull

set.seed(myseed)
enetFull <- train(Xtrain, ytrain,
                method = "enet",
                tuneLength = tuneLength,
                trControl = fullCtrl,
                preProcess = default_preproc)
enetFull

set.seed(myseed)
penFull <- train(Xtrain, ytrain,
                method = "penalized",
                tuneLength = tuneLength,
                trControl = fullCtrl,
                preProcess = default_preproc)
penFull

set.seed(myseed)
briFull <- train(Xtrain, ytrain,
                method = "bridge",
                tuneLength = tuneLength,
                trControl = fullCtrl,
                preProcess = default_preproc)
briFull

set.seed(myseed)
svmLFull <- train(Xtrain, ytrain,
                method = "svmLinear2",
                tuneLength = tuneLength,
                trControl = fullCtrl,
                preProcess = default_preproc)
svmLFull

set.seed(myseed)
svmRFull <- train(Xtrain, ytrain,
                method = "svmRadial",
                tuneLength = tuneLength,
                trControl = fullCtrl,
                preProcess = default_preproc)
svmRFull

set.seed(myseed)
xgbFull <- train(Xtrain, ytrain,
                method = "xgbTree",
                tuneLength = tuneLength,
                trControl = fullCtrl,
                preProcess = default_preproc)
xgbFull

resamps <- resamples(list(rndForest = rfFull,
                          bayLasso = blaFull,
                          elasticN = enetFull,
                          penalize = penFull,
                          bayRidge = briFull,
                          svmLinear = svmLFull,
                          svmRadial = svmRFull,
                          XGB = xgbFull))

bwplot(resamps)  # dotplot(resamps) shoes 95% CI
difValues <- diff(resamps)
summary(difValues)
bwplot(difValues)

# evaluate test set using postResample(predict(rfFull, Xtest), obs = ytest)

```

Now we try out some RFE:

```{r}
varSeq = 1:ncol(X)
ctrl <- rfeControl(method = "repeatedcv", repeats = 5,
                   saveDetails = TRUE,
                   index = index,
                   returnResamp = "final")

ctrl$functions <- rfFuncs
set.seed(myseed)
rfRFE <- rfe(Xtrain, ytrain,
             sizes = varSeq,
             rfeControl = ctrl)
rfRFE

cvCtrl <- trainControl(method = "cv",
                       verboseIter = FALSE,
                       allowParallel = FALSE)
ctrl$functions <- caretFuncs

rndForest = rfFull,
                          bayLasso = blaFull,
                          elasticN = enetFull,
                          penalize = penFull,
                          bayRidge = briFull,
                          svmLinear = svmLFull,
                          svmRadial = svmRFull,

set.seed(myseed)
blaRFE <- rfe(Xtrain, ytrain,
              sizes = varSeq,
              rfeControl = ctrl,
              method = "blassoAveraged",
              tuneLength = tuneLength,
              preProc = default_preproc,
              trControl = cvCtrl)
blaRFE

set.seed(myseed)
enetRFE <- rfe(Xtrain, ytrain,
              sizes = varSeq,
              rfeControl = ctrl,
              method = "enet",
              tuneLength = tuneLength,
              preProc = default_preproc,
              trControl = cvCtrl)
enetRFE

set.seed(myseed)
penRFE <- rfe(Xtrain, ytrain,
              sizes = varSeq,
              rfeControl = ctrl,
              method = "penalyzed",
              tuneLength = tuneLength,
              preProc = default_preproc,
              trControl = cvCtrl)
penRFE

set.seed(myseed)
briRFE <- rfe(Xtrain, ytrain,
              sizes = varSeq,
              rfeControl = ctrl,
              method = "bridge",
              tuneLength = tuneLength,
              preProc = default_preproc,
              trControl = cvCtrl)
briRFE

set.seed(myseed)
svmLRFE <- rfe(Xtrain, ytrain,
              sizes = varSeq,
              rfeControl = ctrl,
              method = "svmLinear2",
              tuneLength = tuneLength,
              preProc = default_preproc,
              trControl = cvCtrl)
svmLRFE

set.seed(myseed)
svmRRFE <- rfe(Xtrain, ytrain,
              sizes = varSeq,
              rfeControl = ctrl,
              method = "svmRadial",
              tuneLength = tuneLength,
              preProc = default_preproc,
              trControl = cvCtrl)
svmRRFE

set.seed(myseed)
xgbRFE <- rfe(Xtrain, ytrain,
              sizes = varSeq,
              rfeControl = ctrl,
              method = "xgbTree",
              tuneLength = tuneLength,
              preProc = default_preproc,
              trControl = cvCtrl)
xgbRFE

```

```{r}
# rm_me = (merged$age_at_scan > 9) | (rowSums(is.na(X)) > 0)
# ldata = X[!rm_me, ]
# groups = y[!rm_me]
root_fname = '~/data/baseline_prediction/results/stripped_allDTISE9_NVvsADHD'
# save(ldata, groups, file=sprintf('%s.RData', root_fname), compress=T)
# 
#runInCluster(root_fname, train_test_ratio=.8, cpuDiff=0, nsplits=30, nrepeatcv=5, run_models=c('rndForest', 'lr', 'xgb'))
do_metrics_plots(root_fname)
```

```{r}
# rm_me = (merged$age_at_scan > 8) | (rowSums(is.na(X)) > 0)
# ldata = X[!rm_me, ]
# groups = y[!rm_me]
root_fname = '~/data/baseline_prediction/results/stripped_allDTISE8_NVvsADHD'
# save(ldata, groups, file=sprintf('%s.RData', root_fname), compress=T)

#runInCluster(root_fname, train_test_ratio=.8, cpuDiff=0, nsplits=30, nrepeatcv=5, run_models=c('rndForest', 'lr', 'xgb'))

do_metrics_plots(root_fname)
```

The results are a bit concerning. It looks like we need all that data, in particular the data from the older kids, in order to make a baseline prediction. Not sure how clinically informative that is. Should we try to really fit our models within the < 8 y.o range, and then expand it?

The trick is that I only have 76 scans < 8, but 120 < 9. That's a more manageable number that we could work with in better fitting classifiers.

Another way of getting more scans would be to possibly derive the visual thresholds from histograms in the age subgroups. Maybe that would savage  some of the data?

# Deeper exploring of < 9 y.o. scans

What does the histogram look like?

```{r}
tract_data = read.csv('~/data/baseline_prediction/stripped/dti.csv')
tract_data = tract_data[tract_data$age_at_scan <= 9, ]
par(mfrow=c(2, 3))
nbreaks = 40
hist(tract_data$fa_avg, breaks=nbreaks)
hist(tract_data$ad_avg, breaks=nbreaks)
hist(tract_data$rd_avg, breaks=nbreaks)
hist(tract_data$norm.trans, breaks=nbreaks)
hist(tract_data$norm.rot, breaks=nbreaks)
hist(tract_data$goodSlices, breaks=nbreaks)
```

It turns out that it doesn't make that much difference in the cut-offs, at least visually.

```{r}
rm_me = (tract_data$fa_avg < .4 | tract_data$ad_avg < 1.18 | tract_data$rd_avg > .65 | tract_data$rd_avg < .5 |
         tract_data$norm.trans > .45 | tract_data$norm.rot > .008 | tract_data$goodSlices < 45 | tract_data$goodSlices > 70)
print(sprintf('Reducing from %d to %d scans', nrow(tract_data), nrow(tract_data)-sum(rm_me)))
tract_data = tract_data[!rm_me, ]
```
``` {r}
gf_fname = '~/data/baseline_prediction/stripped/clinical.csv'
gf = read.csv(gf_fname)
gf = gf[gf$BASELINE=='BASELINE', ]
my_ids = intersect(gf$MRN, tract_data$MRN)
merged = mergeOnClosestDate(gf, tract_data, my_ids)
rm_me = abs(merged$dateX.minus.dateY.months) > 12
print(sprintf('Reducing from %d to %d scans', nrow(merged), nrow(merged)-sum(rm_me)))
merged = merged[!rm_me, ]
```

OK, so we arrived at the same number as before. Do we have any NAs?

```{r}
phen_vars = c(which(grepl("^FA_", colnames(merged))),
              which(grepl("^AD_", colnames(merged))),
              which(grepl("^RD_", colnames(merged))),
              which(grepl("^MO_", colnames(merged)))
              )
X = merged[, phen_vars]
y = merged$DX_BASELINE
y[y != 'NV'] = 'ADHD'
y = factor(y)
sum(rowSums(is.na(X)) > 0)
```

Nope. Now, let's do a more in-depth study of this data using Random Forests:

```{r}
library(caret)
library(randomForest)
seed = 107
inTraining <- createDataPartition(y, p = .75, list = FALSE)
training <- cbind(X[ inTraining,], y[inTraining])
testing  <- cbind(X[-inTraining,], y[-inTraining])
colnames(training)[ncol(X) + 1] = 'Class'
colnames(testing)[ncol(X) + 1] = 'Class'
```

```{r}
fitControl <- trainControl(## 10-fold CV
                           method = "oob",
                           number = 10,
                           ## repeated ten times
                           repeats = 10)
set.seed(seed)
rndForestGrid <- expand.grid(.mtry=c(1:sqrt(ncol(X))))
myFit <- train(Class ~ ., data = training, 
                 method = "rf", 
                 trControl = fitControl,
               tuneGrid=rndForestGrid,
               # search='grid',
               # tuneLength=10,
               preProcess = c('nzv', 'pca'),
              verbose=F)
myFit
```

It doesn't look like we're going to improve much here... let's see how well we do with xgb though:

```{r}
fitControl <- trainControl(## 10-fold CV
                           method = "repeatedcv",
                           number = 10,
                           ## repeated ten times
                           repeats = 10)
set.seed(seed)
myFit <- train(Class ~ ., data = training, 
                 method = "xgbTree", 
                 trControl = fitControl,
               # tuneGrid=rndForestGrid,
               search='grid',
               tuneLength=10,
               preProcess = c('nzv', 'pca'),
              verbose=F)
myFit
```

Also not great... 