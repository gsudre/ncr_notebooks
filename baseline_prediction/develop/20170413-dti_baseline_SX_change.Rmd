---
title: "DTI baseline and symptom change"
output: html_notebook
---

One option we have is to try to predict symptom change from baseline, which would be the end goal anyways. Here, we could go for symptom slope, delta, or corrected delta. Also, it could use the last time point, or just the next one. Finally, we can restrict it to ADHD only, or include NVs in the mix. Let's play a bit with these options, and maybe how different model ensembles perform.

```{r}
source('~/ncr_notebooks/baseline_prediction/src/aux_functions.R')

tract_data = read.csv('~/data/baseline_prediction/stripped/dti.csv')
rm_me = (tract_data$fa_avg < .4 | tract_data$ad_avg < 1.18 | tract_data$rd_avg > .65 | tract_data$rd_avg < .5 |
           tract_data$norm.trans > .45 | tract_data$norm.rot > .008 | tract_data$goodSlices < 45 |
           tract_data$goodSlices > 70)
tract_data = tract_data[!rm_me, ]
gf_fname = '~/data/baseline_prediction/stripped/clinical.csv'
gf = read.csv(gf_fname)
gf_base = gf[gf$BASELINE=='BASELINE', ]
my_ids = intersect(gf_base$MRN, tract_data$MRN)
merged = mergeOnClosestDate(gf_base, tract_data, my_ids)
rm_me = abs(merged$dateX.minus.dateY.months) > 12
merged = merged[!rm_me, ]

phen_vars = c(which(grepl("^FA_", colnames(merged))),
              which(grepl("^AD_", colnames(merged))),
              which(grepl("^RD_", colnames(merged))),
              which(grepl("^MO_", colnames(merged)))
)
X = merged[, phen_vars]
base_class = merged$DX_BASELINE
base_class[base_class != 'NV'] = 'ADHD'
base_class = factor(base_class, levels = c('NV', 'ADHD'))

# just so we have more options for regressors, remove features with NAs
rm_me = colSums(is.na(X)) > 0
X = X[, !rm_me]
```

Now let's define a few different targets. First, we start with some basic SX slope:

```{r}
y = vector()
target_col = which(colnames(gf)=='SX_inatt')
for (s in merged$MRN) {
  idx = gf$MRN==s
  sx_slope = lm(gf[idx, target_col] ~ gf[idx, ]$age)$coefficients[2]
  y = c(y, sx_slope)
}
```

Let's see what we can get here.

```{r}
myseed = 1234
cpuDiff = 1
tuneLength = 10
default_preproc = c("center", 'scale')

library(doMC)
ncpus <- detectBatchCPUs()
njobs <- ncpus - cpuDiff
registerDoMC(njobs)

set.seed(myseed)
split <- createDataPartition(y, p = .8, list = FALSE)
Xtrain <- X[ split, ]
ytrain <- y[ split ]
Xtest  <- X[-split, ]
ytest = y[-split]

set.seed(myseed)
index <- createResample(ytrain, 25)

print(sprintf('No information rate: RMSE=%f', postResample(mean(ytrain), ytest)[1]))

my_control <- trainControl(
  method="boot",
  number=25,
  savePredictions="final",
  index=index
  )

library("caretEnsemble")
model_list <- caretList(
  Xtrain, ytrain,
  tuneLength=10,
  trControl=my_control,
  methodList=c('svmRadial', 'rf', 'xgbTree')
  )

greedy_ensemble <- caretEnsemble(
  model_list, 
  trControl=trainControl(
    number=2
    ))

glm_ensemble <- caretStack(
  model_list,
  method="glm",
  trControl=trainControl(
    method="boot",
    number=10,
    savePredictions="final"
  )
)

gbm_ensemble <- caretStack(
  model_list,
  method="gbm",
  verbose=FALSE,
  tuneLength=10,
  trControl=trainControl(
    method="boot",
    number=10,
    savePredictions="final"
  )
)
summary(greedy_ensemble)
summary(glm_ensemble)
summary(gbm_ensemble)
model_preds <- lapply(model_list, predict, newdata=Xtest)
model_preds <- lapply(model_preds, RMSE, obs=ytest)
model_preds <- data.frame(model_preds)
ens_preds <- predict(greedy_ensemble, newdata=Xtest)
model_preds$greedyE <- RMSE(ens_preds, ytest)
ens_preds <- predict(glm_ensemble, newdata=Xtest)
model_preds$glmE <- RMSE(ens_preds, ytest)
ens_preds <- predict(gbm_ensemble, newdata=Xtest)
model_preds$gbmE <- RMSE(ens_preds, ytest)
```


So, we're not doing better than predicting the mean. Now, it makes sense to only use ADHDs, as we wouldn't realistically put a NV kid in the scanner to see how her symptoms would evolve. Let's try that. 

```{r}
y = vector()
target_col = which(colnames(gf)=='SX_inatt')
for (s in merged[base_class=='ADHD',]$MRN) {
  idx = gf$MRN==s
  sx_slope = lm(gf[idx, target_col] ~ gf[idx, ]$age)$coefficients[2]
  y = c(y, sx_slope)
}
X = X[base_class=='ADHD', ]
```

```{r}
myseed = 1234
cpuDiff = 1
tuneLength = 10
default_preproc = c("center", 'scale')

library(doMC)
ncpus <- detectBatchCPUs()
njobs <- ncpus - cpuDiff
registerDoMC(njobs)

set.seed(myseed)
split <- createDataPartition(y, p = .8, list = FALSE)
Xtrain <- X[ split, ]
ytrain <- y[ split ]
Xtest  <- X[-split, ]
ytest = y[-split]

set.seed(myseed)
index <- createResample(ytrain, 25)

print(sprintf('No information rate: RMSE=%f', postResample(mean(ytrain), ytest)[1]))

my_control <- trainControl(
  method="boot",
  number=25,
  savePredictions="final",
  index=index
  )

library("caretEnsemble")
model_list <- caretList(
  Xtrain, ytrain,
  tuneLength=10,
  trControl=my_control,
  methodList=c('svmRadial', 'rf', 'xgbTree')
  )

greedy_ensemble <- caretEnsemble(
  model_list, 
  trControl=trainControl(
    number=2
    ))

glm_ensemble <- caretStack(
  model_list,
  method="glm",
  trControl=trainControl(
    method="boot",
    number=10,
    savePredictions="final"
  )
)

gbm_ensemble <- caretStack(
  model_list,
  method="gbm",
  verbose=FALSE,
  tuneLength=10,
  trControl=trainControl(
    method="boot",
    number=10,
    savePredictions="final"
  )
)
summary(greedy_ensemble)
summary(glm_ensemble)
summary(gbm_ensemble)
model_preds <- lapply(model_list, predict, newdata=Xtest)
model_preds <- lapply(model_preds, RMSE, obs=ytest)
model_preds <- data.frame(model_preds)
ens_preds <- predict(greedy_ensemble, newdata=Xtest)
model_preds$greedyE <- RMSE(ens_preds, ytest)
ens_preds <- predict(glm_ensemble, newdata=Xtest)
model_preds$glmE <- RMSE(ens_preds, ytest)
ens_preds <- predict(gbm_ensemble, newdata=Xtest)
model_preds$gbmE <- RMSE(ens_preds, ytest)
print(model_preds)
```

Still not good... according to Philip, we could try restricting the age of first assessment to < 12, and/or only look at people with sx >= 2, which would also add some NVs.

```{r}
X = merged[, phen_vars]
rm_me = colSums(is.na(X)) > 0
X = X[, !rm_me]

y = vector()
keep_me = vector()
target_col = which(colnames(gf)=='SX_inatt')
for (s in merged$MRN) {
  if ((gf_base[gf_base$MRN==s,]$age <= 12) && (gf_base[gf_base$MRN==s, target_col] >= 2)) {
    idx = gf$MRN==s
    sx_slope = lm(gf[idx, target_col] ~ gf[idx, ]$age)$coefficients[2]
    keep_me = c(keep_me, which(merged$MRN == s))
    y = c(y, sx_slope)
  }
}
X = X[keep_me, ]
```

```{r}
myseed = 1234
cpuDiff = 1
tuneLength = 10
default_preproc = c("center", 'scale')

library(doMC)
ncpus <- detectBatchCPUs()
njobs <- ncpus - cpuDiff
registerDoMC(njobs)

set.seed(myseed)
split <- createDataPartition(y, p = .8, list = FALSE)
Xtrain <- X[ split, ]
ytrain <- y[ split ]
Xtest  <- X[-split, ]
ytest = y[-split]

set.seed(myseed)
index <- createResample(ytrain, 25)

print(sprintf('No information rate: RMSE=%f', postResample(mean(ytrain), ytest)[1]))

my_control <- trainControl(
  method="boot",
  number=25,
  savePredictions="final",
  index=index
  )

library("caretEnsemble")
model_list <- caretList(
  Xtrain, ytrain,
  tuneLength=10,
  trControl=my_control,
  methodList=c('svmRadial', 'rf', 'xgbTree')
  )

greedy_ensemble <- caretEnsemble(
  model_list, 
  trControl=trainControl(
    number=2
    ))

glm_ensemble <- caretStack(
  model_list,
  method="glm",
  trControl=trainControl(
    method="boot",
    number=10,
    savePredictions="final"
  )
)

gbm_ensemble <- caretStack(
  model_list,
  method="gbm",
  verbose=FALSE,
  tuneLength=10,
  trControl=trainControl(
    method="boot",
    number=10,
    savePredictions="final"
  )
)
summary(greedy_ensemble)
summary(glm_ensemble)
summary(gbm_ensemble)
model_preds <- lapply(model_list, predict, newdata=Xtest)
model_preds <- lapply(model_preds, RMSE, obs=ytest)
model_preds <- data.frame(model_preds)
ens_preds <- predict(greedy_ensemble, newdata=Xtest)
model_preds$greedyE <- RMSE(ens_preds, ytest)
ens_preds <- predict(glm_ensemble, newdata=Xtest)
model_preds$glmE <- RMSE(ens_preds, ytest)
ens_preds <- predict(gbm_ensemble, newdata=Xtest)
model_preds$gbmE <- RMSE(ens_preds, ytest)
print(model_preds)
```

Still nothing... maybe single voxel would help?

```{r}
source('~/ncr_notebooks/baseline_prediction/src/aux_functions.R')
tract_data = read.csv('~/data/baseline_prediction/stripped/dti.csv')
load('~/data/baseline_prediction/dti/fa_voxelwise.RData')
gf_fname = '~/data/baseline_prediction/stripped/clinical.csv'
gf = read.csv(gf_fname)
gf_base = gf[gf$BASELINE=='BASELINE', ]
my_ids = intersect(gf_base$MRN, tract_data$MRN)
merged = mergeOnClosestDate(gf_base, tract_data, my_ids)
rm_me = abs(merged$dateX.minus.dateY.months) > 12
merged = merged[!rm_me, ]
dti_base_vdata = merge(merged$maskid, fa_data, by.x=1, by.y=1, all.y=F, all.x=T)
```

Now we have to do some filtering, as we have way too many variables in the voxel analysis:

```{r}
X = dti_base_vdata[, 2:ncol(dti_base_vdata)]
rm_me = colSums(is.na(X)) > 0
X = X[, !rm_me]

y = vector()
keep_me = vector()
target_col = which(colnames(gf)=='SX_inatt')
for (s in merged$MRN) {
  if ((gf_base[gf_base$MRN==s,]$age <= 12) && (gf_base[gf_base$MRN==s, target_col] >= 2)) {
    idx = gf$MRN==s
    sx_slope = lm(gf[idx, target_col] ~ gf[idx, ]$age)$coefficients[2]
    keep_me = c(keep_me, which(merged$MRN == s))
    y = c(y, sx_slope)
  }
}
X = X[keep_me, ]

myseed = 1234

set.seed(myseed)
split <- createDataPartition(y, p = .8, list = FALSE)

Xtrain <- X[ split, ]
ytrain <- y[ split ]
Xtest  <- X[-split, ]
ytest = y[-split]

# in voxel analysis it's actually more costly to do the correlation in the entire set of variables
# then to run the for loop, so let's first reduce the variables to only the univariate ones
pvals = sapply(Xtrain, function(d) cor.test(d, ytrain)$p.value)
Xtrain = Xtrain[, which(pvals <= .05)]
print(dim(Xtrain))

# now we can remove correlated and non-informative variables
nzv = nearZeroVar(Xtrain)
print(nzv)
if (length(nzv) > 0) {
  Xtrain = Xtrain[, -nzv]
}
correlations = cor(Xtrain, use='na.or.complete')
# library(corrplot)
# corrplot(correlations, order = "hclust")
highCorr = findCorrelation(correlations, cutoff=.75)
print(length(highCorr))
if (length(highCorr) > 0) {
  Xtrain = Xtrain[, -highCorr]
}
print(dim(Xtrain))

keep_me = sapply(colnames(Xtrain), function(d) which(colnames(Xtest) == d))
Xtest = Xtest[, keep_me]
```

Now we're ready for some regression...

```{r}
cpuDiff=1
tuneLength=10
mymod='rf'

set.seed(myseed)
index <- createMultiFolds(ytrain, k = 5, times = 5)

library(doMC)
ncpus <- detectBatchCPUs()
njobs <- ncpus - cpuDiff
registerDoMC(njobs)

set.seed(myseed)
fullCtrl <- trainControl(method = "repeatedcv",
                         index = index,
                         savePredictions="final")
m1 <- train(Xtrain, ytrain,
            method = mymod,
            trControl = fullCtrl,
            tuneLength = tuneLength)
print(m1)
pred = predict(m1, Xtest)
print(postResample(pred, ytest))
print(sprintf('No information rate: RMSE=%f', postResample(mean(ytrain), ytest)[1]))
```

Well, at least our trained RF model does better than the no information rate. It just doesn't predict well.

Let's see if using other methods works better, with this type of filter selection. The other option would be to try wrapper methods for feature selection... more to come.

```{r}
library(caretEnsemble)
model_list <- caretList(
  Xtrain, ytrain,
  tuneLength=10,
  trControl=fullCtrl,
  # when doing classification, also try 'LogitBoost', 'lda', and 'nb'
  methodList=c('rf', 'kernelpls', 'svmRadial', 'lasso', 'knn', 'rpart', 'bagEarthGCV')
  )

greedy_ensemble <- caretEnsemble(
  model_list, 
  trControl=trainControl(
    number=2
    ))

glm_ensemble <- caretStack(
  model_list,
  method="glm",
  trControl=trainControl(
    method="boot",
    number=10,
    savePredictions="final"
  )
)

gbm_ensemble <- caretStack(
  model_list,
  method="gbm",
  verbose=FALSE,
  tuneLength=10,
  trControl=trainControl(
    method="boot",
    number=10,
    savePredictions="final"
  )
)
summary(greedy_ensemble)
model_preds <- lapply(model_list, predict, newdata=Xtest)
model_preds <- lapply(model_preds, function(d, obs) postResample(d, obs)[1], obs=ytest)
model_preds <- data.frame(model_preds)
ens_preds <- predict(greedy_ensemble, newdata=Xtest)
model_preds$greedyE <- postResample(ens_preds, ytest)[1]
ens_preds <- predict(glm_ensemble, newdata=Xtest)
model_preds$glmE <- postResample(ens_preds, ytest)[1]
ens_preds <- predict(gbm_ensemble, newdata=Xtest)
model_preds$gbmE <- postResample(ens_preds, ytest)[1]
print(model_preds)
print(sprintf('No information rate: RMSE=%f', postResample(mean(ytrain), ytest)[1]))
```

Differently than the structural data, our trained models can do better than the NIR, but they just don't generalize too well:

```
> summary(greedy_ensemble)
The following models were ensembled: rf, kernelpls, svmRadial, lasso, knn, rpart, bagEarthGCV
They were weighted:
-0.0408 -0.2308 1.174 0.4966 -0.2964 -0.2609 -0.0135 -0.0488
The resulting RMSE is: 0.549
The fit for each individual model on the RMSE is:
      method      RMSE    RMSESD
          rf 1.0068311 0.2068528
   kernelpls 0.5691138 0.0880059
   svmRadial 0.7625152 0.1906792
       lasso 0.9737567 0.1640132
         knn 0.9109432 0.1822245
       rpart 1.3329387 0.2132794
 bagEarthGCV 0.9712748 0.1708745
> model_preds <- lapply(model_list, predict, newdata=Xtest)
> model_preds <- lapply(model_preds, function(d, obs) postResample(d, obs)[1], obs=ytest)
> model_preds <- data.frame(model_preds)
> ens_preds <- predict(greedy_ensemble, newdata=Xtest)
> model_preds$greedyE <- postResample(ens_preds, ytest)[1]
> ens_preds <- predict(glm_ensemble, newdata=Xtest)
> model_preds$glmE <- postResample(ens_preds, ytest)[1]
> ens_preds <- predict(gbm_ensemble, newdata=Xtest)
> model_preds$gbmE <- postResample(ens_preds, ytest)[1]
> print(model_preds)
           rf kernelpls svmRadial    lasso      knn    rpart bagEarthGCV  greedyE     glmE     gbmE
RMSE 1.482455  1.540107  1.516895 1.633493 1.459702 1.609111     1.48728 1.545074 1.545074 1.610505
> print(sprintf('No information rate: RMSE=%f', postResample(mean(ytrain), ytest)[1]))
[1] "No information rate: RMSE=1.361248"
```

Before we try wrapping methods, as they're more computationally intensive, let's see if we can do better with either HI slopes, or any of the two types of outcomes:

## HI slopes

```
The following models were ensembled: rf, kernelpls, svmRadial, lasso, knn, rpart, bagEarthGCV
They were weighted:
-0.0301 -0.4389 0.4473 0.7253 0.066 0.4456 -0.0379 -0.191
The resulting RMSE is: 0.7677
The fit for each individual model on the RMSE is:
      method      RMSE    RMSESD
          rf 0.9389438 0.2156183
   kernelpls 0.7842068 0.1239863
   svmRadial 0.8016045 0.1429486
       lasso 0.9542475 0.1423419
         knn 0.8466594 0.1468682
       rpart 1.2596263 0.2136318
 bagEarthGCV 0.9509955 0.2132654
> model_preds <- lapply(model_list, predict, newdata=Xtest)
> model_preds <- lapply(model_preds, function(d, obs) postResample(d, obs)[1], obs=ytest)
> model_preds <- data.frame(model_preds)
> ens_preds <- predict(greedy_ensemble, newdata=Xtest)
> model_preds$greedyE <- postResample(ens_preds, ytest)[1]
> ens_preds <- predict(glm_ensemble, newdata=Xtest)
> model_preds$glmE <- postResample(ens_preds, ytest)[1]
> ens_preds <- predict(gbm_ensemble, newdata=Xtest)
> model_preds$gbmE <- postResample(ens_preds, ytest)[1]
> print(model_preds)
            rf kernelpls svmRadial    lasso       knn     rpart bagEarthGCV  greedyE     glmE      gbmE
RMSE 0.9813692  1.048233  1.038616 1.144592 0.9040527 0.9595734   0.9919127 1.019822 1.019822 0.9700945
> print(sprintf('No information rate: RMSE=%f', postResample(mean(ytrain), ytest)[1]))
[1] "No information rate: RMSE=0.959573"
```
As usual, DTI does quite well during training, but test data prediction is crap.

## inatt3 outcomes
Just to have a better comparison to the results above, we'll restrict ourselves to kids < 12 y.o. Also, note that caretEnsemble has not been implemented for multiclass problems. So, we'll need to just use caretList to make running the multiple models easier, but stop there.

```{r}
X = dti_base_vdata[, 2:ncol(dti_base_vdata)]
rm_me = colSums(is.na(X)) > 0
X = X[, !rm_me]

keep_me = merged$age <= 12
X = X[keep_me, ]
y = merged$inatt3_named
y = factor(y, levels=c('low', 'medium', 'high'))
y = y[keep_me]

myseed = 1234
set.seed(myseed)
split <- createDataPartition(y, p = .8, list = FALSE)
Xtrain <- X[ split, ]
ytrain <- y[ split ]
Xtest  <- X[-split, ]
ytest = y[-split]

# in voxel analysis it's actually more costly to do the correlation in the entire set of variables
# then to run the for loop, so let's first reduce the variables to only the univariate ones
pvals = sapply(Xtrain, function(d) summary(aov(lm(d ~ ytrain)))[[1]][[5]][1])
Xtrain = Xtrain[, which(pvals <= .05)]
print(dim(Xtrain))

# now we can remove correlated and non-informative variables
nzv = nearZeroVar(Xtrain)
print(nzv)
if (length(nzv) > 0) {
  Xtrain = Xtrain[, -nzv]
}
correlations = cor(Xtrain, use='na.or.complete')
# # library(corrplot)
# # corrplot(correlations, order = "hclust")
highCorr = findCorrelation(correlations, cutoff=.75)
print(length(highCorr))
if (length(highCorr) > 0) {
  Xtrain = Xtrain[, -highCorr]
}
print(dim(Xtrain))

keep_me = sapply(colnames(Xtrain), function(d) which(colnames(Xtest) == d))
Xtest = Xtest[, keep_me]

tuneLength=10
set.seed(myseed)
index <- createMultiFolds(ytrain, k = 5, times = 5)

set.seed(myseed)
fullCtrl <- trainControl(method = "repeatedcv",
                         index = index,
                         savePredictions="final")

library(caretEnsemble)
model_list <- caretList(
  Xtrain, ytrain,
  tuneLength=10,
  trControl=fullCtrl,
  methodList=c('rf', 'kernelpls', 'svmRadial', 'knn', 'rpart', 'bagEarthGCV', 'LogitBoost', 'lda', 'nb')
  )

model_perf = data.frame(lapply(model_list, function(d) getTrainPerf(d)[1]))
names(model_perf) = names(model_list)
model_preds <- lapply(model_list, predict, newdata=Xtest)
model_preds <- lapply(model_preds, function(d, obs) postResample(d, obs)[1], obs=ytest)
model_preds <- data.frame(model_preds)
names(model_preds) = names(model_list)
print(model_perf)
print(model_preds)
print(sprintf('No information rate: Accuracy=%f', max(table(ytrain)/length(ytrain))))
```
The DTI voxelwise results are not so great:
```
> names(model_preds) = names(model_list)
> print(model_perf)
         rf kernelpls svmRadial       knn    rpart bagEarthGCV LogitBoost       lda        nb
1 0.6162121 0.7777603 0.7822532 0.6211009 0.405154   0.3868308  0.5922029 0.7127004 0.6309091
> print(model_preds)
                rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
Accuracy 0.3684211 0.4473684 0.3684211 0.4473684 0.4210526   0.3684211  0.3055556 0.3947368 0.3947368
> print(sprintf('No information rate: Accuracy=%f', max(table(ytrain)/length(ytrain))))
[1] "No information rate: Accuracy=0.437500"
```

## HI3 outcomes

Here we do something similar, except that the initial preparation of variables is slightly different:

```{r}
X = dti_base_vdata[, 2:ncol(dti_base_vdata)]
rm_me = colSums(is.na(X)) > 0
X = X[, !rm_me]

keep_me = merged$age <= 12
X = X[keep_me, ]
y = merged$HI3_named
y = factor(y)
y = y[keep_me]
```

Again, DTI seems to have overfit the training data, and didn't generalize too well:

```
> print(model_perf)
         rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda       nb
1 0.5972939 0.8790643 0.8683574 0.6485088 0.4853569   0.4519685  0.7047288 0.7741815 0.764938
> print(model_preds)
                rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
Accuracy 0.4473684 0.4473684 0.4210526 0.5789474 0.5526316         0.5  0.4324324 0.3421053 0.3684211
> print(sprintf('No information rate: Accuracy=%f', max(table(ytrain)/length(ytrain))))
[1] "No information rate: Accuracy=0.537500"
```

But what happens if we reduce out outcome categories to only the two affected ones?

## inatt2 outcome
```{r}
X = dti_base_vdata[, 2:ncol(dti_base_vdata)]
rm_me = colSums(is.na(X)) > 0
X = X[, !rm_me]

keep_me = merged$age <= 12
X = X[keep_me, ]
y = merged$inatt3_named
y = factor(y, levels=c('low', 'medium', 'high'))
y = y[keep_me]
keep_me = y!='low'
y = y[keep_me]
y = factor(y, levels=c('medium', 'high'))
X = X[keep_me, ]

myseed = 1234
set.seed(myseed)
split <- createDataPartition(y, p = .8, list = FALSE)
Xtrain <- X[ split, ]
ytrain <- y[ split ]
Xtest  <- X[-split, ]
ytest = y[-split]

# in voxel analysis it's actually more costly to do the correlation in the entire set of variables
# then to run the for loop, so let's first reduce the variables to only the univariate ones
pvals = sapply(Xtrain, function(d) t.test(d ~ ytrain)$p.value)
Xtrain = Xtrain[, which(pvals <= .05)]
print(dim(Xtrain))

# now we can remove correlated and non-informative variables
nzv = nearZeroVar(Xtrain)
print(nzv)
if (length(nzv) > 0) {
  Xtrain = Xtrain[, -nzv]
}
correlations = cor(Xtrain, use='na.or.complete')
# # library(corrplot)
# # corrplot(correlations, order = "hclust")
highCorr = findCorrelation(correlations, cutoff=.75)
print(length(highCorr))
if (length(highCorr) > 0) {
  Xtrain = Xtrain[, -highCorr]
}
print(dim(Xtrain))

keep_me = sapply(colnames(Xtrain), function(d) which(colnames(Xtest) == d))
Xtest = Xtest[, keep_me]

tuneLength=10
set.seed(myseed)
index <- createMultiFolds(ytrain, k = 5, times = 5)

set.seed(myseed)
fullCtrl <- trainControl(method = "repeatedcv",
                         index = index,
                         savePredictions="final",
                         classProbs=TRUE)

library(caretEnsemble)
model_list <- caretList(
  Xtrain, ytrain,
  tuneLength=10,
  trControl=fullCtrl,
  methodList=c('rf', 'kernelpls', 'svmRadial', 'knn', 'rpart', 'bagEarthGCV', 'LogitBoost', 'lda', 'nb')
  )

greedy_ensemble <- caretEnsemble(
  model_list, 
  trControl=trainControl(
    number=2
    ))

glm_ensemble <- caretStack(
  model_list,
  method="glm",
  trControl=trainControl(
    method="boot",
    number=10,
    savePredictions="final"
  )
)

gbm_ensemble <- caretStack(
  model_list,
  method="gbm",
  verbose=FALSE,
  tuneLength=10,
  trControl=trainControl(
    method="boot",
    number=10,
    savePredictions="final"
  )
)
summary(greedy_ensemble)
model_preds <- lapply(model_list, predict, newdata=Xtest)
model_preds <- lapply(model_preds, function(d, obs) postResample(d, obs)[1], obs=ytest)
model_preds <- data.frame(model_preds)
ens_preds <- predict(greedy_ensemble, newdata=Xtest)
model_preds$greedyE <- postResample(ens_preds, ytest)[1]
ens_preds <- predict(glm_ensemble, newdata=Xtest)
model_preds$glmE <- postResample(ens_preds, ytest)[1]
ens_preds <- predict(gbm_ensemble, newdata=Xtest)
model_preds$gbmE <- postResample(ens_preds, ytest)[1]
print(model_preds)
print(sprintf('No information rate: Accuracy=%f', max(table(ytrain)/length(ytrain))))
```

Extreme overfitting in training, with poor generalization... maybe tuning would do better here, potentially using some bootstrapping. We should play with it a bit, as there's clearly structure to be learned.

```
The following models were ensembled: rf, kernelpls, svmRadial, knn, rpart, bagEarthGCV, LogitBoost, lda, nb 
They were weighted: 
-11.8037 -2.1358 29.9478 24.8082 -8.2705 -1.3687 -6.1678 -4.4142 4.2058 -11.6812
The resulting Accuracy is: 0.9615
The fit for each individual model on the Accuracy is: 
      method  Accuracy AccuracySD
          rf 0.8094255 0.07788699
   kernelpls 0.9636601 0.04284163
   svmRadial 0.9555005 0.04217257
         knn 0.9528999 0.04492246
       rpart 0.5604403 0.11708714
 bagEarthGCV 0.7913175 0.08521121
  LogitBoost 0.7409632 0.10738042
         lda 0.8930788 0.06883364
          nb 0.8998899 0.06773090
> model_preds <- lapply(model_list, predict, newdata=Xtest)
> model_preds <- lapply(model_preds, function(d, obs) postResample(d, obs)[1], obs=ytest)
> model_preds <- data.frame(model_preds)
> ens_preds <- predict(greedy_ensemble, newdata=Xtest)
> model_preds$greedyE <- postResample(ens_preds, ytest)[1]
> ens_preds <- predict(glm_ensemble, newdata=Xtest)
> model_preds$glmE <- postResample(ens_preds, ytest)[1]
> ens_preds <- predict(gbm_ensemble, newdata=Xtest)
> model_preds$gbmE <- postResample(ens_preds, ytest)[1]
> print(model_preds)
                rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb   greedyE      glmE      gbmE
Accuracy 0.6666667 0.5238095 0.6666667 0.5714286 0.5714286   0.6190476  0.5238095 0.4761905 0.6666667 0.4761905 0.4761905 0.5714286
> print(sprintf('No information rate: Accuracy=%f', max(table(ytrain)/length(ytrain))))
[1] "No information rate: Accuracy=0.600000"
```

## HI2 outcome
The code for HI is very similar, except that we crop a different factor out:

```{r}
X = dti_base_vdata[, 2:ncol(dti_base_vdata)]
rm_me = colSums(is.na(X)) > 0
X = X[, !rm_me]

keep_me = merged$age <= 12
X = X[keep_me, ]
y = merged$HI3_named
y = y[keep_me]
keep_me = y!='never_affected'
y = y[keep_me]
y = factor(y, levels=c('rapid_improvers', 'severe'))
X = X[keep_me, ]
```

Still overfitting by a lot:

```
The following models were ensembled: rf, kernelpls, svmRadial, knn, rpart, bagEarthGCV, LogitBoost, lda, nb 
They were weighted: 
-76.1551 -80.1719 186.3185 68.9 -43.5882 -2.9825 7.0536 -34.0637 32.1973 6.7523
The resulting Accuracy is: 1
The fit for each individual model on the Accuracy is: 
      method  Accuracy AccuracySD
          rf 0.7734286 0.07645255
   kernelpls 0.9919762 0.02221411
   svmRadial 0.9674048 0.03966168
         knn 0.9545714 0.04863370
       rpart 0.5610714 0.09194390
 bagEarthGCV 0.7572381 0.09111789
  LogitBoost 0.7454524 0.10266187
         lda 0.9407143 0.05274467
          nb 0.9403333 0.06253525
> model_preds <- lapply(model_list, predict, newdata=Xtest)
> model_preds <- lapply(model_preds, function(d, obs) postResample(d, obs)[1], obs=ytest)
> model_preds <- data.frame(model_preds)
> ens_preds <- predict(greedy_ensemble, newdata=Xtest)
> model_preds$greedyE <- postResample(ens_preds, ytest)[1]
> ens_preds <- predict(glm_ensemble, newdata=Xtest)
> model_preds$glmE <- postResample(ens_preds, ytest)[1]
> ens_preds <- predict(gbm_ensemble, newdata=Xtest)
> model_preds$gbmE <- postResample(ens_preds, ytest)[1]
> print(model_preds)
                rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb   greedyE      glmE      gbmE
Accuracy 0.5882353 0.5882353 0.5882353 0.5882353 0.4705882   0.5294118  0.5882353 0.5882353 0.5294118 0.5882353 0.5882353 0.6470588
> print(sprintf('No information rate: Accuracy=%f', max(table(ytrain)/length(ytrain))))
[1] "No information rate: Accuracy=0.648649"
>
```

# Built-in feature selection 
Let's play a bit with algorithms that have built-in feature selection. We'll go with the 3-case inatt because it had some interesting results (with structural data), and we can see how well they survive:

## inatt3
```{r}
X = dti_base_vdata[, 2:ncol(dti_base_vdata)]
rm_me = colSums(is.na(X)) > 0
X = X[, !rm_me]

keep_me = merged$age <= 12
X = X[keep_me, ]
y = merged$inatt3_named
y = factor(y, levels=c('low', 'medium', 'high'))
y = y[keep_me]

myseed = 1234
set.seed(myseed)
split <- createDataPartition(y, p = .8, list = FALSE)
Xtrain <- X[ split, ]
ytrain <- y[ split ]
Xtest  <- X[-split, ]
ytest = y[-split]

tuneLength=10
set.seed(myseed)
index <- createMultiFolds(ytrain, k = 5, times = 5)

set.seed(myseed)
fullCtrl <- trainControl(method = "repeatedcv",
                         index = index,
                         savePredictions="final")

# here we should be able to burn through our CPUs without running out of memory if we stick to 8 core machines.
# I'm at 54Gb memory in struct and 8 cores, so I guess I cannot go to 16 in 100Gb of ram. DTI is only at 7. Maybe some PCA?
# I'm actually up to 82Gb now (don't know what method). And DTI is at 24.
library(doMC)
registerDoMC(cores = 8)
set.seed(myseed)
library(caretEnsemble)
model_list <- caretList(
  Xtrain, ytrain,
  tuneLength=10,
  trControl=fullCtrl,
  methodList=c('rf', 'svmRadial', 'bagEarthGCV', 'LogitBoost', 'rpart', 'PenalizedLDA')
  )


model_perf = data.frame(lapply(model_list, function(d) getTrainPerf(d)[1]))
names(model_perf) = names(model_list)
model_preds <- lapply(model_list, predict, newdata=Xtest)
model_preds <- lapply(model_preds, function(d, obs) postResample(d, obs)[1], obs=ytest)
model_preds <- data.frame(model_preds)
names(model_preds) = names(model_list)
print(model_perf)
print(model_preds)
print(sprintf('No information rate: Accuracy=%f', max(table(ytrain)/length(ytrain))))
```

Not much there:

```
> print(model_perf)
         rf svmRadial bagEarthGCV LogitBoost     rpart PenalizedLDA
1 0.4711987 0.4624035   0.4155763   0.426629 0.4177395    0.4376711
> print(model_preds)
                rf svmRadial bagEarthGCV LogitBoost     rpart PenalizedLDA
Accuracy 0.4210526 0.3947368   0.2894737  0.4117647 0.4473684    0.4473684
> print(sprintf('No information rate: Accuracy=%f', max(table(ytrain)/length(ytrain))))
[1] "No information rate: Accuracy=0.437500"
```

## HI3

Not much here either:
```
> print(model_perf)
         rf svmRadial bagEarthGCV LogitBoost     rpart PenalizedLDA
1 0.5326012 0.5376415   0.4802382  0.4657237 0.4975556    0.5376415
> print(model_preds)
                rf svmRadial bagEarthGCV LogitBoost     rpart PenalizedLDA
Accuracy 0.5789474 0.5526316   0.4736842  0.4722222 0.5526316    0.5526316
> print(sprintf('No information rate: Accuracy=%f', max(table(ytrain)/length(ytrain))))
[1] "No information rate: Accuracy=0.537500"
```
Maybe a combination of RD and AD would do better?

# AD

```{r}
source('~/ncr_notebooks/baseline_prediction/src/aux_functions.R')
tract_data = read.csv('~/data/baseline_prediction/stripped/dti.csv')
load('~/data/baseline_prediction/dti/ad_voxelwise.RData')
dti_vdata = cbind(tract_data$maskid, ad_data)
gf_fname = '~/data/baseline_prediction/stripped/clinical.csv'
gf = read.csv(gf_fname)
gf_base = gf[gf$BASELINE=='BASELINE', ]
my_ids = intersect(gf_base$MRN, tract_data$MRN)
merged = mergeOnClosestDate(gf_base, tract_data, my_ids)
rm_me = abs(merged$dateX.minus.dateY.months) > 12
merged = merged[!rm_me, ]
dti_base_vdata = merge(merged$maskid, dti_vdata, by.x=1, by.y=1, all.y=F, all.x=T)
```

## inatt3

First we do the univariate filtering:

```{r}
X = dti_base_vdata[, 2:ncol(dti_base_vdata)]
rm_me = colSums(is.na(X)) > 0
X = X[, !rm_me]

keep_me = merged$age <= 12
X = X[keep_me, ]
y = merged$inatt3_named
y = factor(y, levels=c('low', 'medium', 'high'))
y = y[keep_me]

myseed = 1234
set.seed(myseed)
split <- createDataPartition(y, p = .8, list = FALSE)
Xtrain <- X[ split, ]
ytrain <- y[ split ]
Xtest  <- X[-split, ]
ytest = y[-split]

# in voxel analysis it's actually more costly to do the correlation in the entire set of variables
# then to run the for loop, so let's first reduce the variables to only the univariate ones
pvals = sapply(Xtrain, function(d) summary(aov(lm(d ~ ytrain)))[[1]][[5]][1])
Xtrain = Xtrain[, which(pvals <= .05)]
print(dim(Xtrain))

# now we can remove correlated and non-informative variables
nzv = nearZeroVar(Xtrain)
print(nzv)
if (length(nzv) > 0) {
  Xtrain = Xtrain[, -nzv]
}
correlations = cor(Xtrain, use='na.or.complete')
# # library(corrplot)
# # corrplot(correlations, order = "hclust")
highCorr = findCorrelation(correlations, cutoff=.75)
print(length(highCorr))
if (length(highCorr) > 0) {
  Xtrain = Xtrain[, -highCorr]
}
print(dim(Xtrain))

keep_me = sapply(colnames(Xtrain), function(d) which(colnames(Xtest) == d))
Xtest = Xtest[, keep_me]

tuneLength=10
set.seed(myseed)
index <- createMultiFolds(ytrain, k = 5, times = 5)

set.seed(myseed)
fullCtrl <- trainControl(method = "repeatedcv",
                         index = index,
                         savePredictions="final")

library(caretEnsemble)
library(doMC)
registerDoMC(cores = 8)
model_list <- caretList(
  Xtrain, ytrain,
  tuneLength=10,
  trControl=fullCtrl,
  methodList=c('rf', 'kernelpls', 'svmRadial', 'knn', 'rpart', 'bagEarthGCV', 'LogitBoost', 'lda', 'nb')
  )

model_perf = data.frame(lapply(model_list, function(d) getTrainPerf(d)[1]))
names(model_perf) = names(model_list)
model_preds <- lapply(model_list, predict, newdata=Xtest)
model_preds <- lapply(model_preds, function(d, obs) postResample(d, obs)[1], obs=ytest)
model_preds <- data.frame(model_preds)
names(model_preds) = names(model_list)
print(model_perf)
print(model_preds)
print(sprintf('No information rate: Accuracy=%f', max(table(ytrain)/length(ytrain))))
```

```
> print(model_perf)
         rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
1 0.6473583 0.9002713 0.8875342 0.7701967 0.4311058   0.3637606  0.7094752 0.8301637 0.7849145
> print(model_preds)
          rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
Accuracy 0.5       0.5       0.5 0.3684211 0.4473684   0.3421053  0.4571429 0.4473684 0.4210526
> print(sprintf('No information rate: Accuracy=%f', max(table(ytrain)/length(ytrain))))
[1] "No information rate: Accuracy=0.437500"
```
It's really quite poor, similar to FA. There's a nice overfit in training, but it would need to be properly tuned to better fit the test set.

Now we try everything:

```{r}
X = dti_base_vdata[, 2:ncol(dti_base_vdata)]
rm_me = colSums(is.na(X)) > 0
X = X[, !rm_me]

keep_me = merged$age <= 12
X = X[keep_me, ]
y = merged$inatt3_named
y = factor(y, levels=c('low', 'medium', 'high'))
y = y[keep_me]

myseed = 1234
set.seed(myseed)
split <- createDataPartition(y, p = .8, list = FALSE)
Xtrain <- X[ split, ]
ytrain <- y[ split ]
Xtest  <- X[-split, ]
ytest = y[-split]

tuneLength=10
set.seed(myseed)
index <- createMultiFolds(ytrain, k = 5, times = 5)

set.seed(myseed)
fullCtrl <- trainControl(method = "repeatedcv",
                         index = index,
                         savePredictions="final")

library(caretEnsemble)
library(doMC)
registerDoMC(cores = 8)
model_list <- caretList(
  Xtrain, ytrain,
  tuneLength=10,
  trControl=fullCtrl,
  methodList=c('rf', 'svmRadial', 'bagEarthGCV', 'LogitBoost', 'rpart', 'PenalizedLDA')
  )

model_perf = data.frame(lapply(model_list, function(d) getTrainPerf(d)[1]))
names(model_perf) = names(model_list)
model_preds <- lapply(model_list, predict, newdata=Xtest)
model_preds <- lapply(model_preds, function(d, obs) postResample(d, obs)[1], obs=ytest)
model_preds <- data.frame(model_preds)
names(model_preds) = names(model_list)
print(model_perf)
print(model_preds)
print(sprintf('No information rate: Accuracy=%f', max(table(ytrain)/length(ytrain))))
```

## HI3
Same as above, but using HI3:

```{r}
X = dti_base_vdata[, 2:ncol(dti_base_vdata)]
rm_me = colSums(is.na(X)) > 0
X = X[, !rm_me]

keep_me = merged$age <= 12
X = X[keep_me, ]
y = merged$HI3_named
y = factor(y)
y = y[keep_me]
```

Filtered results:
```
> print(model_perf)
         rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
1 0.6473583 0.9002713 0.8875342 0.7701967 0.4311058   0.3637606  0.7094752 0.8301637 0.7849145
> print(model_preds)
          rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost       lda        nb
Accuracy 0.5       0.5       0.5 0.3684211 0.4473684   0.3421053  0.4571429 0.4473684 0.4210526
> print(sprintf('No information rate: Accuracy=%f', max(table(ytrain)/length(ytrain))))
[1] "No information rate: Accuracy=0.437500"
```
Same as before.

Unfiltered results:
```
> print(model_perf)
         rf svmRadial bagEarthGCV LogitBoost     rpart PenalizedLDA
1 0.4803776 0.4673326   0.3943569   0.456534 0.4362268    0.4477639
> print(model_preds)
                rf svmRadial bagEarthGCV LogitBoost     rpart PenalizedLDA
Accuracy 0.4210526 0.4473684   0.3947368  0.4545455 0.4473684    0.4736842
> print(sprintf('No information rate: Accuracy=%f', max(table(ytrain)/length(ytrain))))
[1] "No information rate: Accuracy=0.437500"
```
Not much better...

# RD
```{r}
source('~/ncr_notebooks/baseline_prediction/src/aux_functions.R')
tract_data = read.csv('~/data/baseline_prediction/stripped/dti.csv')
load('~/data/baseline_prediction/dti/rd_voxelwise.RData')
dti_vdata = cbind(tract_data$maskid, rd_data)
gf_fname = '~/data/baseline_prediction/stripped/clinical.csv'
gf = read.csv(gf_fname)
gf_base = gf[gf$BASELINE=='BASELINE', ]
my_ids = intersect(gf_base$MRN, tract_data$MRN)
merged = mergeOnClosestDate(gf_base, tract_data, my_ids)
rm_me = abs(merged$dateX.minus.dateY.months) > 12
merged = merged[!rm_me, ]
dti_base_vdata = merge(merged$maskid, dti_vdata, by.x=1, by.y=1, all.y=F, all.x=T)
```
## inatt3
Filtered results:
```
> print(model_perf)
         rf kernelpls svmRadial       knn     rpart bagEarthGCV LogitBoost
1 0.5436901 0.7111022  0.681085 0.5451698 0.3874621   0.3862696   0.528638
        lda        nb
1 0.6424768 0.6057173
> print(model_preds)
                rf kernelpls svmRadial       knn     rpart bagEarthGCV
Accuracy 0.4210526 0.4210526 0.4210526 0.3947368 0.1842105   0.2631579
         LogitBoost       lda        nb
Accuracy  0.3513514 0.3947368 0.4210526
> print(sprintf('No information rate: Accuracy=%f', max(table(ytrain)/length(ytrain))))
[1] "No information rate: Accuracy=0.437500"
```
Same as with AD and FA. Poor generalization results.

Unfiltered results:
```
> print(model_perf)
         rf svmRadial bagEarthGCV LogitBoost     rpart PenalizedLDA
1 0.4813245 0.4601405   0.3951927  0.4256468 0.4760423    0.4376711
> print(model_preds)
                rf svmRadial bagEarthGCV LogitBoost     rpart PenalizedLDA
Accuracy 0.4210526 0.4210526   0.3157895  0.3888889 0.3684211    0.4473684
[1] "No information rate: Accuracy=0.437500"
```
DTI results clearly not great. Here, we're not even training well.


## HI3
Filtered results:
```
> print(model_perf)
         rf kernelpls svmRadial      knn     rpart bagEarthGCV LogitBoost
1 0.5447837 0.7111022  0.681085 0.544741 0.3874621   0.3868854   0.528638
        lda        nb
1 0.6424768 0.6057173
> print(model_preds)
                rf kernelpls svmRadial       knn     rpart bagEarthGCV
Accuracy 0.4210526 0.4210526 0.4210526 0.4210526 0.1842105   0.3421053
         LogitBoost       lda        nb
Accuracy  0.3513514 0.3947368 0.4210526
> print(sprintf('No information rate: Accuracy=%f', max(table(ytrain)/length(ytrain))))
[1] "No information rate: Accuracy=0.437500"
```
For hi, not even training is that good.

Unfiltered results:
```
> print(model_perf)
         rf svmRadial bagEarthGCV LogitBoost     rpart PenalizedLDA
1 0.4813245 0.4601405   0.3951927  0.4256468 0.4760423    0.4376711
> print(model_preds)
                rf svmRadial bagEarthGCV LogitBoost     rpart PenalizedLDA
Accuracy 0.4210526 0.4210526   0.3157895  0.3888889 0.3684211    0.4473684
> print(sprintf('No information rate: Accuracy=%f', max(table(ytrain)/length(ytrain))))
[1] "No information rate: Accuracy=0.437500"
```
Unfiltered results are not any better



Now we can do some wrapping feature selection to see if it does better than filtering. Note that we'll need to train the functions by themselves and then make a list, otherwise it won't work for ensembles:

```{r}
X = dti_base_vdata[, 2:ncol(dti_base_vdata)]
rm_me = colSums(is.na(X)) > 0
X = X[, !rm_me]

y = vector()
keep_me = vector()
target_col = which(colnames(gf)=='SX_inatt')
for (s in merged$MRN) {
  if ((gf_base[gf_base$MRN==s,]$age <= 12) && (gf_base[gf_base$MRN==s, target_col] >= 2)) {
    idx = gf$MRN==s
    sx_slope = lm(gf[idx, target_col] ~ gf[idx, ]$age)$coefficients[2]
    keep_me = c(keep_me, which(merged$MRN == s))
    y = c(y, sx_slope)
  }
}
X = X[keep_me, ]

myseed = 1234

set.seed(myseed)
split <- createDataPartition(y, p = .8, list = FALSE)

Xtrain <- X[ split, ]
ytrain <- y[ split ]
Xtest  <- X[-split, ]
ytest = y[-split]
print(dim(Xtrain))

varSeq = 1:ncol(X)
ctrl <- rfeControl(method = "repeatedcv",
                   saveDetails = TRUE,
                   index = index,
                   returnResamp = "final")
ctrl$functions <- rfFuncs
set.seed(myseed)
rfRFE <- rfe(Xtrain, ytrain,
             sizes = varSeq,
             rfeControl = ctrl,
             summaryFunction = defaultSummary)
rfRFE

```




