---
title: "Playing with LOOCV"
output: html_notebook
---

I want to spend a bit more time with LOOCV because it will be the best way to implement the voting scheme per modality. If we end up tossing everything together then it doesn't matter. But if we do that, if will be interesting to have some unsupervised step to combine the modalities. For example, any of the methods listed here:

https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-15-162

might be interesting. 

Of course, the other option would be to just throw all data into a classifier that takes NaNs (e.g. ada family). That would also not restrict us to LOOCV.

So, let's create different 315 x features matrices (actually, 290, after restricting for age <= 12), just using residuals as usual. Then, we can merge them together if we are going to just throw everything in, or keep them separate for the voting scheme or unsupervised learning.

```{r}
get_needed_residuals = function(y, fm_str, cutoff, df) {
  fm = as.formula(fm_str)
  fit = lm(fm)
  # selecting which covariates to use
  fm = "y ~ "
  for (r in 2:dim(summary(fit)$coefficients)[1]) {
    if (summary(fit)$coefficients[r, 4] < cutoff) {
      cname = rownames(summary(fit)$coefficients)[r]
      cname = gsub("SEXMale", "SEX", cname)
      fm = sprintf('%s + %s', fm, cname)
    }
  }
  # don't do anything if no variables were significant
  if (fm != 'y ~ ') {
    idx = !is.na(y)
    opt_fit = lm(as.formula(fm))
    y[idx] = opt_fit$residuals
  }
  return(y)
}

source('~/ncr_notebooks/baseline_prediction/src/aux_functions.R')
gf_fname = '~/data/baseline_prediction/stripped/clinical.csv'
gf = read.csv(gf_fname)
gf_base = gf[gf$BASELINE=='BASELINE' & gf$age <= 12, ]
my_ids = unique(gf_base$MRN)
```

```{r}
tract_data = read.csv('~/data/baseline_prediction/stripped/dti.csv')
rm_me = (tract_data$fa_avg < .4 | tract_data$ad_avg < 1.18 | tract_data$rd_avg > .65 | tract_data$rd_avg < .5 |
         tract_data$norm.trans > .45 | tract_data$norm.rot > .008 | tract_data$goodSlices < 45 | tract_data$goodSlices > 70)
tract_data = tract_data[!rm_me, ]
merged = mergeOnClosestDate(gf_base, tract_data, my_ids)
phen_vars = c(which(grepl("^FA_", colnames(merged))),
              which(grepl("^AD_", colnames(merged))),
              which(grepl("^RD_", colnames(merged))),
              which(grepl("^MO_", colnames(merged)))
              )
rm_me = abs(merged$dateX.minus.dateY.months) > 12
X = merged[, phen_vars]
X[which(rm_me), ] = NA
library(parallel)
cl <- makeCluster(4)
X_resid = parSapply(cl, X, get_needed_residuals, 'y ~ df$age + I(df$age^2) + df$SEX', .1, merged)
stopCluster(cl)
dti_tracts = as.data.frame(X_resid)
```

```{r}
struct_data = read.csv('~/data/baseline_prediction/stripped/structural.csv')
rm_me = (struct_data$mprage_score > 2)
struct_data = struct_data[!rm_me, ]
merged = mergeOnClosestDate(gf_base, struct_data, my_ids)
X = merged[, 32:301]
rm_me = abs(merged$dateX.minus.dateY.months) > 12  | merged$age_at_scan > 12
X[which(rm_me), ] = NA
cl <- makeCluster(4)
X_resid = parSapply(cl, X, get_needed_residuals, 'y ~ df$age + I(df$age^2) + df$SEX', .1, merged)
stopCluster(cl)
struct_rois = as.data.frame(X_resid)
```

```{r}
beery_data = read.csv('~/data/baseline_prediction/stripped/beeryVMI.csv')
mbeery = mergeOnClosestDate(gf_base, beery_data, my_ids, y.date='record.date.collected', y.id='Medical.Record...MRN')
rm_me = abs(mbeery$dateX.minus.dateY.months) > 12
mbeery[which(rm_me),] = NA
X = mbeery$Standard.score

cpt_data = read.csv('~/data/baseline_prediction/stripped/cpt.csv')
mcpt = mergeOnClosestDate(gf_base, cpt_data, my_ids)
rm_me = abs(mcpt$dateX.minus.dateY.months) > 12
mcpt[which(rm_me),] = NA
phen_vars = c('N_of_omissions', 'N_commissions', 'hit_RT', 'hit_RT_SE', 'variability_of_SE', 'N_perservations',
              'hit_RT_block_change', 'hit_RT_SE_block_change', 'hit_RT_ISI_change', 'hit_RT_SE_ISI_change')
keep_me = sapply(phen_vars, function(d) which(colnames(mcpt) == d))
X = cbind(X, mcpt[, keep_me])
colnames(X)[1] = 'BeeryVM.StdScore'

iq_data = read.csv('~/data/baseline_prediction/stripped/iq.csv')
miq = mergeOnClosestDate(gf_base, iq_data, my_ids, y.id='Medical.Record...MRN', y.date='record.date.collected')
rm_me = abs(miq$dateX.minus.dateY.months) > 12
miq[which(rm_me),] = NA
X = cbind(X, miq$FSIQ)
colnames(X)[ncol(X)] = 'FSIQ'

wisc_data = read.csv('~/data/baseline_prediction/stripped/wisc.csv')
mwisc = mergeOnClosestDate(gf_base, wisc_data, my_ids, y.id='Medical.Record...MRN', y.date='record.date.collected')
rm_me = abs(mwisc$dateX.minus.dateY.months) > 12
mwisc[which(rm_me),] = NA
phen_vars = c('Raw.score..DSF', 'Raw.score..DSB', 'Raw.score..SSF', 'Raw.score..SSB')
keep_me = sapply(phen_vars, function(d) which(colnames(mwisc) == d))
X = cbind(X, mwisc[, keep_me])

wj_data = read.csv('~/data/baseline_prediction/stripped/wj.csv')
mwj = mergeOnClosestDate(gf_base, wj_data, my_ids, y.id='Medical.Record...MRN', y.date='record.date.collected')
rm_me = abs(mwj$dateX.minus.dateY.months) > 12
mwj[which(rm_me),] = NA
phen_vars = c('Raw.Score..VM', 'Raw.Score..DS', 'PS')
keep_me = sapply(phen_vars, function(d) which(colnames(mwj) == d))
X = cbind(X, mwj[, keep_me])
cl <- makeCluster(4)
X_resid = parSapply(cl, X, get_needed_residuals, 'y ~ df$age + I(df$age^2) + df$SEX', .1, gf_base)
stopCluster(cl)
neuropsych = as.data.frame(X_resid)
```

# Geospatial
```{r}
geo_data = read.csv('~/data/baseline_prediction/stripped/geospatial.csv')
merged = merge(gf_base, geo_data, by='MRN')
# some variables are not being read as numeric...
merged$Home_Price = as.numeric(merged$Home_Price)
merged$Fam_Income = as.numeric(merged$Fam_Income)
merged$Crime_Rate = as.numeric(merged$Crime_Rate)
phen_vars = c('SES', 'Home_Type', 'Home_Price', 'Fam_Income', 'Pop_BPL', 'Fam_BPL', 'Pub_School',
              'Crime_Rate', 'Green_Space', 'Park_Access', 'Air_Quality', 'Obesity_Rate',
              'Food_Index', 'Exercise_Access', 'Excessive_Drinking',
              'poverty_education', 'social_environment', 'physical_health_environment', 
              'physical_envronment')
keep_me = sapply(phen_vars, function(d) which(colnames(merged) == d))
X = merged[, keep_me]
cl <- makeCluster(4)
X_resid = parSapply(cl, X, get_needed_residuals, 'y ~ df$age + I(df$age^2) + df$SEX', .1, merged)
stopCluster(cl)
geospatial = as.data.frame(X_resid)
```

```{r}
prs_data = read.csv('~/data/baseline_prediction/stripped/PRS.csv')
# we don't need the extra BASELINE column
prs_data = prs_data[, -3]
merged = merge(gf_base, prs_data, by='MRN', all.x = T)
X = merged[, 29:ncol(merged)]
cl <- makeCluster(4)
X_resid = parSapply(cl, X, get_needed_residuals, 'y ~ df$age + I(df$age^2) + df$SEX', .1, merged)
stopCluster(cl)
prs = as.data.frame(X_resid)
```

OK, now everything is residulized into 290 x feat matrices. Time to play.

```{r}
ntimes = 50
model = 'svmRadial'
myseed = 1234
tuneLength = 10
cpuDiff = 0
dsets = c('prs', 'geospatial', 'neuropsych', 'struct_rois', 'dti_tracts')
vote_nvVSadhd = function(X, s, uni=T, pca=T, do_rfe=F) {
  y = gf_base$DX_BASELINE
  y[y!='NV'] = 'ADHD'
  y = factor(y, levels=c('ADHD', 'NV'))
  
  Xtrain <- X[ -s, ]
  ytrain <- y[ -s ]
  Xtest  <- X[s, ]
  
  # remove training examples that are all NaN. Note that this will never happen for testing!
  rm_me = rowSums(is.na(Xtrain)) == ncol(Xtrain)
  Xtrain = Xtrain[!rm_me,]
  ytrain = ytrain[!rm_me]
  
  # keep only good univariate variables
  if (uni > 0) {
    pvals = sapply(Xtrain, function(d, ytrain) t.test(d ~ ytrain)$p.value, ytrain)
    good_vars = which(pvals <= uni)
    if (length(good_vars) > 1) {
      Xtrain = Xtrain[, good_vars]
      keep_me = sapply(colnames(Xtrain), function(d) which(colnames(Xtest) == d))
      Xtest = Xtest[, keep_me]
    } else {
      return(data.frame(ADHD = NA, NV = NA))
    }
  }
  
  if (pca) {
    pp <- preProcess(Xtrain, method = c('pca'), thresh=.9)
    Xtrain<- predict(pp, Xtrain)
    Xtest <- predict(pp, Xtest)
  }
  
  if (do_rfe) {
    set.seed(myseed)

    ctrl <- rfeControl(functions = treebagFuncs,
                       method = "boot",
                       repeats = 5,
                       verbose = FALSE
                       )
    subsets = floor(seq(from=1, to=ncol(Xtrain)/2, length.out=min(10, ncol(Xtrain)/2)))
    mymod <- rfe(Xtrain, ytrain,
                     sizes = subsets,
                     rfeControl = ctrl)
  } else{
    set.seed(myseed)
    fullCtrl <- trainControl(method = "boot",
                             number = ntimes,
                             savePredictions="final",
                             classProbs=TRUE,
                             summaryFunction=twoClassSummary,
                             allowParallel=T)
    set.seed(myseed)
    mymod = train(Xtrain, ytrain,
                  tuneLength=tuneLength,
                  trControl=fullCtrl,
                  metric='ROC',
                  method=model,
                  preProcess = c('medianImpute'))
  }
  return(predict(mymod, newdata=Xtest, type='prob')) 
}

nsubjs = nrow(gf_base)
preds = matrix(nrow=nsubjs, ncol=length(dsets))
colnames(preds) = dsets
rownames(preds) = gf_base$MRN
library(doMC)
ncpus <- detectBatchCPUs()
njobs <- ncpus - cpuDiff
registerDoMC(njobs)

# get a vote for each dset if the participant has data in the domain
for (s in 1:2){#nsubjs) {
  print(sprintf('LOOCV %d / %d', s, nsubjs))
  for (dset in dsets) {
    eval(parse(text=sprintf('nfeats = ncol(%s)', dset)))
    eval(parse(text=sprintf('na_feats = sum(is.na(%s[s, ]))', dset)))
    if (na_feats < nfeats) {
      eval(parse(text=sprintf('X = %s', dset)))
      if (dset %in% c('brain_fa', 'brain_ad', 'brain_rd',
                      'brain_thickness', 'brain_volume', 'brain_area',
                      'struct_rois')) {
        uni = .05
        pca = T
        do_rfe = F
      } else {
        uni = 0
        pca = F
        do_rfe = F
      }
      preds[s, dset] = vote_nvVSadhd(X, s, uni=uni, pca=pca, do_rfe=do_rfe)['ADHD'][[1]]
    }
  }
}
```

We should also save them:

```{r}
save(preds, file='~/data/tmp/vote_nvVSadhd_kernelpls_uni_boot200.RData')
```

Then we need a voting scheme. 

```{r}
y = gf_base$DX_BASELINE
y[y!='NV'] = 'ADHD'
y = factor(y, levels=c('ADHD', 'NV'))
bin = preds
bin[preds > .5] = 1
bin[preds < .5] = 2
num = colSums(bin==as.numeric(y), na.rm=T)
den = colSums(!is.na(bin))
print(num / den)
mean_vote = rowMeans(preds, na.rm=T)
mean_vote[mean_vote > .5] = 1
mean_vote[mean_vote < .5] = 2
print(mean(mean_vote==as.numeric(y), na.rm=T))
```

LET'S FOCUS ON CLASSIFIERS THAT CAN HANDLE 3 CLASSES, and alo robust to outliers
CHANGE CODE TO DO SUBJECTS INDEPENDENTLY SAVING TO A LOG FILE
ADD 3 GROUP CLASSIFICATION
ADD SYMPTOM SLOPE WITHIN ADHD PREDICTION
ADD BASELINE SX PREDICTION
ADD VOXEL VARIABLES

I want to try a series of models that do some sort of feature selection themselves (whether it's stepwise through a wrapper or implicit), timing them, and checking whether they deal with multiple classes. We can run them for 3 different subjects to get an average + sd for time running.

```{r}
ntimes = 50
model = 'logreg'
myseed = 1234
tuneLength = 10
cpuDiff = 0
dsets = c('prs', 'geospatial', 'neuropsych', 'struct_rois', 'dti_tracts')

vote_hiOutcome = function(X, s, uni=T, pca=T, do_rfe=F) {
  y = gf_base$HI3_named
  y = factor(y)
  
  Xtrain <- X[ -s, ]
  ytrain <- y[ -s ]
  Xtest  <- X[s, ]
  
  # remove training examples that are all NaN. Note that this will never happen for testing!
  rm_me = rowSums(is.na(Xtrain)) == ncol(Xtrain)
  Xtrain = Xtrain[!rm_me,]
  ytrain = ytrain[!rm_me]
  
  if (pca) {
    pp <- preProcess(Xtrain, method = c('pca'), thresh=.9)
    Xtrain<- predict(pp, Xtrain)
    Xtest <- predict(pp, Xtest)
  }
  
  set.seed(myseed)
  fullCtrl <- trainControl(method = "boot",
                           number = ntimes,
                           savePredictions="final",
                           classProbs=TRUE,
                           summaryFunction=multiClassSummary,
                           allowParallel=T)
  set.seed(myseed)
  mymod = train(Xtrain, ytrain,
                tuneLength=tuneLength,
                trControl=fullCtrl,
                metric='Mean_ROC',
                method=model,
                preProcess = c('medianImpute'))
  return(predict(mymod, newdata=Xtest)) 
}

nsubjs = nrow(gf_base)
preds = matrix(nrow=nsubjs, ncol=length(dsets))
colnames(preds) = dsets
rownames(preds) = gf_base$MRN
library(doMC)
ncpus <- detectBatchCPUs()
njobs <- ncpus - cpuDiff
registerDoMC(njobs)

# get a vote for each dset if the participant has data in the domain
for (s in 1:2){#nsubjs) {
  print(sprintf('LOOCV %d / %d', s, nsubjs))
  for (dset in dsets) {
    eval(parse(text=sprintf('nfeats = ncol(%s)', dset)))
    eval(parse(text=sprintf('na_feats = sum(is.na(%s[s, ]))', dset)))
    if (na_feats < nfeats) {
      eval(parse(text=sprintf('X = %s', dset)))
      if (dset %in% c('brain_fa', 'brain_ad', 'brain_rd',
                      'brain_thickness', 'brain_volume', 'brain_area',
                      'struct_rois')) {
        pca = T
      } else {
        pca = F
      }
      preds[s, dset] = vote_hiOutcome(X, s, pca=pca)[[1]]
    }
  }
}
```