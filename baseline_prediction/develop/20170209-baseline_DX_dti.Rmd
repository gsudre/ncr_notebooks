---
title: "DTI baseline DX"
output: html_notebook
---

Let's try to get baseline DX from DTI. First, some data cleaning:

```{r}
library(caret)
library(doParallel)
# for replication purposes
set.seed(107)
source('~/ncr_notebooks/baseline_prediction//src/aux_functions.R')

gf_fname = '~/data/baseline_prediction/dti_match_02092017.csv'
data_fname = '~/data/baseline_prediction/dti_match_02092017.csv'
gf = read.csv(gf_fname)
group_var = 'DX_A'
all_data = read.csv(data_fname)
mrn_gf = unique(gf[, c('ID', group_var)])

idx = all_data$DTI_select < 2 & all_data$QC_GRADE < 3
data = all_data[idx,]

ldata = merge(mrn_gf, data, by.x='ID', by.y='ID')
eval(parse(text=sprintf('groups = ldata$\"%s.y\"', group_var)))
idx = groups=='NV' | groups=='ADHD'
groups = factor(groups[idx])
ldata = ldata[idx,]

phen_vars = c(which(grepl("MASKID", colnames(ldata))),
              which(grepl("^FA_", colnames(ldata))),
              which(grepl("^AD_", colnames(ldata))),
              which(grepl("^RD_", colnames(ldata)))
              )
ldata = ldata[, phen_vars]

```

At this point we have only clean DTI data in ldata. Let's check whether they have any remaining outliers in the property averages:

```{r}
mean_data = read.csv('~/data/baseline_prediction/dti_tortoiseExported_meanTSA_02012017.csv')
# remove data columns from mean_data to avoid duplicates
phen_idx = grepl("^FA_", colnames(mean_data)) | 
            grepl("^AD_", colnames(mean_data)) |
            grepl("^RD_", colnames(mean_data))

mean_data = mean_data[, !phen_idx]
ldata2 = merge(ldata, mean_data, by.x='MASKID', by.y='Mask.ID...Scan')

par(mfrow=c(1, 3))

nbreaks = 40
hist(ldata2$fa_avg, breaks=nbreaks)
hist(ldata2$ad_avg, breaks=nbreaks)
hist(ldata2$rd_avg, breaks=nbreaks)
```

Cleaning up based on visual cutoffs:

``` {r}
rm_me = ldata2$fa_avg < .4 | ldata2$ad_avg < 1.18 | ldata2$rd_avg > .7
ldata = ldata2[!rm_me, ]
groups = groups[!rm_me]
```

Now we keep only the variables we'll be using for predictions:

``` {r}
phen_vars = c(which(grepl("^FA_", colnames(ldata))),
              which(grepl("^AD_", colnames(ldata))),
              which(grepl("^RD_", colnames(ldata)))
              )
ldata = ldata[, phen_vars]
```

Start classifying and plotting the results:
```{r}
ncv = 5
nrepeatcv = 2
nsplits = 5
train_test_ratio = .85
root_fname = '~/tmp/dti_NVvsADHD_3props'
# saving output
sink(sprintf('%s.log', root_fname), append=FALSE, split=TRUE)
ncores = detectCores(all.tests = FALSE, logical = TRUE)
njobs = ncores - 3
run_models = c('rndForest', 'lr', 'lsvm', 'rsvm', 'xgb', 'gbm') #c('rndForest', 'lr', 'lsvm', 'rsvm', 'xgb', 'gbm')

inTrain = createDataPartition(y = groups,
                              times= nsplits,
                              p=train_test_ratio)
metric = "ROC"
default_preproc = c("center", 'scale')
default_options = c()
ctrl_cv <- trainControl(method = "repeatedcv",
                        number = ncv,
                        repeats = nrepeatcv,
                        classProbs = TRUE,
                        returnData = FALSE,
                        summaryFunction = twoClassSummary,
                        preProcOptions = default_options,
                        search='grid')

source('~/ncr_notebooks/baseline_prediction/src/do_classification.R')
source('~/ncr_notebooks/baseline_prediction/src/do_metrics_plots.R')
```

Given the time they're taking to run, gbm and lsvm don't seem to be worth the effort, at least not with their current search grids. Maybe they do better if we PCA the features first?

```{r}
ncv = 5
nrepeatcv = 2
nsplits = 5
train_test_ratio = .85
root_fname = '~/tmp/dti_NVvsADHD_3props_pca'
# saving output
sink(sprintf('%s.log', root_fname), append=FALSE, split=TRUE)
save(list=c('ldata', 'groups'), file=sprintf('%s.RData', root_fname))
ncores = detectCores(all.tests = FALSE, logical = TRUE)
njobs = ncores - 3
run_models = c('rndForest', 'lr', 'lsvm', 'rsvm', 'xgb', 'gbm') #c('rndForest', 'lr', 'lsvm', 'rsvm', 'xgb', 'gbm')

inTrain = createDataPartition(y = groups,
                              times= nsplits,
                              p=train_test_ratio)
metric = "ROC"
default_preproc = c('pca')
default_options = c()
ctrl_cv <- trainControl(method = "repeatedcv",
                        number = ncv,
                        repeats = nrepeatcv,
                        classProbs = TRUE,
                        returnData = FALSE,
                        summaryFunction = twoClassSummary,
                        preProcOptions = default_options,
                        search='grid')

source('~/ncr_notebooks/baseline_prediction/src/do_classification.R')
```

```{r}
source('~/ncr_notebooks/baseline_prediction/src/do_metrics_plots.R')
```

Correlation seems to have helped lsvm and gmb, but not to the point of making it worth it spending that time. Let's try the independent set of features, and their PCAs:

```{r}
phen_vars = c(which(grepl("^FA_", colnames(ldata))))
ldata = ldata[, phen_vars]
ncv = 5
nrepeatcv = 2
nsplits = 5
train_test_ratio = .85
root_fname = '~/tmp/dti_NVvsADHD_FAonly'
# saving output
sink(sprintf('%s.log', root_fname), append=FALSE, split=TRUE)
save(list=c('ldata', 'groups'), file=sprintf('%s.RData', root_fname))
ncores = detectCores(all.tests = FALSE, logical = TRUE)
njobs = ncores - 3
run_models = c('rndForest', 'lr', 'rsvm', 'xgb') #c('rndForest', 'lr', 'lsvm', 'rsvm', 'xgb', 'gbm')

inTrain = createDataPartition(y = groups,
                              times= nsplits,
                              p=train_test_ratio)
metric = "ROC"
default_preproc = c('center', 'scale')
default_options = c()
ctrl_cv <- trainControl(method = "repeatedcv",
                        number = ncv,
                        repeats = nrepeatcv,
                        classProbs = TRUE,
                        returnData = FALSE,
                        summaryFunction = twoClassSummary,
                        preProcOptions = default_options,
                        search='grid')

source('~/ncr_notebooks/baseline_prediction/src/do_classification.R')
```

```{r}
source('~/ncr_notebooks/baseline_prediction/src/do_metrics_plots.R')
```
What if we PCA that?
```{r}
ncv = 5
nrepeatcv = 2
nsplits = 5
train_test_ratio = .85
root_fname = '~/tmp/dti_NVvsADHD_FAonly_pca'
# saving output
sink(sprintf('%s.log', root_fname), append=FALSE, split=TRUE)
save(list=c('ldata', 'groups'), file=sprintf('%s.RData', root_fname))
ncores = detectCores(all.tests = FALSE, logical = TRUE)
njobs = ncores - 3
run_models = c('rndForest', 'lr', 'rsvm', 'xgb') #c('rndForest', 'lr', 'lsvm', 'rsvm', 'xgb', 'gbm')

inTrain = createDataPartition(y = groups,
                              times= nsplits,
                              p=train_test_ratio)
metric = "ROC"
default_preproc = c('pca')
default_options = c()
ctrl_cv <- trainControl(method = "repeatedcv",
                        number = ncv,
                        repeats = nrepeatcv,
                        classProbs = TRUE,
                        returnData = FALSE,
                        summaryFunction = twoClassSummary,
                        preProcOptions = default_options,
                        search='grid')

source('~/ncr_notebooks/baseline_prediction/src/do_classification.R')
```

```{r}
source('~/ncr_notebooks/baseline_prediction/src/do_metrics_plots.R')
```

PCA seems to have helped overall... interesting. Let's try just AD and RD:

```{r}
phen_vars = c(which(grepl("^AD_", colnames(ldata))),
              which(grepl("^RD_", colnames(ldata))))
ldata = ldata[, phen_vars]

ncv = 5
nrepeatcv = 2
nsplits = 5
train_test_ratio = .85
root_fname = '~/tmp/dti_NVvsADHD_ADRD'
# saving output
sink(sprintf('%s.log', root_fname), append=FALSE, split=TRUE)
save(list=c('ldata', 'groups'), file=sprintf('%s.RData', root_fname))
ncores = detectBatchCPUs()
njobs = ncores - 3
run_models = c('rndForest', 'lr', 'rsvm', 'xgb') #c('rndForest', 'lr', 'lsvm', 'rsvm', 'xgb', 'gbm')

inTrain = createDataPartition(y = groups,
                              times= nsplits,
                              p=train_test_ratio)
metric = "ROC"
default_preproc = c('center', 'scale')
default_options = c()
ctrl_cv <- trainControl(method = "repeatedcv",
                        number = ncv,
                        repeats = nrepeatcv,
                        classProbs = TRUE,
                        returnData = FALSE,
                        summaryFunction = twoClassSummary,
                        preProcOptions = default_options,
                        search='grid')

source('~/ncr_notebooks/baseline_prediction/src/do_classification.R')
```

```{r}
source('~/ncr_notebooks/baseline_prediction/src/do_metrics_plots.R')
```

And the PCA countepart:

```{r}
ncv = 5
nrepeatcv = 2
nsplits = 5
train_test_ratio = .85
root_fname = '~/tmp/dti_NVvsADHD_ADRD_pca'
# saving output
sink(sprintf('%s.log', root_fname), append=FALSE, split=TRUE)
save(list=c('ldata', 'groups'), file=sprintf('%s.RData', root_fname))
ncores = detectBatchCPUs()
njobs = ncores - 3
run_models = c('rndForest', 'lr', 'rsvm', 'xgb') #c('rndForest', 'lr', 'lsvm', 'rsvm', 'xgb', 'gbm')

inTrain = createDataPartition(y = groups,
                              times= nsplits,
                              p=train_test_ratio)
metric = "ROC"
default_preproc = c('pca')
default_options = c()
ctrl_cv <- trainControl(method = "repeatedcv",
                        number = ncv,
                        repeats = nrepeatcv,
                        classProbs = TRUE,
                        returnData = FALSE,
                        summaryFunction = twoClassSummary,
                        preProcOptions = default_options,
                        search='grid')

source('~/ncr_notebooks/baseline_prediction/src/do_classification.R')
```
```{r}
source('~/ncr_notebooks/baseline_prediction/src/do_metrics_plots.R')
```

PCA might have hurt this a bit, now.
