---
title: "Analyzing DTI residuals"
output: html_notebook
---

Philip suggested we should use the residuals of regressing the brain variables on age and sex. It's worth a try. 

```{r}
source('~/ncr_notebooks/baseline_prediction/src/aux_functions.R')

tract_data = read.csv('~/data/baseline_prediction/stripped/dti.csv')
rm_me = (tract_data$fa_avg < .4 | tract_data$ad_avg < 1.18 | tract_data$rd_avg > .65 | tract_data$rd_avg < .5 |
         tract_data$norm.trans > .45 | tract_data$norm.rot > .008 | tract_data$goodSlices < 45 | tract_data$goodSlices > 70)
print(sprintf('Reducing from %d to %d scans', nrow(tract_data), nrow(tract_data)-sum(rm_me)))
tract_data = tract_data[!rm_me, ]
gf_fname = '~/data/baseline_prediction/stripped/clinical.csv'
gf = read.csv(gf_fname)
gf = gf[gf$BASELINE=='BASELINE', ]
my_ids = intersect(gf$MRN, tract_data$MRN)
merged = mergeOnClosestDate(gf, tract_data, my_ids)
rm_me = abs(merged$dateX.minus.dateY.months) > 12
print(sprintf('Reducing from %d to %d scans', nrow(merged), nrow(merged)-sum(rm_me)))
merged = merged[!rm_me, ]

phen_vars = c(which(grepl("^FA_", colnames(merged))),
              which(grepl("^AD_", colnames(merged))),
              which(grepl("^RD_", colnames(merged))),
              which(grepl("^MO_", colnames(merged)))
              )
X = merged[, phen_vars]
y = merged$DX_BASELINE
y[y != 'NV'] = 'ADHD'
y = factor(y)
rm_me = rowSums(is.na(X)) > 0
X = X[!rm_me, ]
merged = merged[!rm_me, ]
y = y[!rm_me]
X_resid = sapply(X, function(d) lm(d ~ merged$age_at_scan + merged$SEX)$residuals)
X_resid = as.data.frame(X_resid)
```

Now we start some ML shenanigans...

Let's first focus on the variables that have good T-stats for now. We will manually set them at first, but then we can do it within the CV loops:

```{r}
library(pROC)

phen_vars = c('FA_left_cst', 'FA_right_cst', 'FA_right_ifo', 'FA_right_ilf', 'FA_right_slf',
              'RD_right_slf', 'MO_left_ifo', 'MO_right_ifo', 'MO_right_ilf', 'MO_right_slf')
keep_me = sapply(phen_vars, function(d) which(colnames(X_resid) == d))
X = X_resid[, keep_me]

myseed = 1234
cpuDiff = 1
tuneLength = 10
default_preproc = c("center", 'scale')
set.seed(myseed)
split <- createDataPartition(y, p = .8, list = FALSE)

Xtrain <- X[ split, ]
ytrain <- y[ split ]
Xtest  <- X[-split, ]
ytest = y[-split]

set.seed(myseed)
index <- createMultiFolds(ytrain, k = 5, times = 5)

library(doMC)
ncpus <- detectBatchCPUs()
njobs <- ncpus - cpuDiff
registerDoMC(njobs)
selFunc = 'best'  # maybe try oneSE and tolerance as well?

set.seed(myseed)
fullCtrl <- trainControl(method = "repeatedcv",
                         index = index,
                         search='grid',
                         summaryFunction = twoClassSummary,
                         classProbs = TRUE)
rndForestGrid <- expand.grid(.mtry=c(1:sqrt(ncol(Xtrain))))
rfFull <- train(Xtrain, ytrain,
                method = "rf",
                trControl = fullCtrl,
                tuneGrid=rndForestGrid,
                metric = 'Accuracy',
                preProcess = default_preproc)
  
rfFull
pred = predict(rfFull, Xtest)
postResample(pred, ytest)
roc(as.numeric(ytest), as.numeric(pred))

varSeq = 1:ncol(X)
ctrl <- rfeControl(method = "repeatedcv",
                   saveDetails = TRUE,
                   index = index,
                   returnResamp = "final")

ctrl$functions <- rfFuncs
set.seed(myseed)
rfRFE <- rfe(Xtrain, ytrain,
             sizes = varSeq,
             rfeControl = ctrl,
             preProcess = default_preproc,
             summaryFunction = twoClassSummary,
             metric = 'Accuracy')
rfRFE
pred = predict(rfRFE, Xtest)
postResample(pred, ytest)
roc(as.numeric(ytest), as.numeric(pred$pred))
```

Very underwhelming results... let's see what we can get with symptom regression. None of them has a significant correlation to symptoms, so let's include everything.

```{r}
thresh = 1
keep_me = merged$SX_inatt >= thresh & merged$SX_HI >= thresh
X = X_resid[keep_me,]
y = merged[keep_me,]$SX_inatt

myseed = 1234
cpuDiff = 1
tuneLength = 10
default_preproc = c("center", 'scale')
set.seed(myseed)
split <- createDataPartition(y, p = .8, list = FALSE)

Xtrain <- X[ split, ]
ytrain <- y[ split ]
Xtest  <- X[-split, ]
ytest = y[-split]

set.seed(myseed)
index <- createMultiFolds(ytrain, k = 5, times = 5)

library(doMC)
ncpus <- detectBatchCPUs()
njobs <- ncpus - cpuDiff
registerDoMC(njobs)
selFunc = 'best'  # maybe try oneSE and tolerance as well?

set.seed(myseed)
fullCtrl <- trainControl(method = "repeatedcv",
                         index = index,
                         search='grid',
                         summaryFunction = defaultSummary)
rndForestGrid <- expand.grid(.mtry=c(1:sqrt(ncol(Xtrain))))
rfFull <- train(Xtrain, ytrain,
                method = "rf",
                trControl = fullCtrl,
                tuneGrid=rndForestGrid,
                preProcess = default_preproc)
  
rfFull
pred = predict(rfFull, Xtest)
postResample(pred, ytest)

varSeq = 1:ncol(X)
ctrl <- rfeControl(method = "repeatedcv",
                   saveDetails = TRUE,
                   index = index,
                   returnResamp = "final")

ctrl$functions <- rfFuncs
set.seed(myseed)
rfRFE <- rfe(Xtrain, ytrain,
             sizes = varSeq,
             rfeControl = ctrl,
             preProcess = default_preproc,
             summaryFunction = defaultSummary)
rfRFE
pred = predict(rfRFE, Xtest)
postResample(pred, ytest)
```

It's interesting that by removing the 0/0s we get an improvement in symptom regression. That's what I expected. But is the residualizing process doing anything?

```{r}
thresh = 1
keep_me = merged$SX_inatt >= thresh & merged$SX_HI >= thresh
X = X_resid[keep_me,]
y = merged[keep_me,]$SX_inatt

myseed = 1234
cpuDiff = 1
tuneLength = 10
default_preproc = c("center", 'scale')
set.seed(myseed)
split <- createDataPartition(y, p = .8, list = FALSE)

Xtrain <- X[ split, ]
ytrain <- y[ split ]
Xtest  <- X[-split, ]
ytest = y[-split]

set.seed(myseed)
index <- createMultiFolds(ytrain, k = 5, times = 5)

library(doMC)
ncpus <- detectBatchCPUs()
njobs <- ncpus - cpuDiff
registerDoMC(njobs)
selFunc = 'best'  # maybe try oneSE and tolerance as well?

set.seed(myseed)
fullCtrl <- trainControl(method = "repeatedcv",
                         index = index,
                         search='grid',
                         summaryFunction = defaultSummary)
rndForestGrid <- expand.grid(.mtry=c(1:sqrt(ncol(Xtrain))))
rfFull <- train(Xtrain, ytrain,
                method = "rf",
                trControl = fullCtrl,
                tuneGrid=rndForestGrid,
                preProcess = default_preproc)
  
rfFull
pred = predict(rfFull, Xtest)
postResample(pred, ytest)

varSeq = 1:ncol(X)
ctrl <- rfeControl(method = "repeatedcv",
                   saveDetails = TRUE,
                   index = index,
                   returnResamp = "final")

ctrl$functions <- rfFuncs
set.seed(myseed)
rfRFE <- rfe(Xtrain, ytrain,
             sizes = varSeq,
             rfeControl = ctrl,
             preProcess = default_preproc,
             summaryFunction = defaultSummary)
rfRFE
pred = predict(rfRFE, Xtest)
postResample(pred, ytest)
```




